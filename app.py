import os
import json
from typing import Optional, Union, Tuple
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from config import Config

# Cargar variables de entorno
load_dotenv()

# Cargar variables de entorno
load_dotenv()

app = Flask(__name__)
app.config.from_object(Config)

print("ü§ñ Configurando modelos de IA...")

# Configurar ambos modelos de IA
models = {}

# Configurar Llama3 (local)
try:
    models['llama3'] = ChatOllama(
        model="llama3",
        temperature=Config.DEFAULT_TEMPERATURE
    )
    print("‚úÖ Llama3 configurado correctamente")
except Exception as e:
    print(f"‚ö†Ô∏è Llama3 no disponible: {e}")
    models['llama3'] = None

# Configurar Google Gemini (API)
try:
    if Config.GOOGLE_API_KEY:
        os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
        models['gemini-1.5-flash'] = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=Config.DEFAULT_TEMPERATURE
        )
        print("‚úÖ Google Gemini 1.5 Flash configurado correctamente")
    else:
        print("‚ö†Ô∏è Google Gemini no disponible: No se encontr√≥ GOOGLE_API_KEY")
        models['gemini-1.5-flash'] = None
except Exception as e:
    print(f"‚ö†Ô∏è Google Gemini no disponible: {e}")
    models['gemini-1.5-flash'] = None

# Verificar que al menos un modelo est√© disponible
available_models = [k for k, v in models.items() if v is not None]
if not available_models:
    print("‚ùå Error: No hay modelos disponibles")
    print("üí° Aseg√∫rate de que:")
    print("   - Ollama est√© ejecut√°ndose con llama3 instalado, O")
    print("   - Tengas GOOGLE_API_KEY configurada en .env")
    exit(1)

print(f"üéØ Modelos disponibles: {', '.join(available_models)}")

# Funci√≥n para obtener el modelo seg√∫n la selecci√≥n
def get_model(model_name: str) -> Optional[Union[ChatOllama, ChatGoogleGenerativeAI]]:
    """Obtiene el modelo solicitado o el primero disponible como fallback"""
    if model_name in models and models[model_name] is not None:
        return models[model_name]
    
    # Fallback al primer modelo disponible
    for model_key, model_instance in models.items():
        if model_instance is not None:
            print(f"‚ö†Ô∏è Usando {model_key} como fallback")
            return model_instance
    
    return None

# Configurar herramientas para el agente
tools = [DuckDuckGoSearchRun()]

# Prompt optimizado para b√∫squedas eficientes
agent_prompt = PromptTemplate.from_template("""
Eres un asistente de IA especializado en proporcionar respuestas precisas y actuales. Tienes acceso a herramientas de b√∫squeda web.

Herramientas disponibles:
{tools}

REGLAS IMPORTANTES: 
1. SIEMPRE usa b√∫squeda web para: clima actual, noticias recientes, precios actuales, eventos deportivos
2. Para clima, busca t√©rminos espec√≠ficos como: "weather [ciudad] today current" o "clima actual [ciudad]"
3. NUNCA digas que no tienes acceso a informaci√≥n en tiempo real - ¬°S√ç LO TIENES!
4. Si una b√∫squeda falla, intenta con t√©rminos diferentes
5. Proporciona respuestas √∫tiles basadas en los resultados obtenidos

Formato OBLIGATORIO:
Question: la pregunta que debes responder
Thought: necesito buscar informaci√≥n actual sobre [tema espec√≠fico]
Action: duckduckgo_search
Action Input: [t√©rminos de b√∫squeda espec√≠ficos y efectivos]
Observation: [resultados de la b√∫squeda]
Thought: ahora tengo informaci√≥n actual y puedo responder
Final Answer: [respuesta completa en espa√±ol basada en los resultados]

Pregunta: {input}
Thought:{agent_scratchpad}
""")

# Crear el agente con manejo de errores
agents = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        try:
            agent = create_react_agent(model_instance, tools, agent_prompt)
            agents[model_name] = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                max_iterations=6,  # Aumentado para permitir m√°s b√∫squedas
                max_execution_time=30,  # L√≠mite de tiempo de 30 segundos
                handle_parsing_errors=True,
                return_intermediate_steps=True
            )
            print(f"‚úÖ Agente {model_name} creado correctamente")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creando agente {model_name}: {e}")
            agents[model_name] = None

# Tambi√©n configurar un chat simple sin agente para preguntas b√°sicas
simple_chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente √∫til y amigable. Responde de manera clara y concisa en espa√±ol. Siempre mant√©n un tono profesional pero cercano."),
    ("user", "{pregunta}")
])

simple_chains = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        simple_chains[model_name] = simple_chat_prompt | model_instance | StrOutputParser()
        print(f"‚úÖ Chat simple {model_name} configurado")

@app.route('/')
def index() -> str:
    return render_template('index.html', modelos_disponibles=available_models)

@app.route('/chat', methods=['POST'])
def chat() -> Union[Response, Tuple[Response, int]]:
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modo = data.get('modo', 'simple')  # 'simple' o 'agente'
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Obtener el modelo seleccionado
        modelo = get_model(modelo_seleccionado)
        if modelo is None:
            return jsonify({'error': 'No hay modelos disponibles'}), 500
        
        if modo == 'agente' and modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            # Usar el agente para preguntas que puedan requerir b√∫squeda web
            try:
                respuesta = agents[modelo_seleccionado].invoke({"input": pregunta})
                return jsonify({
                    'respuesta': respuesta['output'],
                    'modo': 'agente',
                    'modelo_usado': modelo_seleccionado
                })
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente {modelo_seleccionado}: {error_msg}")
                
                # Manejo espec√≠fico para diferentes tipos de errores
                if "iteration limit" in error_msg.lower() or "time limit" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è La b√∫squeda tom√≥ m√°s tiempo del esperado. Intentando respuesta r√°pida...\n\n"
                elif "parsing" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è Hubo un problema procesando la b√∫squeda. Usando respuesta directa...\n\n"
                else:
                    fallback_msg = f"‚ö†Ô∏è Error en b√∫squeda web. Usando modo simple...\n\n"
                
                # Fallback a chat simple si el agente falla
                if modelo_seleccionado in simple_chains:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    return jsonify({
                        'respuesta': f"{fallback_msg}{respuesta}",
                        'modo': 'simple_fallback',
                        'modelo_usado': modelo_seleccionado
                    })
                else:
                    return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible para fallback'}), 500
        else:
            # Usar chat simple
            if modelo_seleccionado in simple_chains:
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                return jsonify({
                    'respuesta': respuesta,
                    'modo': 'simple',
                    'modelo_usado': modelo_seleccionado
                })
            else:
                return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error al procesar la pregunta: {str(e)}'}), 500

@app.route('/ejemplo-agente', methods=['POST'])
def ejemplo_agente() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para demostrar capacidades del agente"""
    try:
        ejemplos = [
            "¬øCu√°l es el clima actual en Quito, Ecuador?",
            "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            "¬øCu√°l es el precio actual del Bitcoin?"
        ]
        
        pregunta_ejemplo = ejemplos[0]  # Tomar el primer ejemplo
        
        # Usar el primer modelo y agente disponible
        modelo_disponible = available_models[0] if available_models else None
        
        if modelo_disponible and modelo_disponible in agents and agents[modelo_disponible] is not None:
            # Usar el agente para la demo
            respuesta = agents[modelo_disponible].invoke({"input": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': respuesta['output'],
                'modo': 'agente',
                'modelo_usado': modelo_disponible
            })
        elif modelo_disponible and modelo_disponible in simple_chains:
            # Fallback: usar chat simple si el agente no est√° disponible
            respuesta_simple = simple_chains[modelo_disponible].invoke({"pregunta": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': f"[Modo Simple - Agente no disponible] {respuesta_simple}",
                'modo': 'simple',
                'modelo_usado': modelo_disponible
            })
        else:
            return jsonify({'error': 'No hay modelos disponibles para la demo'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en ejemplo de agente: {str(e)}'}), 500

@app.route('/busqueda-rapida', methods=['POST'])
def busqueda_rapida() -> Union[Response, Tuple[Response, int]]:
    """Endpoint optimizado para b√∫squedas web r√°pidas"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # B√∫squeda directa con DuckDuckGo sin agente complejo
        search_tool = DuckDuckGoSearchRun()
        
        # Mejorar t√©rminos de b√∫squeda seg√∫n el tipo de pregunta
        def mejorar_consulta_busqueda(pregunta_original):
            pregunta_lower = pregunta_original.lower()
            if 'clima' in pregunta_lower or 'weather' in pregunta_lower:
                if 'quito' in pregunta_lower:
                    return [
                        "weather Quito Ecuador today current",
                        "clima actual Quito Ecuador hoy",
                        "Quito weather forecast today"
                    ]
            elif 'precio' in pregunta_lower and 'bitcoin' in pregunta_lower:
                return [
                    "Bitcoin price today USD current",
                    "precio Bitcoin actual USD"
                ]
            elif 'noticias' in pregunta_lower:
                return [
                    f"{pregunta_original} √∫ltimas 24 horas",
                    f"latest news {pregunta_original}"
                ]
            
            # B√∫squeda gen√©rica mejorada
            return [pregunta_original, f"{pregunta_original} 2025"]
        
        try:
            consultas = mejorar_consulta_busqueda(pregunta)
            search_results = None
            consulta_exitosa = None
            
            # Intentar m√∫ltiples consultas hasta obtener resultados √∫tiles
            for consulta in consultas:
                try:
                    print(f"üîç Buscando: {consulta}")
                    resultado = search_tool.invoke(consulta)
                    
                    # Verificar si el resultado tiene contenido √∫til
                    if resultado and len(resultado.strip()) > 50 and not "No se encontraron resultados" in resultado:
                        search_results = resultado
                        consulta_exitosa = consulta
                        print(f"‚úÖ B√∫squeda exitosa con: {consulta}")
                        break
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en b√∫squeda '{consulta}': {e}")
                    continue
            
            if not search_results:
                return jsonify({
                    'respuesta': "‚ùå No pude obtener informaci√≥n actualizada desde internet en este momento. Esto puede deberse a limitaciones temporales del servicio de b√∫squeda. Te recomiendo:\n\n1. Consultar directamente sitios como Google Weather, AccuWeather o Weather.com\n2. Usar aplicaciones m√≥viles de clima\n3. Intentar la b√∫squeda m√°s tarde",
                    'modo': 'busqueda_fallback',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'Error DuckDuckGo'
                })
            
            # Usar el modelo para resumir y formatear los resultados
            modelo = get_model(modelo_seleccionado)
            if modelo and modelo_seleccionado in simple_chains:
                prompt_busqueda = f"""
                INSTRUCCIONES: Eres un asistente experto que SIEMPRE debe proporcionar informaci√≥n √∫til basada en los resultados de b√∫squeda.

                PREGUNTA DEL USUARIO: "{pregunta}"
                
                RESULTADOS DE B√öSQUEDA OBTENIDOS:
                {search_results}

                IMPORTANTE:
                1. NUNCA digas que no tienes acceso a informaci√≥n en tiempo real
                2. SIEMPRE analiza y extrae informaci√≥n √∫til de los resultados
                3. Si hay datos espec√≠ficos (temperatura, precios, fechas), incl√∫yelos
                4. Estructura la respuesta de forma clara y √∫til
                5. Si los resultados no son perfectos, extrae lo que sea √∫til y adm√≠telo
                
                RESPUESTA REQUERIDA (en espa√±ol):
                """
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_busqueda})
                
                return jsonify({
                    'respuesta': f"üîç **B√∫squeda realizada con:** {consulta_exitosa}\n\n{respuesta}",
                    'modo': 'busqueda_rapida',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'DuckDuckGo'
                })
            else:
                return jsonify({
                    'respuesta': f"**Resultados de b√∫squeda directa:**\n\n{search_results}",
                    'modo': 'busqueda_directa',
                    'modelo_usado': 'ninguno',
                    'fuente': 'DuckDuckGo'
                })
                
        except Exception as search_error:
            return jsonify({'error': f'Error en b√∫squeda: {str(search_error)}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Error en b√∫squeda r√°pida: {str(e)}'}), 500

@app.route('/clima-directo', methods=['POST'])
def clima_directo() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para consultas de clima con respuesta directa inteligente"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Usar el modelo para generar una respuesta √∫til sobre clima
        modelo = get_model(modelo_seleccionado)
        if modelo and modelo_seleccionado in simple_chains:
            prompt_clima = f"""
            El usuario pregunta sobre el clima en {ciudad}. Aunque no puedo acceder a datos meteorol√≥gicos en tiempo real desde mi entrenamiento, puedo proporcionar informaci√≥n √∫til y recomendaciones pr√°cticas.

            Pregunta del usuario: "{pregunta}"

            Proporciona una respuesta √∫til que incluya:
            1. Reconocimiento de la limitaci√≥n para datos en tiempo real
            2. Informaci√≥n general sobre el clima t√≠pico de {ciudad} seg√∫n la √©poca del a√±o (es julio 2025)
            3. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
               - Sitios web espec√≠ficos (weather.com, accuweather.com, clima.com)
               - Aplicaciones m√≥viles recomendadas
               - B√∫squedas espec√≠ficas en Google
            4. Consejos pr√°cticos para el clima t√≠pico de la regi√≥n en esta √©poca

            S√© √∫til, espec√≠fico y pr√°ctico en tu respuesta.
            """
            
            respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            
            return jsonify({
                'respuesta': f"üå§Ô∏è **Informaci√≥n sobre clima en {ciudad}**\n\n{respuesta}",
                'modo': 'clima_directo',
                'modelo_usado': modelo_seleccionado,
                'fuente': 'Respuesta inteligente'
            })
        else:
            return jsonify({
                'respuesta': f"Para obtener el clima actual en {ciudad}, te recomiendo consultar:\n‚Ä¢ weather.com\n‚Ä¢ accuweather.com\n‚Ä¢ Google Weather\n‚Ä¢ Apps m√≥viles de clima",
                'modo': 'clima_basico',
                'modelo_usado': 'ninguno',
                'fuente': 'Recomendaciones est√°ticas'
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

if __name__ == '__main__':
    print("ü§ñ Iniciando aplicaci√≥n de IA con Agentes...")
    print(f"üß† Modelos disponibles: {', '.join(available_models)}")
    print("üîç Herramientas: B√∫squeda web con DuckDuckGo")
    print("üåê Servidor: http://127.0.0.1:5000")
    print("üöÄ Aplicaci√≥n multi-modelo lista!")
    app.run(debug=True, host='127.0.0.1', port=5000)
