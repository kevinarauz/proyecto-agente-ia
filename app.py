import os
import json
import requests
from typing import Optional, Union, Tuple
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain.tools import BaseTool
from langchain_core.tools import Tool
from config import Config

# Cargar variables de entorno
load_dotenv()

# Cargar variables de entorno
load_dotenv()

app = Flask(__name__)
app.config.from_object(Config)

print("ü§ñ Configurando modelos de IA...")

# Configurar ambos modelos de IA
models = {}

# Configurar Llama3 (local)
try:
    models['llama3'] = ChatOllama(
        model="llama3",
        temperature=Config.DEFAULT_TEMPERATURE
    )
    print("‚úÖ Llama3 configurado correctamente")
except Exception as e:
    print(f"‚ö†Ô∏è Llama3 no disponible: {e}")
    models['llama3'] = None

# Configurar Google Gemini (API)
try:
    if Config.GOOGLE_API_KEY:
        os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
        models['gemini-1.5-flash'] = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash"
        )
        print("‚úÖ Google Gemini 1.5 Flash configurado correctamente")
    else:
        print("‚ö†Ô∏è Google Gemini no disponible: No se encontr√≥ GOOGLE_API_KEY")
        models['gemini-1.5-flash'] = None
except Exception as e:
    print(f"‚ö†Ô∏è Google Gemini no disponible: {e}")
    models['gemini-1.5-flash'] = None

# Verificar que al menos un modelo est√© disponible
available_models = [k for k, v in models.items() if v is not None]
if not available_models:
    print("‚ùå Error: No hay modelos disponibles")
    print("üí° Aseg√∫rate de que:")
    print("   - Ollama est√© ejecut√°ndose con llama3 instalado, O")
    print("   - Tengas GOOGLE_API_KEY configurada en .env")
    exit(1)

print(f"üéØ Modelos disponibles: {', '.join(available_models)}")

# Funci√≥n para obtener el modelo seg√∫n la selecci√≥n
def get_model(model_name: str) -> Optional[Union[ChatOllama, ChatGoogleGenerativeAI]]:
    """Obtiene el modelo solicitado o el primero disponible como fallback"""
    if model_name in models and models[model_name] is not None:
        return models[model_name]
    
    # Fallback al primer modelo disponible
    for model_key, model_instance in models.items():
        if model_instance is not None:
            print(f"‚ö†Ô∏è Usando {model_key} como fallback")
            return model_instance
    
    return None

# Funci√≥n para obtener clima usando API gratuita
def obtener_clima_api(ciudad: str = "Quito") -> dict:
    """Obtiene informaci√≥n del clima usando API gratuita de wttr.in"""
    try:
        # API gratuita que no requiere clave
        url = f"https://wttr.in/{ciudad}?format=j1"
        
        print(f"üåê Consultando API de clima para {ciudad}...")
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer informaci√≥n relevante
            current = data.get('current_condition', [{}])[0]
            weather_info = {
                'temperatura': current.get('temp_C', 'N/A'),
                'descripcion': current.get('weatherDesc', [{}])[0].get('value', 'N/A'),
                'humedad': current.get('humidity', 'N/A'),
                'sensacion_termica': current.get('FeelsLikeC', 'N/A'),
                'velocidad_viento': current.get('windspeedKmph', 'N/A'),
                'direccion_viento': current.get('winddir16Point', 'N/A'),
                'hora_consulta': current.get('observation_time', 'N/A'),
                'ciudad': ciudad
            }
            
            print(f"‚úÖ Clima obtenido: {weather_info['temperatura']}¬∞C en {ciudad}")
            return {'success': True, 'data': weather_info}
            
        else:
            print(f"‚ö†Ô∏è Error API clima: {response.status_code}")
            return {'success': False, 'error': f'Error de API: {response.status_code}'}
            
    except requests.exceptions.Timeout:
        print("‚ö†Ô∏è Timeout al consultar API de clima")
        return {'success': False, 'error': 'Timeout en consulta'}
    except Exception as e:
        print(f"‚ö†Ô∏è Error consultando API de clima: {e}")
        return {'success': False, 'error': str(e)}

# Funci√≥n para b√∫squeda web avanzada usando m√∫ltiples APIs
def busqueda_web_avanzada(query: str) -> str:
    """B√∫squeda web usando m√∫ltiples m√©todos para mayor confiabilidad"""
    print(f"üîç B√∫squeda web avanzada: {query}")
    
    resultados = []
    
    # M√©todo 1: DuckDuckGo (original)
    try:
        ddg_search = DuckDuckGoSearchRun()
        resultado_ddg = ddg_search.invoke(query)
        if resultado_ddg and len(resultado_ddg.strip()) > 30:
            resultados.append(f"[DuckDuckGo] {resultado_ddg}")
            print("‚úÖ DuckDuckGo: Resultados obtenidos")
        else:
            print("‚ö†Ô∏è DuckDuckGo: Sin resultados √∫tiles")
    except Exception as e:
        print(f"‚ö†Ô∏è DuckDuckGo fall√≥: {e}")
    
    # M√©todo 2: API de b√∫squeda alternativa (usando scraping b√°sico)
    try:
        # Usar una API p√∫blica de b√∫squeda o scraping b√°sico
        url = f"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer resultados de la API
            abstract = data.get('Abstract', '')
            answer = data.get('Answer', '')
            
            if abstract:
                resultados.append(f"[API Abstract] {abstract}")
                print("‚úÖ API DuckDuckGo: Abstract obtenido")
            
            if answer:
                resultados.append(f"[API Answer] {answer}")
                print("‚úÖ API DuckDuckGo: Answer obtenido")
                
    except Exception as e:
        print(f"‚ö†Ô∏è API DuckDuckGo fall√≥: {e}")
    
    # M√©todo 3: Para clima espec√≠fico, usar nuestra API
    if any(palabra in query.lower() for palabra in ['clima', 'weather', 'temperatura', 'temperature']):
        try:
            # Extraer ciudad de la consulta
            ciudad = 'Quito'  # Default
            if 'quito' in query.lower():
                ciudad = 'Quito'
            elif 'guayaquil' in query.lower():
                ciudad = 'Guayaquil'
            elif 'cuenca' in query.lower():
                ciudad = 'Cuenca'
            elif 'new york' in query.lower() or 'nueva york' in query.lower():
                ciudad = 'New York'
            elif 'london' in query.lower() or 'londres' in query.lower():
                ciudad = 'London'
            elif 'madrid' in query.lower():
                ciudad = 'Madrid'
            
            clima_data = obtener_clima_api(ciudad)
            if clima_data['success']:
                data = clima_data['data']
                clima_info = f"Clima actual en {ciudad}: {data['temperatura']}¬∞C, {data['descripcion']}, Humedad: {data['humedad']}%, Viento: {data['velocidad_viento']} km/h"
                resultados.append(f"[Clima API] {clima_info}")
                print(f"‚úÖ Clima API: Datos obtenidos para {ciudad}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è API Clima fall√≥: {e}")
    
    # Compilar resultados
    if resultados:
        resultado_final = "\n\n".join(resultados)
        print(f"‚úÖ B√∫squeda completada con {len(resultados)} fuentes")
        return resultado_final
    else:
        print("‚ùå No se obtuvieron resultados de ninguna fuente")
        return f"No se pudo obtener informaci√≥n actualizada sobre '{query}'. Se recomienda consultar fuentes directas como Google, sitios web oficiales o aplicaciones especializadas."

# Crear herramienta personalizada para b√∫squeda web
def crear_herramienta_busqueda():
    """Crea una herramienta de b√∫squeda web personalizada"""
    return Tool(
        name="web_search",
        description="Busca informaci√≥n actual en internet sobre cualquier tema. √ötil para noticias, precios, eventos actuales, etc. Input debe ser una consulta de b√∫squeda espec√≠fica.",
        func=busqueda_web_avanzada
    )

def formatear_respuesta_clima(clima_data: dict, modelo_seleccionado: str) -> str:
    """Formatea la respuesta del clima de manera atractiva"""
    if clima_data['success']:
        data = clima_data['data']
        return f"""üå§Ô∏è **Clima actual en {data['ciudad']}**

üìä **Condiciones actuales:**
‚Ä¢ **Temperatura:** {data['temperatura']}¬∞C
‚Ä¢ **Sensaci√≥n t√©rmica:** {data['sensacion_termica']}¬∞C
‚Ä¢ **Condiciones:** {data['descripcion']}
‚Ä¢ **Humedad:** {data['humedad']}%
‚Ä¢ **Viento:** {data['velocidad_viento']} km/h ({data['direccion_viento']})
‚Ä¢ **√öltima actualizaci√≥n:** {data['hora_consulta']}

üèîÔ∏è **Informaci√≥n contextual:**
Quito se encuentra a 2,850 metros sobre el nivel del mar, lo que influye en su clima templado durante todo el a√±o. Las temperaturas suelen oscilar entre 10¬∞C y 25¬∞C.

üì± **Para m√°s detalles:**
‚Ä¢ Pron√≥stico extendido: [wttr.in/Quito](https://wttr.in/Quito)
‚Ä¢ Apps recomendadas: AccuWeather, Weather Underground
‚Ä¢ Sitios web: weather.com, tiempo.com

*Datos obtenidos de API meteorol√≥gica en tiempo real*"""
    else:
        return f"""‚ùå **No se pudo obtener el clima actual**

Error: {clima_data['error']}

üå§Ô∏è **Informaci√≥n general sobre Quito:**
Quito tiene un clima subtropical de monta√±a con temperaturas relativamente estables durante todo el a√±o:
‚Ä¢ **D√≠a:** 18-24¬∞C
‚Ä¢ **Noche:** 8-15¬∞C
‚Ä¢ **Julio:** Estaci√≥n seca, d√≠as soleados y noches frescas

üì± **Consulta informaci√≥n actualizada en:**
‚Ä¢ Google: "clima Quito Ecuador"
‚Ä¢ AccuWeather.com
‚Ä¢ Weather.com
‚Ä¢ Apps m√≥viles de clima"""

# Configurar herramientas para el agente
herramienta_busqueda = crear_herramienta_busqueda()
tools = [herramienta_busqueda, DuckDuckGoSearchRun()]

# Prompt optimizado para b√∫squedas generales
agent_prompt = PromptTemplate.from_template("""
Eres un asistente de IA especializado en proporcionar respuestas precisas y actuales. Tienes acceso a herramientas de b√∫squeda web.

Herramientas disponibles:
{tools}

REGLAS IMPORTANTES: 
1. SIEMPRE usa b√∫squeda web para: noticias recientes, precios actuales, eventos deportivos, informaci√≥n actualizada
2. NUNCA digas que no tienes acceso a informaci√≥n en tiempo real - ¬°S√ç LO TIENES!
3. Si una b√∫squeda falla, intenta con t√©rminos diferentes
4. Proporciona respuestas √∫tiles basadas en los resultados obtenidos
5. Adapta tu b√∫squeda seg√∫n el tipo de informaci√≥n solicitada

Formato OBLIGATORIO:
Question: la pregunta que debes responder
Thought: necesito buscar informaci√≥n actual sobre [tema espec√≠fico]
Action: duckduckgo_search
Action Input: [t√©rminos de b√∫squeda espec√≠ficos y efectivos]
Observation: [resultados de la b√∫squeda]
Thought: ahora tengo informaci√≥n actual y puedo responder
Final Answer: [respuesta completa en espa√±ol basada en los resultados]

Pregunta: {input}
Thought:{agent_scratchpad}
""")

# Crear el agente con manejo de errores y herramientas mejoradas
agents = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        try:
            # Usar el prompt est√°ndar de React que funciona
            react_prompt = hub.pull("hwchase17/react")
            
            agent = create_react_agent(model_instance, tools, react_prompt)
            agents[model_name] = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                max_iterations=4,  # Reducido porque ahora es m√°s eficiente
                max_execution_time=20,  # Reducido
                handle_parsing_errors=True,
                return_intermediate_steps=True
            )
            print(f"‚úÖ Agente {model_name} creado con herramientas avanzadas")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creando agente {model_name}: {e}")
            agents[model_name] = None

# Tambi√©n configurar un chat simple sin agente para preguntas b√°sicas
simple_chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente √∫til y amigable. Responde de manera clara y concisa en espa√±ol. Siempre mant√©n un tono profesional pero cercano."),
    ("user", "{pregunta}")
])

simple_chains = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        simple_chains[model_name] = simple_chat_prompt | model_instance | StrOutputParser()
        print(f"‚úÖ Chat simple {model_name} configurado")

@app.route('/')
def index() -> str:
    return render_template('index.html', modelos_disponibles=available_models)

@app.route('/chat', methods=['POST'])
def chat() -> Union[Response, Tuple[Response, int]]:
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modo = data.get('modo', 'simple')  # 'simple' o 'agente'
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Obtener el modelo seleccionado
        modelo = get_model(modelo_seleccionado)
        if modelo is None:
            return jsonify({'error': 'No hay modelos disponibles'}), 500
        
        if modo == 'agente' and modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            # Usar el agente para preguntas que puedan requerir b√∫squeda web
            try:
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                if 'intermediate_steps' in respuesta_completa:
                    for paso in respuesta_completa['intermediate_steps']:
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            pasos_intermedios.append({
                                'action': accion.tool,
                                'action_input': accion.tool_input,
                                'observation': observacion[:200] + '...' if len(observacion) > 200 else observacion
                            })
                
                return jsonify({
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente',
                    'modelo_usado': modelo_seleccionado,
                    'pasos_intermedios': pasos_intermedios
                })
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente {modelo_seleccionado}: {error_msg}")
                
                # Manejo espec√≠fico para diferentes tipos de errores
                if "iteration limit" in error_msg.lower() or "time limit" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è La b√∫squeda tom√≥ m√°s tiempo del esperado. Intentando respuesta r√°pida...\n\n"
                elif "parsing" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è Hubo un problema procesando la b√∫squeda. Usando respuesta directa...\n\n"
                else:
                    fallback_msg = f"‚ö†Ô∏è Error en b√∫squeda web. Usando modo simple...\n\n"
                
                # Fallback a chat simple si el agente falla
                if modelo_seleccionado in simple_chains:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    return jsonify({
                        'respuesta': f"{fallback_msg}{respuesta}",
                        'modo': 'simple_fallback',
                        'modelo_usado': modelo_seleccionado
                    })
                else:
                    return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible para fallback'}), 500
        else:
            # Usar chat simple
            if modelo_seleccionado in simple_chains:
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                return jsonify({
                    'respuesta': respuesta,
                    'modo': 'simple',
                    'modelo_usado': modelo_seleccionado
                })
            else:
                return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error al procesar la pregunta: {str(e)}'}), 500

@app.route('/ejemplo-agente', methods=['POST'])
def ejemplo_agente() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para demostrar capacidades del agente"""
    try:
        ejemplos = [
            "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            "¬øCu√°l es el precio actual del Bitcoin?",
            "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            "¬øCu√°les son las √∫ltimas noticias tecnol√≥gicas?"
        ]
        
        pregunta_ejemplo = ejemplos[0]  # Tomar el primer ejemplo
        
        # Usar el primer modelo y agente disponible
        modelo_disponible = available_models[0] if available_models else None
        
        if modelo_disponible and modelo_disponible in agents and agents[modelo_disponible] is not None:
            # Usar el agente para la demo
            respuesta = agents[modelo_disponible].invoke({"input": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': respuesta['output'],
                'modo': 'agente',
                'modelo_usado': modelo_disponible
            })
        elif modelo_disponible and modelo_disponible in simple_chains:
            # Fallback: usar chat simple si el agente no est√° disponible
            respuesta_simple = simple_chains[modelo_disponible].invoke({"pregunta": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': f"[Modo Simple - Agente no disponible] {respuesta_simple}",
                'modo': 'simple',
                'modelo_usado': modelo_disponible
            })
        else:
            return jsonify({'error': 'No hay modelos disponibles para la demo'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en ejemplo de agente: {str(e)}'}), 500

@app.route('/busqueda-rapida', methods=['POST'])
def busqueda_rapida() -> Union[Response, Tuple[Response, int]]:
    """Endpoint optimizado para b√∫squedas web r√°pidas"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # B√∫squeda directa con DuckDuckGo sin agente complejo
        search_tool = DuckDuckGoSearchRun()
        
        # Mejorar t√©rminos de b√∫squeda seg√∫n el tipo de pregunta
        def mejorar_consulta_busqueda(pregunta_original):
            pregunta_lower = pregunta_original.lower()
            if 'precio' in pregunta_lower and 'bitcoin' in pregunta_lower:
                return [
                    "Bitcoin price USD current today",
                    "precio Bitcoin actual d√≥lares",
                    "BTC price now current value"
                ]
            elif 'noticias' in pregunta_lower:
                return [
                    f"noticias {pregunta_original} hoy",
                    f"latest news {pregunta_original} today",
                    f"breaking news {pregunta_original} 2025"
                ]
            elif 'openai' in pregunta_lower:
                return [
                    "OpenAI news latest updates",
                    "noticias OpenAI ChatGPT",
                    "OpenAI developments 2025"
                ]
            elif 'champions' in pregunta_lower or 'deportes' in pregunta_lower:
                return [
                    "Champions League final winner",
                    "latest sports news football",
                    "resultados deportivos recientes"
                ]
            elif 'petr√≥leo' in pregunta_lower or 'oil' in pregunta_lower:
                return [
                    "oil price current USD barrel",
                    "precio petr√≥leo actual",
                    "crude oil price today"
                ]
            
            # B√∫squeda gen√©rica mejorada
            return [pregunta_original, f"{pregunta_original} today", f"{pregunta_original} 2025"]
        
        try:
            consultas = mejorar_consulta_busqueda(pregunta)
            search_results = None
            consulta_exitosa = None
            
            # Intentar m√∫ltiples consultas hasta obtener resultados √∫tiles
            for consulta in consultas:
                try:
                    print(f"üîç Buscando: {consulta}")
                    resultado = search_tool.invoke(consulta)
                    
                    # Verificar si el resultado tiene contenido √∫til
                    if resultado and len(resultado.strip()) > 50:
                        # Verificaciones adicionales para detectar resultados irrelevantes
                        resultado_lower = resultado.lower()
                        palabras_irrelevantes = [
                            "no se encontraron resultados",
                            "p√°gina no encontrada", 
                            "error 404",
                            "no hay resultados",
                            "try again later"
                        ]
                        
                        # Verificar que no tenga palabras irrelevantes
                        es_irrelevante = any(palabra in resultado_lower for palabra in palabras_irrelevantes)
                        
                        if not es_irrelevante:
                            search_results = resultado
                            consulta_exitosa = consulta
                            print(f"‚úÖ B√∫squeda exitosa con: {consulta}")
                            break
                        else:
                            print(f"‚ö†Ô∏è Resultado irrelevante con: {consulta}")
                            
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en b√∫squeda '{consulta}': {e}")
                    continue
            
            if not search_results:
                # Fallback inteligente para cualquier consulta
                modelo = get_model(modelo_seleccionado)
                if modelo and modelo_seleccionado in simple_chains:
                    prompt_fallback = f"""
                    No pude obtener informaci√≥n actualizada de internet sobre "{pregunta}". 
                    
                    Como experto asistente, proporciona una respuesta √∫til que incluya:
                    
                    1. Informaci√≥n general relevante sobre el tema consultado
                    2. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
                       - Sitios web especializados
                       - Aplicaciones m√≥viles relevantes
                       - B√∫squedas espec√≠ficas en Google
                    3. Contexto √∫til sobre el tema
                    4. Consejos pr√°cticos para encontrar la informaci√≥n
                    
                    S√© espec√≠fico y √∫til, adapt√°ndote al tipo de consulta realizada.
                    """
                    
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_fallback})
                    
                    return jsonify({
                        'respuesta': f"üåê **B√∫squeda web limitada - Respuesta inteligente:**\n\n{respuesta_fallback}",
                        'modo': 'busqueda_inteligente_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Asistente inteligente'
                    })
                else:
                    return jsonify({
                        'respuesta': "‚ùå No pude obtener informaci√≥n actualizada desde internet en este momento. Te recomiendo:\n\n1. Consultar sitios web especializados\n2. Usar aplicaciones m√≥viles relevantes\n3. Buscar en Google con t√©rminos espec√≠ficos\n4. Intentar la b√∫squeda m√°s tarde",
                        'modo': 'busqueda_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Error en b√∫squeda'
                    })
            
            # Usar el modelo para resumir y formatear los resultados
            modelo = get_model(modelo_seleccionado)
            if modelo and modelo_seleccionado in simple_chains:
                prompt_busqueda = f"""
                INSTRUCCIONES ESPEC√çFICAS: Eres un experto asistente que debe extraer y presentar informaci√≥n √∫til de resultados de b√∫squeda web.

                PREGUNTA DEL USUARIO: "{pregunta}"
                
                RESULTADOS DE B√öSQUEDA OBTENIDOS:
                {search_results}

                TAREAS OBLIGATORIAS:
                1. ANALIZA cuidadosamente los resultados de b√∫squeda
                2. EXTRAE cualquier informaci√≥n relevante disponible (precios, noticias, datos espec√≠ficos)
                3. Si encuentras datos parciales o relacionados, √öSALOS y s√© transparente sobre las limitaciones
                4. COMPLEMENTA con informaci√≥n contextual √∫til y recomendaciones
                5. NUNCA digas simplemente "no hay informaci√≥n" - siempre proporciona valor

                ESTRUCTURA DE RESPUESTA:
                - Informaci√≥n encontrada (aunque sea limitada)
                - Contexto adicional √∫til
                - Recomendaciones para informaci√≥n m√°s completa
                - Consejos pr√°cticos

                IMPORTANTE: S√© proactivo y √∫til. Si los resultados mencionan sitios web o aplicaciones, incorp√≥ralos como recomendaciones adicionales.
                
                RESPUESTA EN ESPA√ëOL:
                """
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_busqueda})
                
                return jsonify({
                    'respuesta': f"üîç **B√∫squeda realizada con:** {consulta_exitosa}\n\n{respuesta}",
                    'modo': 'busqueda_rapida',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'DuckDuckGo'
                })
            else:
                return jsonify({
                    'respuesta': f"**Resultados de b√∫squeda directa:**\n\n{search_results}",
                    'modo': 'busqueda_directa',
                    'modelo_usado': 'ninguno',
                    'fuente': 'DuckDuckGo'
                })
                
        except Exception as search_error:
            return jsonify({'error': f'Error en b√∫squeda: {str(search_error)}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Error en b√∫squeda r√°pida: {str(e)}'}), 500

@app.route('/clima-directo', methods=['POST'])
def clima_directo() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para consultas de clima con respuesta directa inteligente"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Usar el modelo para generar una respuesta √∫til sobre clima
        modelo = get_model(modelo_seleccionado)
        if modelo and modelo_seleccionado in simple_chains:
            prompt_clima = f"""
            El usuario pregunta sobre el clima en {ciudad}. Aunque no puedo acceder a datos meteorol√≥gicos en tiempo real desde mi entrenamiento, puedo proporcionar informaci√≥n √∫til y recomendaciones pr√°cticas.

            Pregunta del usuario: "{pregunta}"

            Proporciona una respuesta √∫til que incluya:
            1. Reconocimiento de la limitaci√≥n para datos en tiempo real
            2. Informaci√≥n general sobre el clima t√≠pico de {ciudad} seg√∫n la √©poca del a√±o (es julio 2025)
            3. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
               - Sitios web espec√≠ficos (weather.com, accuweather.com, clima.com)
               - Aplicaciones m√≥viles recomendadas
               - B√∫squedas espec√≠ficas en Google
            4. Consejos pr√°cticos para el clima t√≠pico de la regi√≥n en esta √©poca

            S√© √∫til, espec√≠fico y pr√°ctico en tu respuesta.
            """
            
            respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            
            return jsonify({
                'respuesta': f"üå§Ô∏è **Informaci√≥n sobre clima en {ciudad}**\n\n{respuesta}",
                'modo': 'clima_directo',
                'modelo_usado': modelo_seleccionado,
                'fuente': 'Respuesta inteligente'
            })
        else:
            return jsonify({
                'respuesta': f"Para obtener el clima actual en {ciudad}, te recomiendo consultar:\n‚Ä¢ weather.com\n‚Ä¢ accuweather.com\n‚Ä¢ Google Weather\n‚Ä¢ Apps m√≥viles de clima",
                'modo': 'clima_basico',
                'modelo_usado': 'ninguno',
                'fuente': 'Recomendaciones est√°ticas'
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-api', methods=['POST'])
def clima_api() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para obtener clima usando API externa gratuita"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        # Extraer ciudad de la pregunta si est√° presente
        if 'quito' in pregunta.lower():
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta.lower():
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta.lower():
            ciudad = 'Cuenca'
        
        print(f"üå§Ô∏è Solicitando clima para {ciudad}...")
        
        # Obtener clima usando API
        clima_data = obtener_clima_api(ciudad)
        
        # Formatear respuesta
        respuesta_formateada = formatear_respuesta_clima(clima_data, modelo_seleccionado)
        
        return jsonify({
            'respuesta': respuesta_formateada,
            'modo': 'clima_api',
            'modelo_usado': modelo_seleccionado,
            'fuente': 'wttr.in API',
            'ciudad': ciudad,
            'success': clima_data['success']
        })
        
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima API: {str(e)}'}), 500

@app.route('/agente-general', methods=['POST'])
def agente_general() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para demostraciones del agente con b√∫squedas generales"""
    try:
        data = request.get_json()
        tipo_demo = data.get('tipo', 'clima')  # clima, noticias, bitcoin, deportes
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        # Preguntas predefinidas para diferentes tipos de demos
        demos = {
            'noticias': "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            'bitcoin': "¬øCu√°l es el precio actual del Bitcoin en USD?",
            'deportes': "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            'tech': "¬øCu√°les son las √∫ltimas noticias sobre OpenAI?",
            'economia': "¬øCu√°l es el precio actual del petr√≥leo?",
            'general': "¬øCu√°les son las noticias m√°s importantes de hoy?"
        }
        
        pregunta = demos.get(tipo_demo, demos['noticias'])
        
        # Usar el agente mejorado
        if modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            try:
                print(f"ü§ñ Ejecutando demo '{tipo_demo}' con {modelo_seleccionado}")
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                if 'intermediate_steps' in respuesta_completa:
                    for paso in respuesta_completa['intermediate_steps']:
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            pasos_intermedios.append({
                                'action': accion.tool,
                                'action_input': accion.tool_input,
                                'observation': observacion[:200] + '...' if len(observacion) > 200 else observacion
                            })
                
                return jsonify({
                    'pregunta': pregunta,
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente_general',
                    'modelo_usado': modelo_seleccionado,
                    'tipo_demo': tipo_demo,
                    'pasos_intermedios': pasos_intermedios
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente general {modelo_seleccionado}: {error_msg}")
                
                # Fallback inteligente
                if modelo_seleccionado in simple_chains:
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({
                        "pregunta": f"Aunque no puedo buscar en internet en este momento, responde de la mejor manera posible: {pregunta}"
                    })
                    
                    return jsonify({
                        'pregunta': pregunta,
                        'respuesta': f"‚ö†Ô∏è Agente no disponible. Respuesta b√°sica:\n\n{respuesta_fallback}",
                        'modo': 'fallback_general',
                        'modelo_usado': modelo_seleccionado,
                        'tipo_demo': tipo_demo
                    })
        
        return jsonify({'error': 'Agente no disponible'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en agente general: {str(e)}'}), 500

if __name__ == '__main__':
    print("ü§ñ Iniciando aplicaci√≥n de IA con Agentes...")
    print(f"üß† Modelos disponibles: {', '.join(available_models)}")
    print("üîç Herramientas: B√∫squeda web avanzada")
    print("üåê Servidor: http://127.0.0.1:5000")
    print("üöÄ Aplicaci√≥n de consultas generales lista!")
    app.run(debug=True, host='127.0.0.1', port=5000)
