import os
import json
import requests
import time
import io
import logging
import re
import traceback
from typing import Optional, Union, Tuple
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv

# Importar ChatOllama de la nueva ubicaci√≥n (si est√° disponible), sino usar la anterior
try:
    from langchain_ollama import ChatOllama
    print("‚úÖ Usando langchain_ollama (versi√≥n actualizada)")
except ImportError:
    from langchain_community.chat_models import ChatOllama
    print("‚ö†Ô∏è Usando langchain_community.chat_models (versi√≥n deprecada)")

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain.tools import BaseTool
from langchain_core.tools import Tool
from config import Config

# Definir ChatLMStudio directamente aqu√≠ para evitar problemas de importaci√≥n
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import Any, List, Optional
from pydantic import Field
import requests

class ChatLMStudio(BaseChatModel):
    """Chat model wrapper for LM Studio API."""
    
    # Declarar expl√≠citamente los campos que necesitamos
    model: str = Field(description="Nombre del modelo a usar")
    base_url: str = Field(default="http://localhost:1234", description="URL base del servidor LM Studio")
    temperature: float = Field(default=0.7, description="Temperatura para generaci√≥n")
    max_tokens: int = Field(default=1000, description="M√°ximo n√∫mero de tokens")
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate chat completion."""
        # Convert messages to API format
        api_messages = []
        
        # Combinar mensajes del sistema con el primer mensaje del usuario
        system_content = ""
        user_messages = []
        
        for msg in messages:
            if isinstance(msg, SystemMessage):
                # Acumular contenido del sistema, asegur√°ndonos de que sea string
                content = msg.content if isinstance(msg.content, str) else str(msg.content)
                system_content += content + "\n\n"
            elif isinstance(msg, HumanMessage):
                user_messages.append(msg)
            elif isinstance(msg, AIMessage):
                content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "assistant", "content": content})
        
        # Si hay contenido del sistema, combinarlo con el primer mensaje del usuario
        if system_content and user_messages:
            first_user_msg = user_messages[0]
            first_content = first_user_msg.content if isinstance(first_user_msg.content, str) else str(first_user_msg.content)
            combined_content = f"{system_content.strip()}\n\nUsuario: {first_content}"
            api_messages.insert(0, {"role": "user", "content": combined_content})
            
            # Agregar los mensajes restantes del usuario
            for msg in user_messages[1:]:
                user_content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "user", "content": user_content})
        else:
            # Si no hay sistema, agregar mensajes del usuario normalmente
            for msg in user_messages:
                user_content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "user", "content": user_content})
        
        # Asegurar que no hay mensajes vac√≠os
        if not api_messages:
            api_messages = [{"role": "user", "content": "Hola"}]
        
        # Make API request
        response = requests.post(
            f"{self.base_url}/v1/chat/completions",
            json={
                "model": self.model,
                "messages": api_messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": False
            },
            headers={"Content-Type": "application/json"},
            timeout=300  # Aumentar timeout a 5 minutos para modelos lentos
        )
        
        if response.status_code != 200:
            raise ValueError(f"LM Studio API error: {response.status_code} {response.text}")
        
        result = response.json()
        content = result["choices"][0]["message"]["content"]
        
        # Return result
        message = AIMessage(content=content)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    @property
    def _llm_type(self) -> str:
        return "lm-studio-api"

def generar_respuesta_fallback(pregunta: str) -> str:
    """Genera una respuesta de fallback simple para casos de timeout"""
    pregunta_lower = pregunta.lower()
    
    if 'java' in pregunta_lower:
        return """Java es un lenguaje de programaci√≥n orientado a objetos desarrollado por Oracle. 
Sus caracter√≠sticas principales son: multiplataforma (JVM), orientado a objetos, robusto y seguro. 
Se usa principalmente en aplicaciones empresariales, Android y desarrollo web."""
    elif 'python' in pregunta_lower:
        return """Python es un lenguaje de programaci√≥n interpretado y de alto nivel. 
Es conocido por su sintaxis simple, versatilidad y amplia comunidad. 
Se usa en desarrollo web, ciencia de datos, IA, automatizaci√≥n y m√°s."""
    elif 'javascript' in pregunta_lower:
        return """JavaScript es un lenguaje de programaci√≥n interpretado usado principalmente para desarrollo web. 
Permite crear interactividad en p√°ginas web y tambi√©n se usa en el backend con Node.js."""
    else:
        return f"""Disculpa, el modelo tard√≥ demasiado en procesar tu consulta sobre "{pregunta}". 
Por favor, intenta reformular la pregunta de manera m√°s espec√≠fica o prueba con otro modelo."""

# Cargar variables de entorno
load_dotenv()

app = Flask(__name__)
app.config.from_object(Config)

def formatear_duracion(segundos):
    """Convierte segundos a formato legible (minutos y segundos)"""
    if segundos < 1:
        return f"{round(segundos * 1000)}ms"
    elif segundos < 60:
        return f"{round(segundos, 1)}s"
    else:
        minutos = int(segundos // 60)
        segundos_restantes = round(segundos % 60, 1)
        if segundos_restantes == 0:
            return f"{minutos}m"
        else:
            return f"{minutos}m {segundos_restantes}s"

print("ü§ñ Configurando modelos de IA...")

# Configurar modelos de IA
models = {}

# Configurar modelos de Ollama (locales)
ollama_models = [
    ('llama3', 'Llama3 8B'),
    ('deepseek-coder', 'DeepSeek Coder'),
    ('phi3', 'Microsoft Phi-3'),
    ('gemma:2b', 'Google Gemma 2B'),
    ('gemma3:4b', 'Google Gemma3 4B')
]

for model_key, model_name in ollama_models:
    try:
        models[model_key] = ChatOllama(
            model=model_key
        )
        print(f"‚úÖ {model_name} configurado correctamente")
    except Exception as e:
        print(f"‚ö†Ô∏è {model_name} no disponible: {e}")
        models[model_key] = None

# Configurar Google Gemini (API)
try:
    if Config.GOOGLE_API_KEY:
        os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
        models['gemini-1.5-flash'] = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash"
        )
        print("‚úÖ Google Gemini 1.5 Flash configurado correctamente")
    else:
        print("‚ö†Ô∏è Google Gemini no disponible: No se encontr√≥ GOOGLE_API_KEY")
        models['gemini-1.5-flash'] = None
except Exception as e:
    print(f"‚ö†Ô∏è Google Gemini no disponible: {e}")
    models['gemini-1.5-flash'] = None

# Configurar LM Studio (API local)
try:
    if Config.validate_lmstudio():
        # Configurar Google Gemma 3-12B
        models['lmstudio-gemma'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=Config.DEFAULT_TEMPERATURE,
            max_tokens=Config.MAX_TOKENS
        )
        print("‚úÖ LM Studio (Gemma 3-12B) configurado correctamente")
        
        # Configurar Mistral 7B
        models['lmstudio-mistral'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL_MISTRAL,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=Config.DEFAULT_TEMPERATURE,
            max_tokens=Config.MAX_TOKENS
        )
        print("‚úÖ LM Studio (Mistral 7B) configurado correctamente")
        
        # Configurar DeepSeek Coder
        models['lmstudio-deepseek'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL_DEEPSEEK,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=0.3,  # Temperatura m√°s baja para respuestas m√°s r√°pidas y directas
            max_tokens=800    # Reducir tokens para respuestas m√°s concisas
        )
        print("‚úÖ LM Studio (DeepSeek Coder) configurado correctamente")
    else:
        print("‚ö†Ô∏è LM Studio no disponible")
        models['lmstudio-gemma'] = None
        models['lmstudio-mistral'] = None
        models['lmstudio-deepseek'] = None
except Exception as e:
    print(f"‚ö†Ô∏è LM Studio no disponible: {e}")
    models['lmstudio-gemma'] = None
    models['lmstudio-mistral'] = None
    models['lmstudio-deepseek'] = None

# Verificar que al menos un modelo est√© disponible
available_models = [k for k, v in models.items() if v is not None]
if not available_models:
    print("‚ùå Error: No hay modelos disponibles")
    print("üí° Aseg√∫rate de que:")
    print("   - Ollama est√© ejecut√°ndose con llama3 instalado, O")
    print("   - Tengas GOOGLE_API_KEY configurada en .env, O")
    print("   - LM Studio est√© ejecut√°ndose con un modelo cargado")
    exit(1)

print(f"üéØ Modelos disponibles: {', '.join(available_models)}")

# Funci√≥n para obtener el modelo seg√∫n la selecci√≥n
def get_model(model_name: str) -> Optional[Union[ChatOllama, ChatGoogleGenerativeAI, ChatLMStudio]]:
    """Obtiene el modelo solicitado o el primero disponible como fallback"""
    if model_name in models and models[model_name] is not None:
        return models[model_name]
    
    # Fallback al primer modelo disponible
    for model_key, model_instance in models.items():
        if model_instance is not None:
            print(f"‚ö†Ô∏è Usando {model_key} como fallback")
            return model_instance
    
    return None

# Funci√≥n para obtener clima usando API gratuita
def obtener_clima_api(ciudad: str = "Quito") -> dict:
    """Obtiene informaci√≥n del clima usando API gratuita de wttr.in"""
    try:
        # API gratuita que no requiere clave
        url = f"https://wttr.in/{ciudad}?format=j1"
        
        print(f"üåê Consultando API de clima para {ciudad}...")
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer informaci√≥n relevante
            current = data.get('current_condition', [{}])[0]
            weather_info = {
                'temperatura': current.get('temp_C', 'N/A'),
                'descripcion': current.get('weatherDesc', [{}])[0].get('value', 'N/A'),
                'humedad': current.get('humidity', 'N/A'),
                'sensacion_termica': current.get('FeelsLikeC', 'N/A'),
                'velocidad_viento': current.get('windspeedKmph', 'N/A'),
                'direccion_viento': current.get('winddir16Point', 'N/A'),
                'hora_consulta': current.get('observation_time', 'N/A'),
                'ciudad': ciudad
            }
            
            print(f"‚úÖ Clima obtenido: {weather_info['temperatura']}¬∞C en {ciudad}")
            return {'success': True, 'data': weather_info}
            
        else:
            print(f"‚ö†Ô∏è Error API clima: {response.status_code}")
            return {'success': False, 'error': f'Error de API: {response.status_code}'}
            
    except requests.exceptions.Timeout:
        print("‚ö†Ô∏è Timeout al consultar API de clima")
        return {'success': False, 'error': 'Timeout en consulta'}
    except Exception as e:
        print(f"‚ö†Ô∏è Error consultando API de clima: {e}")
        return {'success': False, 'error': str(e)}

# Funci√≥n para b√∫squeda web avanzada usando m√∫ltiples APIs
def busqueda_web_avanzada(query: str) -> str:
    """B√∫squeda web usando m√∫ltiples m√©todos para mayor confiabilidad"""
    print(f"üîç B√∫squeda web avanzada: {query}")
    
    resultados = []
    
    # M√©todo 1: DuckDuckGo (original)
    try:
        ddg_search = DuckDuckGoSearchRun()
        resultado_ddg = ddg_search.invoke(query)
        if resultado_ddg and len(resultado_ddg.strip()) > 30:
            resultados.append(f"[DuckDuckGo] {resultado_ddg}")
            print("‚úÖ DuckDuckGo: Resultados obtenidos")
        else:
            print("‚ö†Ô∏è DuckDuckGo: Sin resultados √∫tiles")
    except Exception as e:
        print(f"‚ö†Ô∏è DuckDuckGo fall√≥: {e}")
    
    # M√©todo 2: Para noticias espec√≠ficas, usar t√©rminos m√°s espec√≠ficos
    if any(palabra in query.lower() for palabra in ['noticias', 'news', 'hoy', 'today', 'actualidad']):
        try:
            # Buscar noticias m√°s espec√≠ficas
            queries_noticias = [
                "noticias tecnolog√≠a inteligencia artificial hoy",
                "noticias Ecuador √∫ltimas",
                "breaking news today",
                "noticias mundo actualidad"
            ]
            
            for query_especifica in queries_noticias:
                try:
                    ddg_search = DuckDuckGoSearchRun()
                    resultado = ddg_search.invoke(query_especifica)
                    if resultado and len(resultado.strip()) > 30:
                        resultados.append(f"[Noticias {query_especifica}] {resultado[:500]}...")
                        print(f"‚úÖ Noticias encontradas para: {query_especifica}")
                        break  # Si encontramos algo, salir del loop
                except:
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è B√∫squeda de noticias espec√≠ficas fall√≥: {e}")
    
    # M√©todo 3: API de b√∫squeda alternativa (usando scraping b√°sico)
    try:
        # Usar una API p√∫blica de b√∫squeda o scraping b√°sico
        url = f"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer resultados de la API
            abstract = data.get('Abstract', '')
            answer = data.get('Answer', '')
            
            if abstract:
                resultados.append(f"[API Abstract] {abstract}")
                print("‚úÖ API DuckDuckGo: Abstract obtenido")
            
            if answer:
                resultados.append(f"[API Answer] {answer}")
                print("‚úÖ API DuckDuckGo: Answer obtenido")
                
    except Exception as e:
        print(f"‚ö†Ô∏è API DuckDuckGo fall√≥: {e}")
    
    # M√©todo 3: Para clima espec√≠fico, usar nuestra API
    if any(palabra in query.lower() for palabra in ['clima', 'weather', 'temperatura', 'temperature', 'tiempo']):
        try:
            # Extraer ciudad de la consulta
            ciudad = 'Quito'  # Default
            if 'quito' in query.lower():
                ciudad = 'Quito'
            elif 'guayaquil' in query.lower():
                ciudad = 'Guayaquil'
            elif 'cuenca' in query.lower():
                ciudad = 'Cuenca'
            elif 'new york' in query.lower() or 'nueva york' in query.lower():
                ciudad = 'New York'
            elif 'london' in query.lower() or 'londres' in query.lower():
                ciudad = 'London'
            elif 'madrid' in query.lower():
                ciudad = 'Madrid'
            
            clima_data = obtener_clima_api(ciudad)
            if clima_data['success']:
                data = clima_data['data']
                clima_info = f"Clima actual en {ciudad}: {data['temperatura']}¬∞C, {data['descripcion']}, Humedad: {data['humedad']}%, Viento: {data['velocidad_viento']} km/h"
                resultados.append(f"[Clima API] {clima_info}")
                print(f"‚úÖ Clima API: Datos obtenidos para {ciudad}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è API Clima fall√≥: {e}")
    
    # Compilar resultados
    if resultados:
        resultado_final = "\n\n".join(resultados)
        print(f"‚úÖ B√∫squeda completada con {len(resultados)} fuentes")
        return resultado_final
    else:
        print("‚ùå No se obtuvieron resultados de ninguna fuente")
        return f"No se pudo obtener informaci√≥n actualizada sobre '{query}'. Se recomienda consultar fuentes directas como Google, sitios web oficiales o aplicaciones especializadas."

# Crear herramienta personalizada para b√∫squeda web
def crear_herramienta_busqueda():
    """Crea una herramienta de b√∫squeda web personalizada"""
    return Tool(
        name="web_search",
        description="Busca informaci√≥n actual en internet sobre cualquier tema. √ötil para noticias, precios, eventos actuales, etc. Input debe ser una consulta de b√∫squeda espec√≠fica.",
        func=busqueda_web_avanzada
    )

def ejecutar_comando_ollama(command: str) -> str:
    """Ejecuta comandos de Ollama y retorna el resultado"""
    try:
        import subprocess
        import json
        
        command = command.strip()
        if not command:
            return "‚ùå Error: Comando vac√≠o"
        
        # Lista de comandos permitidos por seguridad
        allowed_commands = ['ps', 'list', 'serve', 'run', 'stop', 'show', 'create', 'rm', 'cp', 'push', 'pull']
        
        # Parsear el comando
        cmd_parts = command.split()
        if not cmd_parts or cmd_parts[0] not in allowed_commands:
            return f"‚ùå Error: Comando no permitido. Comandos disponibles: {', '.join(allowed_commands)}"
        
        # Construir comando completo
        full_command = ['ollama'] + cmd_parts
        
        try:
            # Ejecutar comando con timeout
            if cmd_parts[0] == 'serve':
                # Para serve, iniciar en background
                process = subprocess.Popen(
                    full_command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return "‚úÖ Ollama serve iniciado en background. Verifica el estado con 'ollama ps'."
            else:
                # Para otros comandos, ejecutar y obtener output
                result = subprocess.run(
                    full_command,
                    capture_output=True,
                    text=True,
                    timeout=30,
                    check=False
                )
                
                # Combinar stdout y stderr
                output = ''
                if result.stdout:
                    output += result.stdout
                if result.stderr:
                    output += result.stderr
                
                if not output:
                    output = f"‚úÖ Comando '{command}' ejecutado exitosamente (sin output)"
                
                return f"üìã Resultado de 'ollama {command}':\n{output}"
                
        except subprocess.TimeoutExpired:
            return f"‚è∞ Error: Timeout al ejecutar comando 'ollama {command}' (>30s)"
        except subprocess.CalledProcessError as e:
            return f"‚ùå Error al ejecutar comando 'ollama {command}': {e}"
        except Exception as e:
            return f"‚ùå Error inesperado: {str(e)}"
            
    except Exception as e:
        return f"‚ùå Error al ejecutar comando Ollama: {str(e)}"

def crear_herramienta_ollama():
    """Crea una herramienta para ejecutar comandos Ollama"""
    return Tool(
        name="ollama_command",
        description="Ejecuta comandos de Ollama para gestionar modelos de IA. Comandos disponibles: ps (ver modelos en ejecuci√≥n), list (listar modelos), serve (iniciar servicio), run <modelo> (ejecutar modelo), stop <modelo> (detener modelo), show <modelo> (info del modelo), pull <modelo> (descargar modelo). Input debe ser el comando sin 'ollama' (ej: 'ps', 'list', 'run llama3')",
        func=ejecutar_comando_ollama
    )

def formatear_respuesta_clima(clima_data: dict, modelo_seleccionado: str) -> str:
    """Formatea la respuesta del clima de manera atractiva"""
    if clima_data['success']:
        data = clima_data['data']
        return f"""üå§Ô∏è **Clima actual en {data['ciudad']}**

üìä **Condiciones actuales:**
‚Ä¢ **Temperatura:** {data['temperatura']}¬∞C
‚Ä¢ **Sensaci√≥n t√©rmica:** {data['sensacion_termica']}¬∞C
‚Ä¢ **Condiciones:** {data['descripcion']}
‚Ä¢ **Humedad:** {data['humedad']}%
‚Ä¢ **Viento:** {data['velocidad_viento']} km/h ({data['direccion_viento']})
‚Ä¢ **√öltima actualizaci√≥n:** {data['hora_consulta']}

üèîÔ∏è **Informaci√≥n contextual:**
Quito se encuentra a 2,850 metros sobre el nivel del mar, lo que influye en su clima templado durante todo el a√±o. Las temperaturas suelen oscilar entre 10¬∞C y 25¬∞C.

üì± **Para m√°s detalles:**
‚Ä¢ Pron√≥stico extendido: [wttr.in/Quito](https://wttr.in/Quito)
‚Ä¢ Apps recomendadas: AccuWeather, Weather Underground
‚Ä¢ Sitios web: weather.com, tiempo.com

*Datos obtenidos de API meteorol√≥gica en tiempo real*"""
    else:
        return f"""‚ùå **No se pudo obtener el clima actual**

Error: {clima_data['error']}

üå§Ô∏è **Informaci√≥n general sobre Quito:**
Quito tiene un clima subtropical de monta√±a con temperaturas relativamente estables durante todo el a√±o:
‚Ä¢ **D√≠a:** 18-24¬∞C
‚Ä¢ **Noche:** 8-15¬∞C
‚Ä¢ **Julio:** Estaci√≥n seca, d√≠as soleados y noches frescas

üì± **Consulta informaci√≥n actualizada en:**
‚Ä¢ Google: "clima Quito Ecuador"
‚Ä¢ AccuWeather.com
‚Ä¢ Weather.com
‚Ä¢ Apps m√≥viles de clima"""

# Configurar herramientas para el agente
herramienta_busqueda = crear_herramienta_busqueda()
herramienta_ollama = crear_herramienta_ollama()
tools = [herramienta_busqueda, herramienta_ollama, DuckDuckGoSearchRun()]

# Prompt optimizado para b√∫squedas generales
agent_prompt = PromptTemplate.from_template("""
Eres un asistente de IA especializado en proporcionar respuestas precisas y actuales. Tienes acceso a herramientas de b√∫squeda web y gesti√≥n de modelos Ollama.

Herramientas disponibles:
{tools}

REGLAS IMPORTANTES: 
1. SIEMPRE usa b√∫squeda web para: noticias recientes, precios actuales, eventos deportivos, informaci√≥n actualizada
2. USA ollama_command para: gestionar modelos Ollama, ver modelos disponibles, ejecutar/detener modelos, obtener informaci√≥n de modelos
3. NUNCA digas que no tienes acceso a informaci√≥n en tiempo real - ¬°S√ç LO TIENES!
4. Si una b√∫squeda falla, intenta con t√©rminos diferentes
5. Proporciona respuestas √∫tiles basadas en los resultados obtenidos
6. Para comandos Ollama, usa solo el comando sin 'ollama' (ej: 'ps', 'list', 'run llama3')

COMANDOS OLLAMA DISPONIBLES:
‚Ä¢ ps - Ver modelos en ejecuci√≥n
‚Ä¢ list - Listar todos los modelos instalados
‚Ä¢ serve - Iniciar el servicio Ollama
‚Ä¢ run <modelo> - Ejecutar un modelo espec√≠fico
‚Ä¢ stop <modelo> - Detener un modelo espec√≠fico
‚Ä¢ show <modelo> - Mostrar informaci√≥n detallada de un modelo
‚Ä¢ pull <modelo> - Descargar un nuevo modelo

Formato OBLIGATORIO:
Question: la pregunta que debes responder
Thought: necesito buscar informaci√≥n actual sobre [tema espec√≠fico] O necesito ejecutar comando Ollama [comando]
Action: duckduckgo_search O web_search O ollama_command
Action Input: [t√©rminos de b√∫squeda espec√≠ficos] O [comando ollama sin 'ollama']
Observation: [resultados de la b√∫squeda o comando]
Thought: ahora tengo informaci√≥n actual y puedo responder
Final Answer: [respuesta completa en espa√±ol basada en los resultados]

Pregunta: {input}
Thought:{agent_scratchpad}
""")

# Crear el agente con manejo de errores y herramientas mejoradas
agents = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        try:
            # Usar el prompt est√°ndar de React que funciona
            react_prompt = hub.pull("hwchase17/react")
            
            agent = create_react_agent(model_instance, tools, react_prompt)
            agents[model_name] = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                max_iterations=20,  # Aumentado significativamente para b√∫squedas extensas
                max_execution_time=1800,  # 30 minutos para b√∫squedas completas
                handle_parsing_errors=True,
                return_intermediate_steps=True
            )
            print(f"‚úÖ Agente {model_name} creado con herramientas avanzadas")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creando agente {model_name}: {e}")
            agents[model_name] = None

# Tambi√©n configurar un chat simple sin agente para preguntas b√°sicas
simple_chat_prompt = ChatPromptTemplate.from_messages([
    ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y √∫tiles en espa√±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaci√≥n espec√≠fica y detallada
3. Si te preguntan "¬øqu√© es X?", explica qu√© es X de manera clara y completa
4. Si te preguntan sobre conceptos t√©cnicos, da definiciones precisas
5. Mant√©n un tono profesional pero accesible
6. No digas solo "estoy aqu√≠ para ayudar" - da la respuesta espec√≠fica

EJEMPLOS:
- Si preguntan "¬øqu√© es Java?" ‚Üí Explica que Java es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es Python?" ‚Üí Explica que Python es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es HTML?" ‚Üí Explica que HTML es un lenguaje de marcado

Siempre proporciona informaci√≥n √∫til y espec√≠fica."""),
    ("user", "{pregunta}")
])

# Crear prompts espec√≠ficos para diferentes modelos
def crear_prompt_para_modelo(model_name: str) -> ChatPromptTemplate:
    """Crea un prompt optimizado seg√∫n el modelo espec√≠fico"""
    
    if model_name == 'deepseek-coder':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek Coder, un asistente especializado en programaci√≥n y desarrollo. Proporciona respuestas t√©cnicas precisas y c√≥digo cuando sea necesario.

ESPECIALIDADES:
- Explicar conceptos de programaci√≥n
- Proporcionar ejemplos de c√≥digo
- Debugging y resoluci√≥n de problemas
- Mejores pr√°cticas de desarrollo

FORMATO DE RESPUESTA:
- Respuestas directas y t√©cnicas
- Incluye ejemplos de c√≥digo cuando sea relevante
- Explica conceptos paso a paso
- Menciona ventajas y desventajas cuando sea apropiado"""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'deepseek-r1:8b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek R1, un modelo de razonamiento avanzado dise√±ado para an√°lisis profundo y respuestas reflexivas.

CARACTER√çSTICAS ESPECIALES:
- Razonamiento paso a paso antes de responder
- An√°lisis cr√≠tico y evaluaci√≥n de m√∫ltiples perspectivas
- Explicaciones detalladas y fundamentadas
- Capacidad de autorreflexi√≥n y correcci√≥n

FORMATO DE RESPUESTA OBLIGATORIO:
üîç **AN√ÅLISIS INICIAL:** [Comprensi√≥n del problema/pregunta]

üß† **RAZONAMIENTO:**
‚Ä¢ **Paso 1:** [Primera consideraci√≥n o enfoque]
‚Ä¢ **Paso 2:** [Segunda consideraci√≥n o an√°lisis]
‚Ä¢ **Paso 3:** [Tercera consideraci√≥n o s√≠ntesis]

üìã **RESPUESTA:** [Conclusi√≥n fundamentada y detallada]

üîÑ **REFLEXI√ìN:** [Validaci√≥n de la respuesta y posibles alternativas]

IMPORTANTE: Siempre usa este formato estructurado para mostrar tu proceso de pensamiento completo."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'lmstudio-deepseek':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek Coder ejecut√°ndose en LM Studio. Proporciona respuestas t√©cnicas claras y directas.

INSTRUCCIONES:
- Respuestas concisas pero completas
- Enf√≥cate en aspectos t√©cnicos cuando sea relevante
- Usa formato claro con vi√±etas o numeraci√≥n
- Evita explicaciones excesivamente largas
- Proporciona ejemplos de c√≥digo solo cuando sea necesario

FORMATO DE RESPUESTA:
üîß **EXPLICACI√ìN T√âCNICA:** [Definici√≥n clara y directa]

üíª **CARACTER√çSTICAS PRINCIPALES:**
‚Ä¢ [Caracter√≠stica 1]
‚Ä¢ [Caracter√≠stica 2] 
‚Ä¢ [Caracter√≠stica 3]

üìù **EJEMPLO PR√ÅCTICO:** [Solo si es relevante y breve]

Mant√©n las respuestas enfocadas y √∫tiles."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'gemma:2b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente √∫til. Responde de forma clara y completa.

Si preguntan "¬øqu√© es X?", explica qu√© es X con:
1. Definici√≥n principal
2. Caracter√≠sticas importantes
3. Usos principales

EJEMPLO para "¬øqu√© es Java?":
Java es un lenguaje de programaci√≥n orientado a objetos desarrollado por Sun Microsystems (ahora Oracle). Sus caracter√≠sticas principales son:

‚Ä¢ **Multiplataforma**: "Write once, run anywhere" - el c√≥digo Java se ejecuta en cualquier sistema con JVM
‚Ä¢ **Orientado a objetos**: Organiza el c√≥digo en clases y objetos
‚Ä¢ **Robusto y seguro**: Manejo autom√°tico de memoria y verificaci√≥n de c√≥digo
‚Ä¢ **Usos principales**: Aplicaciones empresariales, aplicaciones m√≥viles (Android), desarrollo web

Es uno de los lenguajes m√°s populares para desarrollo de software empresarial y aplicaciones Android.

Responde siempre de manera √∫til y completa."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'phi3':
        return ChatPromptTemplate.from_messages([
            ("system", f"""Eres Microsoft Phi-3, un modelo compacto pero potente dise√±ado para respuestas r√°pidas y precisas.

FECHA ACTUAL: {time.strftime('%d de %B de %Y')} 

IMPORTANTE: Para preguntas sobre noticias, eventos actuales, precios, clima o informaci√≥n reciente, debes indicar claramente que necesitas b√∫squeda web en tiempo real, ya que tu conocimiento tiene una fecha de corte y puede estar desactualizado.

CARACTER√çSTICAS:
- Respuestas concisas pero completas
- Enfoque en eficiencia y claridad
- Proporciona informaci√≥n pr√°ctica y √∫til
- Evita redundancias y texto innecesario
- SIEMPRE reconoce limitaciones temporales para informaci√≥n actual

FORMATO:
1. Respuesta directa a la pregunta
2. Informaci√≥n clave en 2-3 puntos
3. Ejemplo o aplicaci√≥n pr√°ctica si es relevante
4. Para noticias/eventos actuales: Recomendar b√∫squeda web"""),
            ("user", "{pregunta}")
        ])
    
    else:
        # Prompt por defecto para Llama3 y Gemini
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y √∫tiles en espa√±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaci√≥n espec√≠fica y detallada
3. Si te preguntan "¬øqu√© es X?", explica qu√© es X de manera clara y completa
4. Si te preguntan sobre conceptos t√©cnicos, da definiciones precisas
5. Mant√©n un tono profesional pero accesible
6. No digas solo "estoy aqu√≠ para ayudar" - da la respuesta espec√≠fica

EJEMPLOS:
- Si preguntan "¬øqu√© es Java?" ‚Üí Explica que Java es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es Python?" ‚Üí Explica que Python es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es HTML?" ‚Üí Explica que HTML es un lenguaje de marcado

Siempre proporciona informaci√≥n √∫til y espec√≠fica."""),
            ("user", "{pregunta}")
        ])

simple_chains = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        # Obtener prompt personalizado para cada modelo
        model_prompt = crear_prompt_para_modelo(model_name)
        
        # Configurar chain con temperatura si es posible
        ollama_models_list = ['llama3', 'deepseek-coder', 'deepseek-r1:8b', 'phi3', 'gemma:2b']
        lmstudio_models_list = ['lmstudio-gemma', 'lmstudio-mistral', 'lmstudio-deepseek']
        
        if model_name in ollama_models_list:
            # Para modelos de Ollama, configurar directamente sin bind (temperatura no es compatible)
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        elif model_name in lmstudio_models_list:
            # Para modelos de LM Studio, configurar con temperatura
            configured_model = model_instance
            simple_chains[model_name] = model_prompt | configured_model | StrOutputParser()
        elif model_name == 'gemini-1.5-flash':
            # Para Gemini, configurar temperatura usando bind
            configured_model = model_instance.bind(temperature=Config.DEFAULT_TEMPERATURE)
            simple_chains[model_name] = model_prompt | configured_model | StrOutputParser()
        else:
            # Fallback sin temperatura espec√≠fica
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        
        print(f"‚úÖ Chat simple {model_name} configurado con prompt personalizado")

@app.route('/')
def index() -> str:
    return render_template('index.html', modelos_disponibles=available_models)

@app.route('/chat', methods=['POST'])
def chat() -> Union[Response, Tuple[Response, int]]:
    try:
        print("üîç DEBUG: Iniciando funci√≥n chat()")
        data = request.get_json()
        print(f"üîç DEBUG: Datos recibidos: {data}")
        
        pregunta = data.get('pregunta', '')
        modo = data.get('modo', 'simple')  # 'simple' o 'agente'
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        permitir_internet = data.get('permitir_internet', True)  # Por defecto permitir internet
        
        print(f"üîç DEBUG: pregunta='{pregunta}', modo='{modo}', modelo='{modelo_seleccionado}', internet={permitir_internet}")
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Obtener el modelo seleccionado
        modelo = get_model(modelo_seleccionado)
        print(f"üîç DEBUG: Modelo obtenido: {modelo}")
        
        if modelo is None:
            return jsonify({'error': 'No hay modelos disponibles'}), 500

        # Detecci√≥n inteligente para forzar modo agente cuando se necesite informaci√≥n actual
        palabras_actualidad = [
            'noticias', 'news', 'actualidad', 'hoy', 'today', 'actual', 'reciente', 
            'precio', 'price', 'cotizaci√≥n', '√∫ltimo', 'latest', 'breaking',
            'eventos', 'acontecimiento', 'qu√© pas√≥', 'qu√© est√° pasando'
        ]
        
        # Detecci√≥n de comandos Ollama para activar modo agente
        palabras_ollama = [
            'ollama ps', 'ollama list', 'ollama serve', 'ollama run', 'ollama stop', 
            'ollama show', 'ollama pull', 'modelos ollama', 'ver modelos', 
            'ejecutar modelo', 'detener modelo', 'instalar modelo', 'descargar modelo',
            'estado ollama', 'servicio ollama', 'gestionar ollama', 'comandos ollama'
        ]
        
        necesita_busqueda_web = any(palabra in pregunta.lower() for palabra in palabras_actualidad)
        necesita_comando_ollama = any(palabra in pregunta.lower() for palabra in palabras_ollama)
        
        if (necesita_busqueda_web or necesita_comando_ollama) and permitir_internet and modo == 'simple':
            if necesita_comando_ollama:
                print(f"ü§ñ Detectada consulta de comandos Ollama, cambiando a modo agente")
            else:
                print(f"üîÑ Detectada consulta que requiere informaci√≥n actual, cambiando a modo agente")
            modo = 'agente'

        # Forzar modo simple si internet est√° deshabilitado y se intenta usar modos que requieren web
        if not permitir_internet and modo in ['agente', 'busqueda_rapida']:
            print(f"üîç DEBUG: Forzando modo simple porque internet est√° deshabilitado")
            modo = 'simple'

        if modo == 'agente' and permitir_internet and modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            # Verificar si es una consulta de clima para usar endpoint especializado
            if any(palabra in pregunta.lower() for palabra in ['clima', 'weather', 'temperatura', 'temp', 'tiempo']):
                print(f"üå§Ô∏è Detectada consulta de clima, usando endpoint especializado")
                try:
                    tiempo_inicio = time.time()
                    
                    # Extraer ciudad de la pregunta
                    ciudad = 'Quito'  # Default
                    if 'quito' in pregunta.lower():
                        ciudad = 'Quito'
                    elif 'guayaquil' in pregunta.lower():
                        ciudad = 'Guayaquil'
                    elif 'cuenca' in pregunta.lower():
                        ciudad = 'Cuenca'
                    
                    # Obtener datos del clima directamente
                    clima_data = obtener_clima_api(ciudad)
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    if clima_data['success']:
                        # Usar el modelo para generar una respuesta formateada
                        if modelo_seleccionado in simple_chains:
                            prompt_clima = f"""La consulta del usuario es: "{pregunta}"

Los datos actuales del clima en {ciudad} son:
- Temperatura: {clima_data['data']['temperatura']}¬∞C
- Condiciones: {clima_data['data']['descripcion']}
- Humedad: {clima_data['data']['humedad']}%
- Sensaci√≥n t√©rmica: {clima_data['data']['sensacion_termica']}¬∞C
- Viento: {clima_data['data']['velocidad_viento']} km/h ({clima_data['data']['direccion_viento']})
- Hora de consulta: {clima_data['data']['hora_consulta']}

Responde de manera clara y √∫til con estos datos actuales."""
                            
                            respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
                            
                            return jsonify({
                                'respuesta': respuesta_formateada,
                                'modo': 'clima_directo_agente',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': [
                                    f"üå§Ô∏è Detectada consulta sobre clima en {ciudad}",
                                    "üîç Consultando API meteorol√≥gica en tiempo real",
                                    f"üìä Datos obtenidos: {clima_data['data']['temperatura']}¬∞C, {clima_data['data']['descripcion']}",
                                    "üß† Procesando respuesta personalizada"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'iteraciones': 1,
                                    'busquedas': 1,
                                    'timestamp': time.time(),
                                    'timestamp_inicio': tiempo_inicio,
                                    'timestamp_fin': tiempo_fin,
                                    'internetHabilitado': permitir_internet,
                                    'tipo_consulta': 'clima_optimizada'
                                }
                            })
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en consulta de clima optimizada: {e}")
                    # Fallback al agente normal
                    pass
            
            # Usar el agente para preguntas que puedan requerir b√∫squeda web
            tiempo_inicio = time.time()  # Mover antes del try para tenerlo disponible en except
            logs_agente = []  # Para capturar logs detallados del agente
            
            try:
                print(f"ü§ñ Iniciando agente {modelo_seleccionado} para: {pregunta[:50]}...")
                logs_agente.append("üöÄ Iniciando sistema AgentExecutor")
                logs_agente.append("‚ö° > Entering new AgentExecutor chain...")
                
                # Configurar captura de logs m√°s detallada para obtener <think> y otros elementos
                import io
                import sys
                import logging
                from contextlib import redirect_stdout, redirect_stderr
                
                # Crear capturadores de salida
                captured_output = io.StringIO()
                captured_logs = []
                
                # Handler personalizado para capturar logs
                class LogCapture(logging.Handler):
                    def emit(self, record):
                        captured_logs.append(self.format(record))
                
                log_capture = LogCapture()
                logging.getLogger().addHandler(log_capture)
                
                try:
                    # Ejecutar el agente con captura
                    with redirect_stdout(captured_output):
                        respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                finally:
                    # Remover el handler
                    logging.getLogger().removeHandler(log_capture)
                
                # Obtener toda la salida capturada
                agent_full_output = captured_output.getvalue()
                
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                duracion_formateada = formatear_duracion(duracion)
                
                logs_agente.append("üèÅ > Finished chain.")
                logs_agente.append(f"‚úÖ Agente completado en {duracion_formateada}")
                
                print(f"‚úÖ Agente completado en {duracion_formateada}")
                
                # Procesar la salida capturada para extraer <think> y otros elementos
                think_content = []
                if agent_full_output:
                    # Buscar contenido <think> usando regex
                    import re
                    think_matches = re.findall(r'<think>(.*?)</think>', agent_full_output, re.DOTALL | re.IGNORECASE)
                    for i, think in enumerate(think_matches):
                        think_content.append(f"üí≠ <think> {i+1}: {think.strip()}")
                    
                    # Buscar otros patrones √∫tiles
                    thought_matches = re.findall(r'Thought: (.*?)(?=\n(?:Action:|Observation:|Final Answer:)|$)', agent_full_output, re.DOTALL)
                    for i, thought in enumerate(thought_matches):
                        if thought.strip():
                            logs_agente.append(f"üß† Thought {i+1}: {thought.strip()}")
                    
                    # Buscar acciones espec√≠ficas
                    action_input_matches = re.findall(r'Action Input: (.*?)(?=\n|$)', agent_full_output)
                    for i, action_input in enumerate(action_input_matches):
                        if action_input.strip():
                            logs_agente.append(f"üìù Action Input {i+1}: {action_input.strip()}")
                
                # Agregar logs capturados
                for log_msg in captured_logs:
                    if any(keyword in log_msg.lower() for keyword in ['think', 'thought', 'action', 'observation']):
                        logs_agente.append(f"üìã Log: {log_msg}")
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                pensamientos = logs_agente.copy()  # Empezar con los logs del agente
                
                # Agregar contenido <think> a los pensamientos
                if think_content:
                    pensamientos.extend(think_content)
                    print(f"üìù Capturado {len(think_content)} bloques <think>")
                
                if 'intermediate_steps' in respuesta_completa:
                    print(f"üìä Procesando {len(respuesta_completa['intermediate_steps'])} pasos intermedios...")
                    
                    for i, paso in enumerate(respuesta_completa['intermediate_steps']):
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar b√∫squedas
                            if hasattr(accion, 'tool') and accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                                pensamientos.append(f"üîç Realizando b√∫squeda: '{accion.tool_input}'")
                            
                            # Agregar informaci√≥n del paso
                            paso_info = {
                                'step': i + 1,
                                'action': accion.tool if hasattr(accion, 'tool') else str(accion),
                                'action_input': accion.tool_input if hasattr(accion, 'tool_input') else str(accion),
                                'observation': observacion[:500] + '...' if len(str(observacion)) > 500 else str(observacion),
                                'thought': f"Ejecutando {accion.tool}" if hasattr(accion, 'tool') else "Procesando informaci√≥n"
                            }
                            pasos_intermedios.append(paso_info)
                            
                            # Agregar pensamiento formateado m√°s detallado
                            if hasattr(accion, 'tool'):
                                pensamientos.append(f"üí≠ Paso {i+1}: Usando herramienta '{accion.tool}' con entrada: '{accion.tool_input}'")
                                # Mostrar resultado truncado pero m√°s informativo
                                resultado_truncado = str(observacion)[:300]
                                if len(str(observacion)) > 300:
                                    resultado_truncado += "..."
                                pensamientos.append(f"üìã Resultado: {resultado_truncado}")
                            
                            # Agregar an√°lisis del resultado
                            if "No good" in str(observacion):
                                pensamientos.append("‚ö†Ô∏è B√∫squeda sin resultados √∫tiles, intentando m√©todo alternativo")
                            elif len(str(observacion)) > 50:
                                pensamientos.append("‚úÖ Informaci√≥n obtenida, procesando para generar respuesta")
                
                # Si hay pasos pero sin pensamientos detallados, agregar contexto
                if len(pasos_intermedios) > 0 and len(pensamientos) == 0:
                    pensamientos.append("üîÑ Ejecutando proceso de b√∫squeda y an√°lisis")
                    pensamientos.append(f"üìä Completados {len(pasos_intermedios)} pasos de investigaci√≥n")
                
                # Si no hay pasos intermedios, crear pensamientos b√°sicos
                if len(pasos_intermedios) == 0:
                    pensamientos.append("üí≠ Analizando consulta con conocimiento base")
                    pensamientos.append("üß† Generando respuesta usando modelo de IA")
                    if "noticias" in pregunta.lower() or "hoy" in pregunta.lower():
                        pensamientos.append("üì∞ Nota: Para noticias actuales se recomienda activar b√∫squeda web")
                else:
                    # Agregar pensamiento final
                    pensamientos.append("üéØ Sintetizando informaci√≥n recopilada")
                    pensamientos.append("üìù Formateando respuesta final")
                
                # Verificar si las b√∫squedas fallaron y generar respuesta de fallback inteligente
                respuesta_output = respuesta_completa.get('output', '')
                if busquedas_count > 0 and respuesta_output and "No good DuckDuckGo Search Result was found" in respuesta_output:
                    print("‚ö†Ô∏è B√∫squedas web fallaron, generando respuesta de fallback inteligente...")
                    
                    # Generar respuesta de fallback espec√≠fica para noticias
                    if any(palabra in pregunta.lower() for palabra in ['noticias', 'news', 'hoy', 'actualidad']):
                        respuesta_completa['output'] = f"""üì∞ **Informaci√≥n sobre noticias del d√≠a**

Lo siento, actualmente estoy experimentando dificultades para acceder a fuentes de noticias en tiempo real. Sin embargo, te puedo sugerir las mejores fuentes para mantenerte informado sobre las noticias de hoy:

üåê **Fuentes recomendadas de noticias:**
‚Ä¢ **Internacionales:** BBC News, CNN, Reuters, Associated Press
‚Ä¢ **Ecuador:** El Universo, El Comercio, Primicias, GK
‚Ä¢ **Tecnolog√≠a:** TechCrunch, Wired, The Verge
‚Ä¢ **Deportes:** ESPN, Marca, Fox Sports

üîç **Para noticias espec√≠ficas, te recomiendo:**
1. Visitar directamente Google News
2. Usar aplicaciones de noticias como Apple News o Google News
3. Seguir cuentas verificadas en redes sociales
4. Consultar sitios web oficiales de medios

üì± **Tip:** Configura alertas de Google para temas espec√≠ficos que te interesen.

¬øHay alg√∫n tema espec√≠fico de noticias sobre el que te gustar√≠a que te ayude a encontrar informaci√≥n?"""

                        pensamientos.append("üîÑ B√∫squeda web no disponible, proporcionando fuentes alternativas para noticias")
                        pensamientos.append("üí° Sugiriendo medios confiables y m√©todos alternativos de b√∫squeda")
                    else:
                        # Para otras consultas que requieren informaci√≥n actualizada
                        respuesta_completa['output'] = f"""üîç **Dificultades para acceder a informaci√≥n en tiempo real**

Actualmente no puedo acceder a informaci√≥n actualizada sobre "{pregunta}" debido a limitaciones en las herramientas de b√∫squeda web.

üí° **Te sugiero:**
1. Consultar directamente Google o Bing
2. Visitar sitios web oficiales relacionados con tu consulta
3. Usar aplicaciones especializadas
4. Verificar redes sociales oficiales

‚ùì **¬øPuedo ayudarte con algo m√°s espec√≠fico?**
Mientras tanto, puedo responder preguntas sobre temas generales, explicaciones conceptuales, o ayudarte de otras maneras."""
                        
                        pensamientos.append("‚ö†Ô∏è Informaci√≥n en tiempo real no disponible")
                        pensamientos.append("üõ†Ô∏è Proporcionando alternativas para obtener informaci√≥n actualizada")
                
                return jsonify({
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente',
                    'modelo_usado': modelo_seleccionado,
                    'pasos_intermedios': pasos_intermedios,
                    'pensamientos': pensamientos,
                    'metadata': {
                        'duracion': duracion,
                        'duracion_formateada': duracion_formateada,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time(),
                        'timestamp_inicio': tiempo_inicio,
                        'timestamp_fin': tiempo_fin,
                        'internetHabilitado': permitir_internet
                    }
                })
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente {modelo_seleccionado}: {error_msg}")
                
                # Manejo espec√≠fico para diferentes tipos de errores
                if "iteration limit" in error_msg.lower() or "time limit" in error_msg.lower():
                    print("‚ö†Ô∏è Agente detenido por l√≠mite de tiempo/iteraciones, intentando extraer informaci√≥n parcial...")
                    
                    # Intentar extraer informaci√≥n del error si est√° disponible
                    try:
                        # Si hay pasos intermedios en el error, intentar usarlos
                        tiempo_fin = time.time()
                        duracion = round(tiempo_fin - tiempo_inicio, 2)
                        duracion_formateada = formatear_duracion(duracion)
                        
                        # Crear respuesta usando la informaci√≥n disponible de la b√∫squeda
                        if modelo_seleccionado in simple_chains:
                            prompt_con_contexto = f"""
                            El usuario pregunt√≥: "{pregunta}"
                            
                            Se realiz√≥ una b√∫squeda web que encontr√≥ informaci√≥n parcial. Aunque el proceso se detuvo por l√≠mite de tiempo, puedo proporcionar una respuesta √∫til basada en:
                            
                            INFORMACI√ìN DE B√öSQUEDA ENCONTRADA:
                            - Se encontraron fuentes sobre Google Noticias y c√≥mo buscar noticias
                            - Informaci√≥n sobre configuraci√≥n de b√∫squedas de noticias por ubicaci√≥n
                            - Referencias a herramientas para organizar y encontrar noticias
                            
                            Por favor, proporciona una respuesta √∫til sobre las noticias de hoy, incluyendo:
                            1. Reconocimiento de que la b√∫squeda encontr√≥ informaci√≥n parcial
                            2. Recomendaciones espec√≠ficas para obtener noticias actuales
                            3. Mencionar Google News como herramienta principal encontrada
                            4. Sugerir otros sitios de noticias confiables
                            5. Tips para configurar b√∫squedas de noticias
                            
                            Responde de manera √∫til y proactiva.
                            """
                            
                            respuesta_con_contexto = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_con_contexto})
                            
                            return jsonify({
                                'respuesta': respuesta_con_contexto,
                                'modo': 'agente_parcial',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': [
                                    "üîç B√∫squeda web iniciada exitosamente",
                                    "üìä Informaci√≥n parcial obtenida de fuentes web",
                                    "‚è∞ Proceso detenido por l√≠mite de tiempo",
                                    "üß† Generando respuesta con informaci√≥n disponible",
                                    "üí° Proporcionando recomendaciones adicionales"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'iteraciones': 1,
                                    'busquedas': 1,
                                    'timestamp': time.time(),
                                    'timestamp_inicio': tiempo_inicio,
                                    'timestamp_fin': tiempo_fin,
                                    'internetHabilitado': permitir_internet,
                                    'nota': 'Respuesta generada con informaci√≥n parcial'
                                }
                            })
                    except:
                        pass
                    
                    fallback_msg = f"‚ö†Ô∏è La b√∫squeda tom√≥ m√°s tiempo del esperado. Intentando respuesta r√°pida...\n\n"
                elif "parsing" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è Hubo un problema procesando la b√∫squeda. Usando respuesta directa...\n\n"
                else:
                    fallback_msg = f"‚ö†Ô∏è Error en b√∫squeda web. Usando modo simple...\n\n"
                
                # Fallback a chat simple si el agente falla
                if modelo_seleccionado in simple_chains:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    return jsonify({
                        'respuesta': f"{fallback_msg}{respuesta}",
                        'modo': 'simple_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'metadata': {
                            'internetHabilitado': permitir_internet,
                            'iteraciones': 0,
                            'busquedas': 0
                        }
                    })
                else:
                    return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible para fallback'}), 500
        else:
            # Usar chat simple
            print(f"üîç DEBUG: Usando modo simple con modelo {modelo_seleccionado}")
            if modelo_seleccionado in simple_chains:
                print(f"üîç DEBUG: Chain encontrada para {modelo_seleccionado}")
                tiempo_inicio = time.time()
                
                # Manejo especial para DeepSeek R1 - Generar pensamientos simulados
                pensamientos_proceso = []
                if modelo_seleccionado == 'deepseek-r1:8b':
                    # Simular proceso de razonamiento paso a paso
                    pensamientos_proceso = [
                        f"üîç Analizando pregunta: '{pregunta}'",
                        "üß† Identificando conceptos clave y contexto",
                        "üìö Accediendo a conocimiento base sobre el tema",
                        "üîó Conectando informaci√≥n relevante",
                        "üìù Estructurando respuesta paso a paso",
                        "üîÑ Validando coherencia y completitud"
                    ]
                elif modelo_seleccionado.startswith('lmstudio-'):
                    # Para modelos LM Studio, mostrar proceso de conexi√≥n
                    pensamientos_proceso = [
                        f"üîß Conectando con LM Studio ({modelo_seleccionado})",
                        f"üìù Procesando consulta: '{pregunta}'",
                        "‚ö° Generando respuesta t√©cnica especializada"
                    ]
                
                try:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    print(f"‚úÖ Chat simple completado en {duracion_formateada}")
                    
                    return jsonify({
                        'respuesta': respuesta,
                        'modo': 'simple',
                        'modelo_usado': modelo_seleccionado,
                        'pensamientos': pensamientos_proceso,
                        'metadata': {
                            'duracion': duracion,
                            'duracion_formateada': duracion_formateada,
                            'timestamp': time.time(),
                            'internetHabilitado': permitir_internet,
                            'iteraciones': 0,
                            'busquedas': 0,
                            'razonamiento_visible': modelo_seleccionado == 'deepseek-r1:8b'
                        }
                    })
                
                except Exception as model_error:
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    error_msg = str(model_error)
                    
                    # Manejo espec√≠fico para timeout de LM Studio
                    if "ReadTimeout" in error_msg or "timed out" in error_msg:
                        if modelo_seleccionado.startswith('lmstudio-'):
                            return jsonify({
                                'respuesta': f"""‚è∞ **Timeout del modelo LM Studio**

El modelo {modelo_seleccionado} est√° tardando m√°s de lo esperado en responder. Esto puede deberse a:

üîß **Posibles causas:**
‚Ä¢ El modelo est√° procesando una respuesta compleja
‚Ä¢ LM Studio est√° sobrecargado o funcionando lento
‚Ä¢ La consulta requiere mucho tiempo de procesamiento

üí° **Sugerencias:**
‚Ä¢ Intenta con una pregunta m√°s espec√≠fica
‚Ä¢ Prueba otro modelo disponible
‚Ä¢ Verifica que LM Studio est√© funcionando correctamente

‚ö° **Respuesta r√°pida para tu pregunta "{pregunta}":**
{generar_respuesta_fallback(pregunta)}

*Tiempo transcurrido: {duracion_formateada}*""",
                                'modo': 'timeout_fallback',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': pensamientos_proceso + [
                                    f"‚è∞ Timeout despu√©s de {duracion_formateada}",
                                    "üîÑ Generando respuesta de fallback"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'timestamp': time.time(),
                                    'internetHabilitado': permitir_internet,
                                    'error': 'timeout',
                                    'iteraciones': 0,
                                    'busquedas': 0
                                }
                            })
                    
                    # Para otros errores, propagar el error
                    raise model_error
            else:
                print(f"‚ùå DEBUG: Chain no encontrada para {modelo_seleccionado}")
                return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible'}), 400
            
    except Exception as e:
        print(f"‚ùå DEBUG: Error en funci√≥n chat: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Error al procesar la pregunta: {str(e)}'}), 500

@app.route('/ejemplo-agente', methods=['POST'])
def ejemplo_agente() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para demostrar capacidades del agente"""
    try:
        ejemplos = [
            "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            "¬øCu√°l es el precio actual del Bitcoin?",
            "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            "¬øCu√°les son las √∫ltimas noticias tecnol√≥gicas?"
        ]
        
        pregunta_ejemplo = ejemplos[0]  # Tomar el primer ejemplo
        
        # Usar el primer modelo y agente disponible
        modelo_disponible = available_models[0] if available_models else None
        
        if modelo_disponible and modelo_disponible in agents and agents[modelo_disponible] is not None:
            # Usar el agente para la demo
            respuesta = agents[modelo_disponible].invoke({"input": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': respuesta['output'],
                'modo': 'agente',
                'modelo_usado': modelo_disponible
            })
        elif modelo_disponible and modelo_disponible in simple_chains:
            # Fallback: usar chat simple si el agente no est√° disponible
            respuesta_simple = simple_chains[modelo_disponible].invoke({"pregunta": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': f"[Modo Simple - Agente no disponible] {respuesta_simple}",
                'modo': 'simple',
                'modelo_usado': modelo_disponible
            })
        else:
            return jsonify({'error': 'No hay modelos disponibles para la demo'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en ejemplo de agente: {str(e)}'}), 500

@app.route('/busqueda-rapida', methods=['POST'])
def busqueda_rapida() -> Union[Response, Tuple[Response, int]]:
    """Endpoint optimizado para b√∫squedas web r√°pidas"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # B√∫squeda directa con DuckDuckGo sin agente complejo
        search_tool = DuckDuckGoSearchRun()
        
        # Mejorar t√©rminos de b√∫squeda seg√∫n el tipo de pregunta
        def mejorar_consulta_busqueda(pregunta_original):
            pregunta_lower = pregunta_original.lower()
            if 'precio' in pregunta_lower and 'bitcoin' in pregunta_lower:
                return [
                    "Bitcoin price USD current today",
                    "precio Bitcoin actual d√≥lares",
                    "BTC price now current value"
                ]
            elif 'noticias' in pregunta_lower:
                return [
                    f"noticias {pregunta_original} hoy",
                    f"latest news {pregunta_original} today",
                    f"breaking news {pregunta_original} 2025"
                ]
            elif 'openai' in pregunta_lower:
                return [
                    "OpenAI news latest updates",
                    "noticias OpenAI ChatGPT",
                    "OpenAI developments 2025"
                ]
            elif 'champions' in pregunta_lower or 'deportes' in pregunta_lower:
                return [
                    "Champions League final winner",
                    "latest sports news football",
                    "resultados deportivos recientes"
                ]
            elif 'petr√≥leo' in pregunta_lower or 'oil' in pregunta_lower:
                return [
                    "oil price current USD barrel",
                    "precio petr√≥leo actual",
                    "crude oil price today"
                ]
            
            # B√∫squeda gen√©rica mejorada
            return [pregunta_original, f"{pregunta_original} today", f"{pregunta_original} 2025"]
        
        try:
            consultas = mejorar_consulta_busqueda(pregunta)
            search_results = None
            consulta_exitosa = None
            
            # Intentar m√∫ltiples consultas hasta obtener resultados √∫tiles
            for consulta in consultas:
                try:
                    print(f"üîç Buscando: {consulta}")
                    resultado = search_tool.invoke(consulta)
                    
                    # Verificar si el resultado tiene contenido √∫til
                    if resultado and len(resultado.strip()) > 50:
                        # Verificaciones adicionales para detectar resultados irrelevantes
                        resultado_lower = resultado.lower()
                        palabras_irrelevantes = [
                            "no se encontraron resultados",
                            "p√°gina no encontrada", 
                            "error 404",
                            "no hay resultados",
                            "try again later"
                        ]
                        
                        # Verificar que no tenga palabras irrelevantes
                        es_irrelevante = any(palabra in resultado_lower for palabra in palabras_irrelevantes)
                        
                        if not es_irrelevante:
                            search_results = resultado
                            consulta_exitosa = consulta
                            print(f"‚úÖ B√∫squeda exitosa con: {consulta}")
                            break
                        else:
                            print(f"‚ö†Ô∏è Resultado irrelevante con: {consulta}")
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en b√∫squeda '{consulta}': {e}")
                    continue
            
            if not search_results:
                # Fallback inteligente para cualquier consulta
                modelo = get_model(modelo_seleccionado)
                if modelo and modelo_seleccionado in simple_chains:
                    prompt_fallback = f"""
                    No pude obtener informaci√≥n actualizada de internet sobre "{pregunta}". 
                    
                    Como experto asistente, proporciona una respuesta √∫til que incluya:
                    
                    1. Informaci√≥n general relevante sobre el tema consultado
                    2. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
                       - Sitios web especializados
                       - Aplicaciones m√≥viles relevantes
                       - B√∫squedas espec√≠ficas en Google
                    3. Contexto √∫til sobre el tema
                    4. Consejos pr√°cticos para encontrar la informaci√≥n
                    
                    S√© espec√≠fico y √∫til, adapt√°ndote al tipo de consulta realizada.
                    """
                    
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_fallback})
                    
                    return jsonify({
                        'respuesta': f"üåê **B√∫squeda web limitada - Respuesta inteligente:**\n\n{respuesta_fallback}",
                        'modo': 'busqueda_inteligente_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Asistente inteligente'
                    })
                else:
                    return jsonify({
                        'respuesta': "‚ùå No pude obtener informaci√≥n actualizada desde internet en este momento. Te recomiendo:\n\n1. Consultar sitios web especializados\n2. Usar aplicaciones m√≥viles relevantes\n3. Buscar en Google con t√©rminos espec√≠ficos\n4. Intentar la b√∫squeda m√°s tarde",
                        'modo': 'busqueda_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Error en b√∫squeda'
                    })
            
            # Usar el modelo para resumir y formatear los resultados
            modelo = get_model(modelo_seleccionado)
            if modelo and modelo_seleccionado in simple_chains:
                prompt_busqueda = f"""
                INSTRUCCIONES ESPEC√çFICAS: Eres un experto asistente que debe extraer y presentar informaci√≥n √∫til de resultados de b√∫squeda web.

                PREGUNTA DEL USUARIO: "{pregunta}"
                
                RESULTADOS DE B√öSQUEDA OBTENIDOS:
                {search_results}

                TAREAS OBLIGATORIAS:
                1. ANALIZA cuidadosamente los resultados de b√∫squeda
                2. EXTRAE cualquier informaci√≥n relevante disponible (precios, noticias, datos espec√≠ficos)
                3. Si encuentras datos parciales o relacionados, √öSALOS y s√© transparente sobre las limitaciones
                4. COMPLEMENTA con informaci√≥n contextual √∫til y recomendaciones
                5. NUNCA digas simplemente "no hay informaci√≥n" - siempre proporciona valor

                ESTRUCTURA DE RESPUESTA:
                - Informaci√≥n encontrada (aunque sea limitada)
                - Contexto adicional √∫til
                - Recomendaciones para informaci√≥n m√°s completa
                - Consejos pr√°cticos

                IMPORTANTE: S√© proactivo y √∫til. Si los resultados mencionan sitios web o aplicaciones, incorp√≥ralos como recomendaciones adicionales.
                
                RESPUESTA EN ESPA√ëOL:
                """
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_busqueda})
                
                return jsonify({
                    'respuesta': f"üîç **B√∫squeda realizada con:** {consulta_exitosa}\n\n{respuesta}",
                    'modo': 'busqueda_rapida',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'DuckDuckGo'
                })
            else:
                return jsonify({
                    'respuesta': f"**Resultados de b√∫squeda directa:**\n\n{search_results}",
                    'modo': 'busqueda_directa',
                    'modelo_usado': 'ninguno',
                    'fuente': 'DuckDuckGo'
                })
                
        except Exception as search_error:
            return jsonify({'error': f'Error en b√∫squeda: {str(search_error)}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Error en b√∫squeda r√°pida: {str(e)}'}), 500

@app.route('/clima-directo', methods=['POST'])
def clima_directo() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para consultas de clima con respuesta directa inteligente"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Usar el modelo para generar una respuesta √∫til sobre clima
        modelo = get_model(modelo_seleccionado)
        if modelo and modelo_seleccionado in simple_chains:
            prompt_clima = f"""
            El usuario pregunta sobre el clima en {ciudad}. Aunque no puedo acceder a datos meteorol√≥gicos en tiempo real desde mi entrenamiento, puedo proporcionar informaci√≥n √∫til y recomendaciones pr√°cticas.

            Pregunta del usuario: "{pregunta}"

            Proporciona una respuesta √∫til que incluya:
            1. Reconocimiento de la limitaci√≥n para datos en tiempo real
            2. Informaci√≥n general sobre el clima t√≠pico de {ciudad} seg√∫n la √©poca del a√±o (es julio 2025)
            3. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
               - Sitios web espec√≠ficos (weather.com, accuweather.com, clima.com)
               - Aplicaciones m√≥viles recomendadas
               - B√∫squedas espec√≠ficas en Google
            4. Consejos pr√°cticos para el clima t√≠pico de la regi√≥n en esta √©poca

            S√© √∫til, espec√≠fico y pr√°ctico en tu respuesta.
            """
            
            respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            
            return jsonify({
                'respuesta': f"üå§Ô∏è **Informaci√≥n sobre clima en {ciudad}**\n\n{respuesta}",
                'modo': 'clima_directo',
                'modelo_usado': modelo_seleccionado,
                'fuente': 'Respuesta inteligente'
            })
        else:
            return jsonify({
                'respuesta': f"Para obtener el clima actual en {ciudad}, te recomiendo consultar:\n‚Ä¢ weather.com\n‚Ä¢ accuweather.com\n‚Ä¢ Google Weather\n‚Ä¢ Apps m√≥viles de clima",
                'modo': 'clima_basico',
                'modelo_usado': 'ninguno',
                'fuente': 'Recomendaciones est√°ticas'
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-actual', methods=['POST'])
def clima_actual() -> Union[Response, Tuple[Response, int]]:
    """Endpoint r√°pido y optimizado para consultas de clima actual"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Extraer ciudad de la pregunta
        ciudad = 'Quito'  # Default
        pregunta_lower = pregunta.lower()
        
        if 'quito' in pregunta_lower:
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta_lower:
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta_lower:
            ciudad = 'Cuenca'
        elif 'new york' in pregunta_lower or 'nueva york' in pregunta_lower:
            ciudad = 'New York'
        elif 'madrid' in pregunta_lower:
            ciudad = 'Madrid'
        elif 'london' in pregunta_lower or 'londres' in pregunta_lower:
            ciudad = 'London'
        
        print(f"üå§Ô∏è Consulta r√°pida de clima para {ciudad}")
        tiempo_inicio = time.time()
        
        # Obtener datos del clima
        clima_data = obtener_clima_api(ciudad)
        
        if clima_data['success']:
            data_clima = clima_data['data']
            
            # Si tenemos un modelo disponible, usarlo para formatear la respuesta
            if modelo_seleccionado in simple_chains:
                prompt_clima = f"""El usuario pregunta: "{pregunta}"

Datos meteorol√≥gicos ACTUALES para {ciudad}:
üå°Ô∏è Temperatura: {data_clima['temperatura']}¬∞C
üå°Ô∏è Sensaci√≥n t√©rmica: {data_clima['sensacion_termica']}¬∞C
‚òÅÔ∏è Condiciones: {data_clima['descripcion']}
üíß Humedad: {data_clima['humedad']}%
üí® Viento: {data_clima['velocidad_viento']} km/h hacia {data_clima['direccion_viento']}
üïê √öltima actualizaci√≥n: {data_clima['hora_consulta']}

Responde de manera natural y √∫til con esta informaci√≥n actualizada."""
                
                respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            else:
                # Respuesta b√°sica sin modelo
                respuesta_formateada = f"""üå§Ô∏è **Clima actual en {ciudad}**

üìä **Condiciones actuales:**
‚Ä¢ **Temperatura:** {data_clima['temperatura']}¬∞C
‚Ä¢ **Sensaci√≥n t√©rmica:** {data_clima['sensacion_termica']}¬∞C
‚Ä¢ **Condiciones:** {data_clima['descripcion']}
‚Ä¢ **Humedad:** {data_clima['humedad']}%
‚Ä¢ **Viento:** {data_clima['velocidad_viento']} km/h ({data_clima['direccion_viento']})
‚Ä¢ **√öltima actualizaci√≥n:** {data_clima['hora_consulta']}

*Datos obtenidos de API meteorol√≥gica en tiempo real*"""
            
            tiempo_fin = time.time()
            duracion = round(tiempo_fin - tiempo_inicio, 2)
            duracion_formateada = formatear_duracion(duracion)
            
            return jsonify({
                'respuesta': respuesta_formateada,
                'modo': 'clima_actual',
                'modelo_usado': modelo_seleccionado,
                'pensamientos': [
                    f"üå§Ô∏è Consultando clima actual en {ciudad}",
                    "üîç Conectando con API meteorol√≥gica wttr.in",
                    f"üìä Datos obtenidos: {data_clima['temperatura']}¬∞C, {data_clima['descripcion']}",
                    "‚úÖ Respuesta generada con datos en tiempo real"
                ],
                'metadata': {
                    'duracion': duracion,
                    'duracion_formateada': duracion_formateada,
                    'timestamp': time.time(),
                    'internetHabilitado': True,
                    'iteraciones': 1,
                    'busquedas': 1,
                    'ciudad': ciudad,
                    'fuente_datos': 'wttr.in',
                    'tipo_consulta': 'clima_rapido'
                }
            })
        else:
            return jsonify({
                'respuesta': f"‚ùå No pude obtener el clima actual de {ciudad}.\n\nError: {clima_data['error']}\n\nüå§Ô∏è **Recomendaciones:**\n‚Ä¢ Consulta weather.com\n‚Ä¢ Usa Google Weather\n‚Ä¢ Apps: AccuWeather, Weather Underground",
                'modo': 'clima_error',
                'modelo_usado': modelo_seleccionado,
                'metadata': {
                    'internetHabilitado': True,
                    'error': clima_data['error'],
                    'ciudad': ciudad
                }
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-api', methods=['POST'])
def clima_api() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para obtener clima usando API externa gratuita"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        # Extraer ciudad de la pregunta si est√° presente
        if 'quito' in pregunta.lower():
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta.lower():
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta.lower():
            ciudad = 'Cuenca'
        
        print(f"üå§Ô∏è Solicitando clima para {ciudad}...")
        
        # Obtener clima usando API
        clima_data = obtener_clima_api(ciudad)
        
        # Formatear respuesta
        respuesta_formateada = formatear_respuesta_clima(clima_data, modelo_seleccionado)
        
        return jsonify({
            'respuesta': respuesta_formateada,
            'modo': 'clima_api',
            'modelo_usado': modelo_seleccionado,
            'fuente': 'wttr.in API',
            'ciudad': ciudad,
            'success': clima_data['success']
        })
        
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima API: {str(e)}'}), 500

@app.route('/agente-general', methods=['POST'])
def agente_general() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para demostraciones del agente con b√∫squedas generales"""
    try:
        data = request.get_json()
        tipo_demo = data.get('tipo', 'clima')  # clima, noticias, bitcoin, deportes
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        # Preguntas predefinidas para diferentes tipos de demos
        demos = {
            'noticias': "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            'bitcoin': "¬øCu√°l es el precio actual del Bitcoin en USD?",
            'deportes': "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            'tech': "¬øCu√°les son las √∫ltimas noticias sobre OpenAI?",
            'economia': "¬øCu√°l es el precio actual del petr√≥leo?",
            'general': "¬øCu√°les son las noticias m√°s importantes de hoy?"
        }
        
        pregunta = demos.get(tipo_demo, demos['noticias'])
        
        # Usar el agente mejorado
        if modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            try:
                print(f"ü§ñ Ejecutando demo '{tipo_demo}' con {modelo_seleccionado}")
                tiempo_inicio = time.time()
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                if 'intermediate_steps' in respuesta_completa:
                    for paso in respuesta_completa['intermediate_steps']:
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar b√∫squedas
                            if accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                            
                            pasos_intermedios.append({
                                'action': accion.tool,
                                'action_input': accion.tool_input,
                                'observation': observacion[:300] + '...' if len(observacion) > 300 else observacion
                            })
                
                return jsonify({
                    'pregunta': pregunta,
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente_general',
                    'modelo_usado': modelo_seleccionado,
                    'tipo_demo': tipo_demo,
                    'pasos_intermedios': pasos_intermedios,
                    'metadata': {
                        'duracion': duracion,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time()
                    }
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente general {modelo_seleccionado}: {error_msg}")
                
                # Fallback inteligente
                if modelo_seleccionado in simple_chains:
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({
                        "pregunta": f"Aunque no puedo buscar en internet en este momento, responde de la mejor manera posible: {pregunta}"
                    })
                    
                    return jsonify({
                        'pregunta': pregunta,
                        'respuesta': f"‚ö†Ô∏è Agente no disponible. Respuesta b√°sica:\n\n{respuesta_fallback}",
                        'modo': 'fallback_general',
                        'modelo_usado': modelo_seleccionado,
                        'tipo_demo': tipo_demo
                    })
        
        return jsonify({'error': 'Agente no disponible'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en agente general: {str(e)}'}), 500

@app.route('/api/models/status', methods=['GET'])
def get_models_status():
    """Obtener el estado actual de todos los modelos"""
    try:
        import requests
        
        model_status = {
            'ollama': {
                'available': False,
                'models': []
            },
            'lmstudio': {
                'available': False,
                'models': []
            },
            'gemini': {
                'available': bool(Config.GOOGLE_API_KEY)
            }
        }
        
        # Verificar Ollama
        try:
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                model_status['ollama']['available'] = True
                ollama_models_data = response.json().get('models', [])
                model_status['ollama']['models'] = [
                    {
                        'name': model['name'],
                        'size': model.get('size', 'Unknown'),
                        'modified': model.get('modified_at', 'Unknown')
                    }
                    for model in ollama_models_data
                ]
        except:
            pass
            
        # Verificar LM Studio
        try:
            response = requests.get(f"{Config.LMSTUDIO_BASE_URL}/v1/models", timeout=5)
            if response.status_code == 200:
                model_status['lmstudio']['available'] = True
                lm_models_data = response.json().get('data', [])
                model_status['lmstudio']['models'] = [
                    {
                        'id': model['id'],
                        'object': model.get('object', 'model')
                    }
                    for model in lm_models_data
                ]
        except:
            pass
            
        return jsonify(model_status)
        
    except Exception as e:
        return jsonify({'error': f'Error al obtener estado de modelos: {str(e)}'}), 500

@app.route('/api/models/ollama/<action>', methods=['POST'])
def control_ollama(action):
    """Controlar modelos de Ollama"""
    try:
        import subprocess
        
        if action == 'start':
            # Intentar iniciar Ollama serve en background
            try:
                subprocess.Popen(
                    ['ollama', 'serve'],
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return jsonify({'message': 'Ollama iniciado en background', 'status': 'started'})
            except Exception as e:
                return jsonify({'error': f'Error al iniciar Ollama: {str(e)}'}), 500
                
        elif action == 'stop':
            # En Windows, intentar terminar el proceso ollama
            try:
                if os.name == 'nt':
                    subprocess.run(['taskkill', '/F', '/IM', 'ollama.exe'], check=False)
                else:
                    subprocess.run(['pkill', 'ollama'], check=False)
                return jsonify({'message': 'Ollama detenido', 'status': 'stopped'})
            except Exception as e:
                return jsonify({'error': f'Error al detener Ollama: {str(e)}'}), 500
                
        else:
            return jsonify({'error': 'Acci√≥n no v√°lida. Use start o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control de Ollama: {str(e)}'}), 500

@app.route('/api/models/lmstudio/<action>', methods=['POST'])
def control_lmstudio(action):
    """Controlar LM Studio"""
    try:
        if action == 'start':
            # Mostrar mensaje de instrucciones para LM Studio
            return jsonify({
                'message': 'Para iniciar LM Studio, abra la aplicaci√≥n manualmente desde el escritorio o men√∫ inicio.',
                'instructions': [
                    '1. Abrir LM Studio desde el escritorio',
                    '2. Cargar un modelo desde la pesta√±a "My Models"',
                    '3. Asegurarse de que el servidor local est√© en puerto 1234'
                ],
                'status': 'manual_start_required'
            })
            
        elif action == 'stop':
            return jsonify({
                'message': 'Para detener LM Studio, cierre la aplicaci√≥n manualmente.',
                'instructions': [
                    '1. Hacer clic en el bot√≥n "Stop" en LM Studio',
                    '2. Cerrar la aplicaci√≥n LM Studio'
                ],
                'status': 'manual_stop_required'
            })
            
        else:
            return jsonify({'error': 'Acci√≥n no v√°lida. Use start o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control de LM Studio: {str(e)}'}), 500

@app.route('/api/models/ollama/individual/<model_name>/<action>', methods=['POST'])
def control_ollama_individual(model_name, action):
    """Controlar modelos individuales de Ollama"""
    try:
        import subprocess
        import requests
        
        if action == 'run':
            # Ejecutar modelo espec√≠fico
            try:
                # Primero verificar si Ollama est√° ejecut√°ndose
                try:
                    requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=3)
                except:
                    # Si Ollama no est√° ejecut√°ndose, iniciarlo primero
                    subprocess.Popen(
                        ['ollama', 'serve'],
                        creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                    )
                    # Esperar un momento para que se inicie
                    import time
                    time.sleep(3)
                
                # Ejecutar el modelo en background
                subprocess.Popen(
                    ['ollama', 'run', model_name],
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                
                return jsonify({
                    'message': f'Modelo {model_name} cargado exitosamente',
                    'model': model_name,
                    'status': 'running'
                })
            except Exception as e:
                return jsonify({'error': f'Error al ejecutar modelo {model_name}: {str(e)}'}), 500
                
        elif action == 'stop':
            # Detener modelo espec√≠fico
            try:
                subprocess.run(['ollama', 'stop', model_name], check=True, timeout=10)
                return jsonify({
                    'message': f'Modelo {model_name} detenido exitosamente',
                    'model': model_name,
                    'status': 'stopped'
                })
            except subprocess.CalledProcessError as e:
                return jsonify({'error': f'Error al detener modelo {model_name}: {str(e)}'}), 500
            except subprocess.TimeoutExpired:
                return jsonify({'error': f'Timeout al detener modelo {model_name}'}), 500
                
        else:
            return jsonify({'error': 'Acci√≥n no v√°lida. Use run o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control individual de Ollama: {str(e)}'}), 500

@app.route('/api/models/ollama/running', methods=['GET'])
def get_running_ollama_models():
    """Obtener modelos de Ollama que est√°n ejecut√°ndose actualmente"""
    try:
        import requests
        
        # Verificar si Ollama est√° ejecut√°ndose
        try:
            # Intentar obtener informaci√≥n de modelos en ejecuci√≥n
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/ps", timeout=5)
            if response.status_code == 200:
                running_models = response.json().get('models', [])
                
                # Tambi√©n obtener todos los modelos disponibles
                tags_response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
                all_models = []
                if tags_response.status_code == 200:
                    all_models = tags_response.json().get('models', [])
                
                # Crear estructura de respuesta con estado de cada modelo
                models_status = []
                for model in all_models:
                    model_name = model['name']
                    is_running = any(rm['name'] == model_name for rm in running_models)
                    
                    model_info = {
                        'name': model_name,
                        'is_running': is_running,
                        'size': model.get('size', 'Unknown'),
                        'modified': model.get('modified_at', 'Unknown'),
                        'digest': model.get('digest', '')[:12] if model.get('digest') else ''
                    }
                    
                    # Si est√° ejecut√°ndose, a√±adir informaci√≥n adicional
                    if is_running:
                        running_info = next((rm for rm in running_models if rm['name'] == model_name), None)
                        if running_info:
                            model_info.update({
                                'vram_usage': running_info.get('size_vram', 0),
                                'expires_at': running_info.get('expires_at', ''),
                            })
                    
                    models_status.append(model_info)
                
                return jsonify({
                    'ollama_available': True,
                    'models': models_status,
                    'running_count': len(running_models)
                })
            else:
                return jsonify({
                    'ollama_available': False,
                    'models': [],
                    'running_count': 0
                })
        except:
            return jsonify({
                'ollama_available': False,
                'models': [],
                'running_count': 0
            })
            
    except Exception as e:
        return jsonify({'error': f'Error al obtener modelos en ejecuci√≥n: {str(e)}'}), 500

@app.route('/api/ollama/console', methods=['POST'])
def ollama_console_command():
    """Ejecutar comandos de consola Ollama"""
    try:
        import subprocess
        import json
        
        data = request.get_json()
        command = data.get('command', '').strip()
        
        if not command:
            return jsonify({'error': 'Comando vac√≠o'}), 400
        
        # Lista de comandos permitidos por seguridad
        allowed_commands = ['ps', 'list', 'serve', 'run', 'stop', 'show', 'create', 'rm', 'cp', 'push', 'pull']
        
        # Parsear el comando
        cmd_parts = command.split()
        if not cmd_parts or cmd_parts[0] not in allowed_commands:
            return jsonify({
                'error': f'Comando no permitido. Comandos disponibles: {", ".join(allowed_commands)}',
                'output': '',
                'success': False
            }), 400
        
        # Construir comando completo
        full_command = ['ollama'] + cmd_parts
        
        try:
            # Ejecutar comando con timeout
            if cmd_parts[0] == 'serve':
                # Para serve, iniciar en background
                process = subprocess.Popen(
                    full_command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return jsonify({
                    'command': command,
                    'output': 'Ollama serve iniciado en background. Verifica el estado con "ollama ps".',
                    'success': True,
                    'background': True
                })
            else:
                # Para otros comandos, ejecutar y obtener output
                result = subprocess.run(
                    full_command,
                    capture_output=True,
                    text=True,
                    timeout=30,
                    check=False
                )
                
                # Combinar stdout y stderr
                output = ''
                if result.stdout:
                    output += result.stdout
                if result.stderr:
                    output += '\n' + result.stderr if output else result.stderr
                
                if not output:
                    output = f'Comando "{command}" ejecutado exitosamente (sin output)'
                
                return jsonify({
                    'command': command,
                    'output': output.strip(),
                    'success': result.returncode == 0,
                    'return_code': result.returncode
                })
                
        except subprocess.TimeoutExpired:
            return jsonify({
                'command': command,
                'output': 'Error: Comando excedi√≥ el tiempo l√≠mite (30s)',
                'success': False,
                'timeout': True
            }), 408
            
        except subprocess.CalledProcessError as e:
            return jsonify({
                'command': command,
                'output': f'Error ejecutando comando: {e.stderr if e.stderr else str(e)}',
                'success': False,
                'error': str(e)
            }), 500
            
    except Exception as e:
        return jsonify({
            'error': f'Error interno del servidor: {str(e)}',
            'output': '',
            'success': False
        }), 500

@app.route('/api/ollama/models-for-actions', methods=['GET'])
def get_ollama_models_for_actions():
    """Obtener lista de modelos para acciones (run/stop)"""
    try:
        import requests
        import subprocess
        
        # Intentar obtener modelos via API REST
        try:
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return jsonify({
                    'models': models,
                    'source': 'api',
                    'available': True
                })
        except:
            pass
        
        # Si falla la API, intentar con comando
        try:
            result = subprocess.run(
                ['ollama', 'list'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                models = []
                for line in lines:
                    if line.strip():
                        # Extraer nombre del modelo (primera columna)
                        parts = line.split()
                        if parts:
                            models.append(parts[0])
                
                return jsonify({
                    'models': models,
                    'source': 'command',
                    'available': True
                })
        except:
            pass
        
        return jsonify({
            'models': [],
            'source': 'none',
            'available': False,
            'error': 'No se pudo obtener lista de modelos'
        })
        
    except Exception as e:
        return jsonify({
            'models': [],
            'available': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print("ü§ñ Iniciando aplicaci√≥n de IA con Agentes...")
    print(f"üß† Modelos disponibles: {', '.join(available_models)}")
    print("üîç Herramientas: B√∫squeda web avanzada")
    print("üåê Servidor: http://127.0.0.1:5000")
    print("üöÄ Aplicaci√≥n de consultas generales lista!")
    app.run(debug=True, host='127.0.0.1', port=5000)
