import os
import json
import requests
import time
import io
import logging
import re
import traceback
from typing import Optional, Union, Tuple
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv

# Importar ChatOllama de la nueva ubicaciÃ³n (si estÃ¡ disponible), sino usar la anterior
try:
    from langchain_ollama import ChatOllama
    print("âœ… Usando langchain_ollama (versiÃ³n actualizada)")
except ImportError:
    from langchain_community.chat_models import ChatOllama
    print("âš ï¸ Usando langchain_community.chat_models (versiÃ³n deprecada)")

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain.tools import BaseTool
from langchain_core.tools import Tool
from config import Config

# Definir ChatLMStudio directamente aquÃ­ para evitar problemas de importaciÃ³n
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import Any, List, Optional
from pydantic import Field
import requests

class ChatLMStudio(BaseChatModel):
    """Chat model wrapper for LM Studio API."""
    
    # Declarar explÃ­citamente los campos que necesitamos
    model: str = Field(description="Nombre del modelo a usar")
    base_url: str = Field(default="http://localhost:1234", description="URL base del servidor LM Studio")
    temperature: float = Field(default=0.7, description="Temperatura para generaciÃ³n")
    max_tokens: int = Field(default=1000, description="MÃ¡ximo nÃºmero de tokens")
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._last_response = None  # Para almacenar la Ãºltima respuesta con reasoning_content
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate chat completion."""
        # Convert messages to API format
        api_messages = []
        
        # Combinar mensajes del sistema con el primer mensaje del usuario
        system_content = ""
        user_messages = []
        
        for msg in messages:
            if isinstance(msg, SystemMessage):
                # Acumular contenido del sistema, asegurÃ¡ndonos de que sea string
                content = msg.content if isinstance(msg.content, str) else str(msg.content)
                system_content += content + "\n\n"
            elif isinstance(msg, HumanMessage):
                user_messages.append(msg)
            elif isinstance(msg, AIMessage):
                content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "assistant", "content": content})
        
        # Si hay contenido del sistema, combinarlo con el primer mensaje del usuario
        if system_content and user_messages:
            first_user_msg = user_messages[0]
            first_content = first_user_msg.content if isinstance(first_user_msg.content, str) else str(first_user_msg.content)
            combined_content = f"{system_content.strip()}\n\nUsuario: {first_content}"
            api_messages.insert(0, {"role": "user", "content": combined_content})
            
            # Agregar los mensajes restantes del usuario
            for msg in user_messages[1:]:
                user_content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "user", "content": user_content})
        else:
            # Si no hay sistema, agregar mensajes del usuario normalmente
            for msg in user_messages:
                user_content = msg.content if isinstance(msg.content, str) else str(msg.content)
                api_messages.append({"role": "user", "content": user_content})
        
        # Asegurar que no hay mensajes vacÃ­os
        if not api_messages:
            api_messages = [{"role": "user", "content": "Hola"}]
        
        # Make API request
        response = requests.post(
            f"{self.base_url}/v1/chat/completions",
            json={
                "model": self.model,
                "messages": api_messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": False
            },
            headers={"Content-Type": "application/json"},
            timeout=300  # Aumentar timeout a 5 minutos para modelos lentos
        )
        
        if response.status_code != 200:
            raise ValueError(f"LM Studio API error: {response.status_code} {response.text}")
        
        result = response.json()
        content = result["choices"][0]["message"]["content"]
        
        # Almacenar la respuesta completa para acceso posterior (reasoning_content, etc.)
        self._last_response = result
        
        # Capturar reasoning_content si estÃ¡ disponible (para modelos de razonamiento como DeepSeek R1)
        reasoning_content = None
        if "choices" in result and len(result["choices"]) > 0:
            choice = result["choices"][0]
            if "message" in choice and "reasoning_content" in choice["message"]:
                reasoning_content = choice["message"]["reasoning_content"]
        
        # Return result
        message = AIMessage(content=content)
        
        # AÃ±adir reasoning_content como metadata si estÃ¡ disponible
        if reasoning_content:
            message.additional_kwargs = {"reasoning_content": reasoning_content}
            
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    @property
    def _llm_type(self) -> str:
        return "lm-studio-api"

def generar_respuesta_fallback(pregunta: str) -> str:
    """Genera una respuesta de fallback simple para casos de timeout"""
    pregunta_lower = pregunta.lower()
    
    if 'java' in pregunta_lower:
        return """Java es un lenguaje de programaciÃ³n orientado a objetos desarrollado por Oracle. 
Sus caracterÃ­sticas principales son: multiplataforma (JVM), orientado a objetos, robusto y seguro. 
Se usa principalmente en aplicaciones empresariales, Android y desarrollo web."""
    elif 'python' in pregunta_lower:
        return """Python es un lenguaje de programaciÃ³n interpretado y de alto nivel. 
Es conocido por su sintaxis simple, versatilidad y amplia comunidad. 
Se usa en desarrollo web, ciencia de datos, IA, automatizaciÃ³n y mÃ¡s."""
    elif 'javascript' in pregunta_lower:
        return """JavaScript es un lenguaje de programaciÃ³n interpretado usado principalmente para desarrollo web. 
Permite crear interactividad en pÃ¡ginas web y tambiÃ©n se usa en el backend con Node.js."""
    else:
        return f"""Disculpa, el modelo tardÃ³ demasiado en procesar tu consulta sobre "{pregunta}". 
Por favor, intenta reformular la pregunta de manera mÃ¡s especÃ­fica o prueba con otro modelo."""

# Cargar variables de entorno
load_dotenv()

app = Flask(__name__)
app.config.from_object(Config)

def formatear_duracion(segundos):
    """Convierte segundos a formato legible (minutos y segundos)"""
    if segundos < 1:
        return f"{round(segundos * 1000)}ms"
    elif segundos < 60:
        return f"{round(segundos, 1)}s"
    else:
        minutos = int(segundos // 60)
        segundos_restantes = round(segundos % 60, 1)
        if segundos_restantes == 0:
            return f"{minutos}m"
        else:
            return f"{minutos}m {segundos_restantes}s"

print("ğŸ¤– Configurando modelos de IA...")

# Configurar modelos de IA
models = {}

# Configurar modelos de Ollama (locales)
ollama_models = [
    ('llama3', 'Llama3 8B'),
    ('deepseek-coder', 'DeepSeek Coder'),
    ('phi3', 'Microsoft Phi-3'),
    ('gemma:2b', 'Google Gemma 2B'),
    ('gemma3:4b', 'Google Gemma3 4B')
]

for model_key, model_name in ollama_models:
    try:
        models[model_key] = ChatOllama(
            model=model_key
        )
        print(f"âœ… {model_name} configurado correctamente")
    except Exception as e:
        print(f"âš ï¸ {model_name} no disponible: {e}")
        models[model_key] = None

# Configurar Google Gemini (API)
try:
    if Config.GOOGLE_API_KEY:
        os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
        models['gemini-1.5-flash'] = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash"
        )
        print("âœ… Google Gemini 1.5 Flash configurado correctamente")
    else:
        print("âš ï¸ Google Gemini no disponible: No se encontrÃ³ GOOGLE_API_KEY")
        models['gemini-1.5-flash'] = None
except Exception as e:
    print(f"âš ï¸ Google Gemini no disponible: {e}")
    models['gemini-1.5-flash'] = None

# Configurar LM Studio (API local)
try:
    if Config.validate_lmstudio():
        # Configurar Google Gemma 3-12B
        models['lmstudio-gemma'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=Config.DEFAULT_TEMPERATURE,
            max_tokens=Config.MAX_TOKENS
        )
        print("âœ… LM Studio (Gemma 3-12B) configurado correctamente")
        
        # Configurar Mistral 7B
        models['lmstudio-mistral'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL_MISTRAL,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=Config.DEFAULT_TEMPERATURE,
            max_tokens=Config.MAX_TOKENS
        )
        print("âœ… LM Studio (Mistral 7B) configurado correctamente")
        
        # Configurar DeepSeek Coder
        models['lmstudio-deepseek'] = ChatLMStudio(
            model=Config.LMSTUDIO_MODEL_DEEPSEEK,
            base_url=Config.LMSTUDIO_BASE_URL,
            temperature=0.3,  # Temperatura mÃ¡s baja para respuestas mÃ¡s rÃ¡pidas y directas
            max_tokens=800    # Reducir tokens para respuestas mÃ¡s concisas
        )
        print("âœ… LM Studio (DeepSeek Coder) configurado correctamente")
    else:
        print("âš ï¸ LM Studio no disponible")
        models['lmstudio-gemma'] = None
        models['lmstudio-mistral'] = None
        models['lmstudio-deepseek'] = None
except Exception as e:
    print(f"âš ï¸ LM Studio no disponible: {e}")
    models['lmstudio-gemma'] = None
    models['lmstudio-mistral'] = None
    models['lmstudio-deepseek'] = None

# Verificar que al menos un modelo estÃ© disponible
available_models = [k for k, v in models.items() if v is not None]
if not available_models:
    print("âŒ Error: No hay modelos disponibles")
    print("ğŸ’¡ AsegÃºrate de que:")
    print("   - Ollama estÃ© ejecutÃ¡ndose con llama3 instalado, O")
    print("   - Tengas GOOGLE_API_KEY configurada en .env, O")
    print("   - LM Studio estÃ© ejecutÃ¡ndose con un modelo cargado")
    exit(1)

print(f"ğŸ¯ Modelos disponibles: {', '.join(available_models)}")

# FunciÃ³n para obtener el modelo segÃºn la selecciÃ³n
def get_model(model_name: str) -> Optional[Union[ChatOllama, ChatGoogleGenerativeAI, ChatLMStudio]]:
    """Obtiene el modelo solicitado o el primero disponible como fallback"""
    if model_name in models and models[model_name] is not None:
        return models[model_name]
    
    # Fallback al primer modelo disponible
    for model_key, model_instance in models.items():
        if model_instance is not None:
            print(f"âš ï¸ Usando {model_key} como fallback")
            return model_instance
    
    return None

# FunciÃ³n para obtener clima usando API gratuita
def obtener_clima_api(ciudad: str = "Quito") -> dict:
    """Obtiene informaciÃ³n del clima usando API gratuita de wttr.in"""
    try:
        # API gratuita que no requiere clave
        url = f"https://wttr.in/{ciudad}?format=j1"
        
        print(f"ğŸŒ Consultando API de clima para {ciudad}...")
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer informaciÃ³n relevante
            current = data.get('current_condition', [{}])[0]
            weather_info = {
                'temperatura': current.get('temp_C', 'N/A'),
                'descripcion': current.get('weatherDesc', [{}])[0].get('value', 'N/A'),
                'humedad': current.get('humidity', 'N/A'),
                'sensacion_termica': current.get('FeelsLikeC', 'N/A'),
                'velocidad_viento': current.get('windspeedKmph', 'N/A'),
                'direccion_viento': current.get('winddir16Point', 'N/A'),
                'hora_consulta': current.get('observation_time', 'N/A'),
                'ciudad': ciudad
            }
            
            print(f"âœ… Clima obtenido: {weather_info['temperatura']}Â°C en {ciudad}")
            return {'success': True, 'data': weather_info}
            
        else:
            print(f"âš ï¸ Error API clima: {response.status_code}")
            return {'success': False, 'error': f'Error de API: {response.status_code}'}
            
    except requests.exceptions.Timeout:
        print("âš ï¸ Timeout al consultar API de clima")
        return {'success': False, 'error': 'Timeout en consulta'}
    except Exception as e:
        print(f"âš ï¸ Error consultando API de clima: {e}")
        return {'success': False, 'error': str(e)}

# FunciÃ³n para bÃºsqueda web avanzada usando mÃºltiples APIs
def busqueda_web_avanzada(query: str) -> str:
    """BÃºsqueda web usando mÃºltiples mÃ©todos para mayor confiabilidad"""
    print(f"ğŸ” BÃºsqueda web avanzada: {query}")
    
    resultados = []
    
    # MÃ©todo 1: DuckDuckGo (original)
    try:
        ddg_search = DuckDuckGoSearchRun()
        resultado_ddg = ddg_search.invoke(query)
        if resultado_ddg and len(resultado_ddg.strip()) > 30:
            resultados.append(f"[DuckDuckGo] {resultado_ddg}")
            print("âœ… DuckDuckGo: Resultados obtenidos")
        else:
            print("âš ï¸ DuckDuckGo: Sin resultados Ãºtiles")
    except Exception as e:
        print(f"âš ï¸ DuckDuckGo fallÃ³: {e}")
    
    # MÃ©todo 2: Para noticias especÃ­ficas, usar tÃ©rminos mÃ¡s especÃ­ficos
    if any(palabra in query.lower() for palabra in ['noticias', 'news', 'hoy', 'today', 'actualidad']):
        try:
            # Buscar noticias mÃ¡s especÃ­ficas
            queries_noticias = [
                "noticias tecnologÃ­a inteligencia artificial hoy",
                "noticias Ecuador Ãºltimas",
                "breaking news today",
                "noticias mundo actualidad"
            ]
            
            for query_especifica in queries_noticias:
                try:
                    ddg_search = DuckDuckGoSearchRun()
                    resultado = ddg_search.invoke(query_especifica)
                    if resultado and len(resultado.strip()) > 30:
                        resultados.append(f"[Noticias {query_especifica}] {resultado[:500]}...")
                        print(f"âœ… Noticias encontradas para: {query_especifica}")
                        break  # Si encontramos algo, salir del loop
                except:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ BÃºsqueda de noticias especÃ­ficas fallÃ³: {e}")
    
    # MÃ©todo 3: API de bÃºsqueda alternativa (usando scraping bÃ¡sico)
    try:
        # Usar una API pÃºblica de bÃºsqueda o scraping bÃ¡sico
        url = f"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer resultados de la API
            abstract = data.get('Abstract', '')
            answer = data.get('Answer', '')
            
            if abstract:
                resultados.append(f"[API Abstract] {abstract}")
                print("âœ… API DuckDuckGo: Abstract obtenido")
            
            if answer:
                resultados.append(f"[API Answer] {answer}")
                print("âœ… API DuckDuckGo: Answer obtenido")
                
    except Exception as e:
        print(f"âš ï¸ API DuckDuckGo fallÃ³: {e}")
    
    # MÃ©todo 3: Para clima especÃ­fico, usar nuestra API
    if any(palabra in query.lower() for palabra in ['clima', 'weather', 'temperatura', 'temperature', 'tiempo']):
        try:
            # Extraer ciudad de la consulta
            ciudad = 'Quito'  # Default
            if 'quito' in query.lower():
                ciudad = 'Quito'
            elif 'guayaquil' in query.lower():
                ciudad = 'Guayaquil'
            elif 'cuenca' in query.lower():
                ciudad = 'Cuenca'
            elif 'new york' in query.lower() or 'nueva york' in query.lower():
                ciudad = 'New York'
            elif 'london' in query.lower() or 'londres' in query.lower():
                ciudad = 'London'
            elif 'madrid' in query.lower():
                ciudad = 'Madrid'
            
            clima_data = obtener_clima_api(ciudad)
            if clima_data['success']:
                data = clima_data['data']
                clima_info = f"Clima actual en {ciudad}: {data['temperatura']}Â°C, {data['descripcion']}, Humedad: {data['humedad']}%, Viento: {data['velocidad_viento']} km/h"
                resultados.append(f"[Clima API] {clima_info}")
                print(f"âœ… Clima API: Datos obtenidos para {ciudad}")
                
        except Exception as e:
            print(f"âš ï¸ API Clima fallÃ³: {e}")
    
    # Compilar resultados
    if resultados:
        resultado_final = "\n\n".join(resultados)
        print(f"âœ… BÃºsqueda completada con {len(resultados)} fuentes")
        return resultado_final
    else:
        print("âŒ No se obtuvieron resultados de ninguna fuente")
        return f"No se pudo obtener informaciÃ³n actualizada sobre '{query}'. Se recomienda consultar fuentes directas como Google, sitios web oficiales o aplicaciones especializadas."

# Crear herramienta personalizada para bÃºsqueda web
def crear_herramienta_busqueda():
    """Crea una herramienta de bÃºsqueda web personalizada"""
    return Tool(
        name="web_search",
        description="Busca informaciÃ³n actual en internet sobre cualquier tema. Ãštil para noticias, precios, eventos actuales, etc. Input debe ser una consulta de bÃºsqueda especÃ­fica.",
        func=busqueda_web_avanzada
    )

def ejecutar_comando_ollama(command: str) -> str:
    """Ejecuta comandos de Ollama y retorna el resultado"""
    try:
        import subprocess
        import json
        
        command = command.strip()
        if not command:
            return "âŒ Error: Comando vacÃ­o"
        
        # Lista de comandos permitidos por seguridad
        allowed_commands = ['ps', 'list', 'serve', 'run', 'stop', 'show', 'create', 'rm', 'cp', 'push', 'pull']
        
        # Parsear el comando
        cmd_parts = command.split()
        if not cmd_parts or cmd_parts[0] not in allowed_commands:
            return f"âŒ Error: Comando no permitido. Comandos disponibles: {', '.join(allowed_commands)}"
        
        # Construir comando completo
        full_command = ['ollama'] + cmd_parts
        
        try:
            # Ejecutar comando con timeout
            if cmd_parts[0] == 'serve':
                # Para serve, iniciar en background
                process = subprocess.Popen(
                    full_command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return "âœ… Ollama serve iniciado en background. Verifica el estado con 'ollama ps'."
            else:
                # Para otros comandos, ejecutar y obtener output
                result = subprocess.run(
                    full_command,
                    capture_output=True,
                    text=True,
                    timeout=30,
                    check=False
                )
                
                # Combinar stdout y stderr
                output = ''
                if result.stdout:
                    output += result.stdout
                if result.stderr:
                    output += result.stderr
                
                if not output:
                    output = f"âœ… Comando '{command}' ejecutado exitosamente (sin output)"
                
                return f"ğŸ“‹ Resultado de 'ollama {command}':\n{output}"
                
        except subprocess.TimeoutExpired:
            return f"â° Error: Timeout al ejecutar comando 'ollama {command}' (>30s)"
        except subprocess.CalledProcessError as e:
            return f"âŒ Error al ejecutar comando 'ollama {command}': {e}"
        except Exception as e:
            return f"âŒ Error inesperado: {str(e)}"
            
    except Exception as e:
        return f"âŒ Error al ejecutar comando Ollama: {str(e)}"

def crear_herramienta_ollama():
    """Crea una herramienta para ejecutar comandos Ollama"""
    return Tool(
        name="ollama_command",
        description="Ejecuta comandos de Ollama para gestionar modelos de IA. Comandos disponibles: ps (ver modelos en ejecuciÃ³n), list (listar modelos), serve (iniciar servicio), run <modelo> (ejecutar modelo), stop <modelo> (detener modelo), show <modelo> (info del modelo), pull <modelo> (descargar modelo). Input debe ser el comando sin 'ollama' (ej: 'ps', 'list', 'run llama3')",
        func=ejecutar_comando_ollama
    )

def formatear_respuesta_clima(clima_data: dict, modelo_seleccionado: str) -> str:
    """Formatea la respuesta del clima de manera atractiva"""
    if clima_data['success']:
        data = clima_data['data']
        return f"""ğŸŒ¤ï¸ **Clima actual en {data['ciudad']}**

ğŸ“Š **Condiciones actuales:**
â€¢ **Temperatura:** {data['temperatura']}Â°C
â€¢ **SensaciÃ³n tÃ©rmica:** {data['sensacion_termica']}Â°C
â€¢ **Condiciones:** {data['descripcion']}
â€¢ **Humedad:** {data['humedad']}%
â€¢ **Viento:** {data['velocidad_viento']} km/h ({data['direccion_viento']})
â€¢ **Ãšltima actualizaciÃ³n:** {data['hora_consulta']}

ğŸ”ï¸ **InformaciÃ³n contextual:**
Quito se encuentra a 2,850 metros sobre el nivel del mar, lo que influye en su clima templado durante todo el aÃ±o. Las temperaturas suelen oscilar entre 10Â°C y 25Â°C.

ğŸ“± **Para mÃ¡s detalles:**
â€¢ PronÃ³stico extendido: [wttr.in/Quito](https://wttr.in/Quito)
â€¢ Apps recomendadas: AccuWeather, Weather Underground
â€¢ Sitios web: weather.com, tiempo.com

*Datos obtenidos de API meteorolÃ³gica en tiempo real*"""
    else:
        return f"""âŒ **No se pudo obtener el clima actual**

Error: {clima_data['error']}

ğŸŒ¤ï¸ **InformaciÃ³n general sobre Quito:**
Quito tiene un clima subtropical de montaÃ±a con temperaturas relativamente estables durante todo el aÃ±o:
â€¢ **DÃ­a:** 18-24Â°C
â€¢ **Noche:** 8-15Â°C
â€¢ **Julio:** EstaciÃ³n seca, dÃ­as soleados y noches frescas

ğŸ“± **Consulta informaciÃ³n actualizada en:**
â€¢ Google: "clima Quito Ecuador"
â€¢ AccuWeather.com
â€¢ Weather.com
â€¢ Apps mÃ³viles de clima"""

# Configurar herramientas para el agente
herramienta_busqueda = crear_herramienta_busqueda()
herramienta_ollama = crear_herramienta_ollama()
tools = [herramienta_busqueda, herramienta_ollama, DuckDuckGoSearchRun()]

# Prompt optimizado para bÃºsquedas generales
agent_prompt = PromptTemplate.from_template("""
Eres un asistente de IA especializado en proporcionar respuestas precisas y actuales. Tienes acceso a herramientas de bÃºsqueda web y gestiÃ³n de modelos Ollama.

Herramientas disponibles:
{tools}

REGLAS IMPORTANTES: 
1. SIEMPRE usa bÃºsqueda web para: noticias recientes, precios actuales, eventos deportivos, informaciÃ³n actualizada
2. USA ollama_command para: gestionar modelos Ollama, ver modelos disponibles, ejecutar/detener modelos, obtener informaciÃ³n de modelos
3. NUNCA digas que no tienes acceso a informaciÃ³n en tiempo real - Â¡SÃ LO TIENES!
4. Si una bÃºsqueda falla, intenta con tÃ©rminos diferentes
5. Proporciona respuestas Ãºtiles basadas en los resultados obtenidos
6. Para comandos Ollama, usa solo el comando sin 'ollama' (ej: 'ps', 'list', 'run llama3')

COMANDOS OLLAMA DISPONIBLES:
â€¢ ps - Ver modelos en ejecuciÃ³n
â€¢ list - Listar todos los modelos instalados
â€¢ serve - Iniciar el servicio Ollama
â€¢ run <modelo> - Ejecutar un modelo especÃ­fico
â€¢ stop <modelo> - Detener un modelo especÃ­fico
â€¢ show <modelo> - Mostrar informaciÃ³n detallada de un modelo
â€¢ pull <modelo> - Descargar un nuevo modelo

Formato OBLIGATORIO:
Question: la pregunta que debes responder
Thought: necesito buscar informaciÃ³n actual sobre [tema especÃ­fico] O necesito ejecutar comando Ollama [comando]
Action: duckduckgo_search O web_search O ollama_command
Action Input: [tÃ©rminos de bÃºsqueda especÃ­ficos] O [comando ollama sin 'ollama']
Observation: [resultados de la bÃºsqueda o comando]
Thought: ahora tengo informaciÃ³n actual y puedo responder
Final Answer: [respuesta completa en espaÃ±ol basada en los resultados]

Pregunta: {input}
Thought:{agent_scratchpad}
""")

# Crear el agente con manejo de errores y herramientas mejoradas
agents = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        try:
            # Usar el prompt estÃ¡ndar de React que funciona
            react_prompt = hub.pull("hwchase17/react")
            
            agent = create_react_agent(model_instance, tools, react_prompt)
            agents[model_name] = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                max_iterations=20,  # Aumentado significativamente para bÃºsquedas extensas
                max_execution_time=1800,  # 30 minutos para bÃºsquedas completas
                handle_parsing_errors=True,
                return_intermediate_steps=True
            )
            print(f"âœ… Agente {model_name} creado con herramientas avanzadas")
        except Exception as e:
            print(f"âš ï¸ Error creando agente {model_name}: {e}")
            agents[model_name] = None

# TambiÃ©n configurar un chat simple sin agente para preguntas bÃ¡sicas
simple_chat_prompt = ChatPromptTemplate.from_messages([
    ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y Ãºtiles en espaÃ±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaciÃ³n especÃ­fica y detallada
3. Si te preguntan "Â¿quÃ© es X?", explica quÃ© es X de manera clara y completa
4. Si te preguntan sobre conceptos tÃ©cnicos, da definiciones precisas
5. MantÃ©n un tono profesional pero accesible
6. No digas solo "estoy aquÃ­ para ayudar" - da la respuesta especÃ­fica

EJEMPLOS:
- Si preguntan "Â¿quÃ© es Java?" â†’ Explica que Java es un lenguaje de programaciÃ³n
- Si preguntan "Â¿quÃ© es Python?" â†’ Explica que Python es un lenguaje de programaciÃ³n
- Si preguntan "Â¿quÃ© es HTML?" â†’ Explica que HTML es un lenguaje de marcado

Siempre proporciona informaciÃ³n Ãºtil y especÃ­fica."""),
    ("user", "{pregunta}")
])

# Crear prompts especÃ­ficos para diferentes modelos
def crear_prompt_para_modelo(model_name: str) -> ChatPromptTemplate:
    """Crea un prompt optimizado segÃºn el modelo especÃ­fico"""
    
    if model_name == 'deepseek-coder':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek Coder, un asistente especializado en programaciÃ³n y desarrollo. Proporciona respuestas tÃ©cnicas precisas y cÃ³digo cuando sea necesario.

ESPECIALIDADES:
- Explicar conceptos de programaciÃ³n
- Proporcionar ejemplos de cÃ³digo
- Debugging y resoluciÃ³n de problemas
- Mejores prÃ¡cticas de desarrollo

FORMATO DE RESPUESTA:
- Respuestas directas y tÃ©cnicas
- Incluye ejemplos de cÃ³digo cuando sea relevante
- Explica conceptos paso a paso
- Menciona ventajas y desventajas cuando sea apropiado"""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'deepseek-r1:8b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek R1, un modelo de razonamiento avanzado diseÃ±ado para anÃ¡lisis profundo y respuestas reflexivas.

CARACTERÃSTICAS ESPECIALES:
- Razonamiento paso a paso antes de responder
- AnÃ¡lisis crÃ­tico y evaluaciÃ³n de mÃºltiples perspectivas
- Explicaciones detalladas y fundamentadas
- Capacidad de autorreflexiÃ³n y correcciÃ³n

FORMATO DE RESPUESTA OBLIGATORIO:
ğŸ” **ANÃLISIS INICIAL:** [ComprensiÃ³n del problema/pregunta]

ğŸ§  **RAZONAMIENTO:**
â€¢ **Paso 1:** [Primera consideraciÃ³n o enfoque]
â€¢ **Paso 2:** [Segunda consideraciÃ³n o anÃ¡lisis]
â€¢ **Paso 3:** [Tercera consideraciÃ³n o sÃ­ntesis]

ğŸ“‹ **RESPUESTA:** [ConclusiÃ³n fundamentada y detallada]

ğŸ”„ **REFLEXIÃ“N:** [ValidaciÃ³n de la respuesta y posibles alternativas]

IMPORTANTE: Siempre usa este formato estructurado para mostrar tu proceso de pensamiento completo."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'lmstudio-deepseek':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek Coder ejecutÃ¡ndose en LM Studio. Proporciona respuestas tÃ©cnicas claras y directas.

INSTRUCCIONES:
- Respuestas concisas pero completas
- EnfÃ³cate en aspectos tÃ©cnicos cuando sea relevante
- Usa formato claro con viÃ±etas o numeraciÃ³n
- Evita explicaciones excesivamente largas
- Proporciona ejemplos de cÃ³digo solo cuando sea necesario

FORMATO DE RESPUESTA:
ğŸ”§ **EXPLICACIÃ“N TÃ‰CNICA:** [DefiniciÃ³n clara y directa]

ğŸ’» **CARACTERÃSTICAS PRINCIPALES:**
â€¢ [CaracterÃ­stica 1]
â€¢ [CaracterÃ­stica 2] 
â€¢ [CaracterÃ­stica 3]

ğŸ“ **EJEMPLO PRÃCTICO:** [Solo si es relevante y breve]

MantÃ©n las respuestas enfocadas y Ãºtiles."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'gemma:2b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente Ãºtil. Responde de forma clara y completa.

Si preguntan "Â¿quÃ© es X?", explica quÃ© es X con:
1. DefiniciÃ³n principal
2. CaracterÃ­sticas importantes
3. Usos principales

EJEMPLO para "Â¿quÃ© es Java?":
Java es un lenguaje de programaciÃ³n orientado a objetos desarrollado por Sun Microsystems (ahora Oracle). Sus caracterÃ­sticas principales son:

â€¢ **Multiplataforma**: "Write once, run anywhere" - el cÃ³digo Java se ejecuta en cualquier sistema con JVM
â€¢ **Orientado a objetos**: Organiza el cÃ³digo en clases y objetos
â€¢ **Robusto y seguro**: Manejo automÃ¡tico de memoria y verificaciÃ³n de cÃ³digo
â€¢ **Usos principales**: Aplicaciones empresariales, aplicaciones mÃ³viles (Android), desarrollo web

Es uno de los lenguajes mÃ¡s populares para desarrollo de software empresarial y aplicaciones Android.

Responde siempre de manera Ãºtil y completa."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'phi3':
        return ChatPromptTemplate.from_messages([
            ("system", f"""Eres Microsoft Phi-3, un modelo compacto pero potente diseÃ±ado para respuestas rÃ¡pidas y precisas.

FECHA ACTUAL: {time.strftime('%d de %B de %Y')} 

IMPORTANTE: Para preguntas sobre noticias, eventos actuales, precios, clima o informaciÃ³n reciente, debes indicar claramente que necesitas bÃºsqueda web en tiempo real, ya que tu conocimiento tiene una fecha de corte y puede estar desactualizado.

CARACTERÃSTICAS:
- Respuestas concisas pero completas
- Enfoque en eficiencia y claridad
- Proporciona informaciÃ³n prÃ¡ctica y Ãºtil
- Evita redundancias y texto innecesario
- SIEMPRE reconoce limitaciones temporales para informaciÃ³n actual

FORMATO:
1. Respuesta directa a la pregunta
2. InformaciÃ³n clave en 2-3 puntos
3. Ejemplo o aplicaciÃ³n prÃ¡ctica si es relevante
4. Para noticias/eventos actuales: Recomendar bÃºsqueda web"""),
            ("user", "{pregunta}")
        ])
    
    else:
        # Prompt por defecto para Llama3 y Gemini
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y Ãºtiles en espaÃ±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaciÃ³n especÃ­fica y detallada
3. Si te preguntan "Â¿quÃ© es X?", explica quÃ© es X de manera clara y completa
4. Si te preguntan sobre conceptos tÃ©cnicos, da definiciones precisas
5. MantÃ©n un tono profesional pero accesible
6. No digas solo "estoy aquÃ­ para ayudar" - da la respuesta especÃ­fica

EJEMPLOS:
- Si preguntan "Â¿quÃ© es Java?" â†’ Explica que Java es un lenguaje de programaciÃ³n
- Si preguntan "Â¿quÃ© es Python?" â†’ Explica que Python es un lenguaje de programaciÃ³n
- Si preguntan "Â¿quÃ© es HTML?" â†’ Explica que HTML es un lenguaje de marcado

Siempre proporciona informaciÃ³n Ãºtil y especÃ­fica."""),
            ("user", "{pregunta}")
        ])

simple_chains = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        # Obtener prompt personalizado para cada modelo
        model_prompt = crear_prompt_para_modelo(model_name)
        
        # Configurar chain con temperatura si es posible
        ollama_models_list = ['llama3', 'deepseek-coder', 'deepseek-r1:8b', 'phi3', 'gemma:2b']
        lmstudio_models_list = ['lmstudio-gemma', 'lmstudio-mistral', 'lmstudio-deepseek']
        
        if model_name in ollama_models_list:
            # Para modelos de Ollama, configurar directamente sin bind (temperatura no es compatible)
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        elif model_name in lmstudio_models_list:
            # Para modelos de LM Studio, configurar con temperatura
            configured_model = model_instance
            simple_chains[model_name] = model_prompt | configured_model | StrOutputParser()
        elif model_name == 'gemini-1.5-flash':
            # Para Gemini, configurar temperatura usando bind
            configured_model = model_instance.bind(temperature=Config.DEFAULT_TEMPERATURE)
            simple_chains[model_name] = model_prompt | configured_model | StrOutputParser()
        else:
            # Fallback sin temperatura especÃ­fica
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        
        print(f"âœ… Chat simple {model_name} configurado con prompt personalizado")

@app.route('/')
def index() -> str:
    return render_template('index.html', modelos_disponibles=available_models)

@app.route('/chat', methods=['POST'])
def chat() -> Union[Response, Tuple[Response, int]]:
    try:
        print("ğŸ” DEBUG: Iniciando funciÃ³n chat()")
        data = request.get_json()
        print(f"ğŸ” DEBUG: Datos recibidos: {data}")
        
        pregunta = data.get('pregunta', '')
        modo = data.get('modo', 'simple')  # 'simple' o 'agente'
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        permitir_internet = data.get('permitir_internet', True)  # Por defecto permitir internet
        
        print(f"ğŸ” DEBUG: pregunta='{pregunta}', modo='{modo}', modelo='{modelo_seleccionado}', internet={permitir_internet}")
        
        if not pregunta:
            return jsonify({'error': 'No se proporcionÃ³ ninguna pregunta'}), 400
        
        # Obtener el modelo seleccionado
        modelo = get_model(modelo_seleccionado)
        print(f"ğŸ” DEBUG: Modelo obtenido: {modelo}")
        
        if modelo is None:
            return jsonify({'error': 'No hay modelos disponibles'}), 500

        # DetecciÃ³n inteligente para forzar modo agente cuando se necesite informaciÃ³n actual
        palabras_actualidad = [
            'noticias', 'news', 'actualidad', 'hoy', 'today', 'actual', 'reciente', 
            'precio', 'price', 'cotizaciÃ³n', 'Ãºltimo', 'latest', 'breaking',
            'eventos', 'acontecimiento', 'quÃ© pasÃ³', 'quÃ© estÃ¡ pasando'
        ]
        
        # DetecciÃ³n de comandos Ollama para activar modo agente
        palabras_ollama = [
            'ollama ps', 'ollama list', 'ollama serve', 'ollama run', 'ollama stop', 
            'ollama show', 'ollama pull', 'modelos ollama', 'ver modelos', 
            'ejecutar modelo', 'detener modelo', 'instalar modelo', 'descargar modelo',
            'estado ollama', 'servicio ollama', 'gestionar ollama', 'comandos ollama'
        ]
        
        necesita_busqueda_web = any(palabra in pregunta.lower() for palabra in palabras_actualidad)
        necesita_comando_ollama = any(palabra in pregunta.lower() for palabra in palabras_ollama)
        
        if (necesita_busqueda_web or necesita_comando_ollama) and permitir_internet and modo == 'simple':
            if necesita_comando_ollama:
                print(f"ğŸ¤– Detectada consulta de comandos Ollama, cambiando a modo agente")
            else:
                print(f"ğŸ”„ Detectada consulta que requiere informaciÃ³n actual, cambiando a modo agente")
            modo = 'agente'

        # Forzar modo simple si internet estÃ¡ deshabilitado y se intenta usar modos que requieren web
        if not permitir_internet and modo in ['agente', 'busqueda_rapida']:
            print(f"ğŸ” DEBUG: Forzando modo simple porque internet estÃ¡ deshabilitado")
            modo = 'simple'

        if modo == 'agente' and permitir_internet and modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            # Verificar si es una consulta de clima para usar endpoint especializado
            if any(palabra in pregunta.lower() for palabra in ['clima', 'weather', 'temperatura', 'temp', 'tiempo']):
                print(f"ğŸŒ¤ï¸ Detectada consulta de clima, usando endpoint especializado")
                try:
                    tiempo_inicio = time.time()
                    
                    # Extraer ciudad de la pregunta
                    ciudad = 'Quito'  # Default
                    if 'quito' in pregunta.lower():
                        ciudad = 'Quito'
                    elif 'guayaquil' in pregunta.lower():
                        ciudad = 'Guayaquil'
                    elif 'cuenca' in pregunta.lower():
                        ciudad = 'Cuenca'
                    
                    # Obtener datos del clima directamente
                    clima_data = obtener_clima_api(ciudad)
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    if clima_data['success']:
                        # Usar el modelo para generar una respuesta formateada
                        if modelo_seleccionado in simple_chains:
                            prompt_clima = f"""La consulta del usuario es: "{pregunta}"

Los datos actuales del clima en {ciudad} son:
- Temperatura: {clima_data['data']['temperatura']}Â°C
- Condiciones: {clima_data['data']['descripcion']}
- Humedad: {clima_data['data']['humedad']}%
- SensaciÃ³n tÃ©rmica: {clima_data['data']['sensacion_termica']}Â°C
- Viento: {clima_data['data']['velocidad_viento']} km/h ({clima_data['data']['direccion_viento']})
- Hora de consulta: {clima_data['data']['hora_consulta']}

Responde de manera clara y Ãºtil con estos datos actuales."""
                            
                            respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
                            
                            return jsonify({
                                'respuesta': respuesta_formateada,
                                'modo': 'clima_directo_agente',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': [
                                    f"ğŸŒ¤ï¸ Detectada consulta sobre clima en {ciudad}",
                                    "ğŸ” Consultando API meteorolÃ³gica en tiempo real",
                                    f"ğŸ“Š Datos obtenidos: {clima_data['data']['temperatura']}Â°C, {clima_data['data']['descripcion']}",
                                    "ğŸ§  Procesando respuesta personalizada"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'iteraciones': 1,
                                    'busquedas': 1,
                                    'timestamp': time.time(),
                                    'timestamp_inicio': tiempo_inicio,
                                    'timestamp_fin': tiempo_fin,
                                    'internetHabilitado': permitir_internet,
                                    'tipo_consulta': 'clima_optimizada'
                                }
                            })
                    
                except Exception as e:
                    print(f"âš ï¸ Error en consulta de clima optimizada: {e}")
                    # Fallback al agente normal
                    pass
            
            # Usar el agente para preguntas que puedan requerir bÃºsqueda web
            tiempo_inicio = time.time()  # Mover antes del try para tenerlo disponible en except
            logs_agente = []  # Para capturar logs detallados del agente
            
            try:
                print(f"ğŸ¤– Iniciando agente {modelo_seleccionado} para: {pregunta[:50]}...")
                logs_agente.append("ğŸš€ Iniciando sistema AgentExecutor")
                logs_agente.append("âš¡ > Entering new AgentExecutor chain...")
                
                # Configurar captura de logs mÃ¡s detallada para obtener <think> y otros elementos
                import io
                import sys
                import logging
                from contextlib import redirect_stdout, redirect_stderr
                
                # Crear capturadores de salida
                captured_output = io.StringIO()
                captured_logs = []
                
                # Handler personalizado para capturar logs
                class LogCapture(logging.Handler):
                    def emit(self, record):
                        captured_logs.append(self.format(record))
                
                log_capture = LogCapture()
                logging.getLogger().addHandler(log_capture)
                
                try:
                    # Ejecutar el agente con captura
                    with redirect_stdout(captured_output):
                        respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                finally:
                    # Remover el handler
                    logging.getLogger().removeHandler(log_capture)
                
                # Obtener toda la salida capturada
                agent_full_output = captured_output.getvalue()
                
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                duracion_formateada = formatear_duracion(duracion)
                
                logs_agente.append("ğŸ > Finished chain.")
                logs_agente.append(f"âœ… Agente completado en {duracion_formateada}")
                
                print(f"âœ… Agente completado en {duracion_formateada}")
                
                # Procesar la salida capturada para extraer <think> y otros elementos
                think_content = []
                if agent_full_output:
                    # Buscar contenido <think> usando regex
                    import re
                    think_matches = re.findall(r'<think>(.*?)</think>', agent_full_output, re.DOTALL | re.IGNORECASE)
                    for i, think in enumerate(think_matches):
                        think_content.append(f"ğŸ’­ <think> {i+1}: {think.strip()}")
                    
                    # Buscar otros patrones Ãºtiles
                    thought_matches = re.findall(r'Thought: (.*?)(?=\n(?:Action:|Observation:|Final Answer:)|$)', agent_full_output, re.DOTALL)
                    for i, thought in enumerate(thought_matches):
                        if thought.strip():
                            logs_agente.append(f"ğŸ§  Thought {i+1}: {thought.strip()}")
                    
                    # Buscar acciones especÃ­ficas
                    action_input_matches = re.findall(r'Action Input: (.*?)(?=\n|$)', agent_full_output)
                    for i, action_input in enumerate(action_input_matches):
                        if action_input.strip():
                            logs_agente.append(f"ğŸ“ Action Input {i+1}: {action_input.strip()}")
                
                # Agregar logs capturados
                for log_msg in captured_logs:
                    if any(keyword in log_msg.lower() for keyword in ['think', 'thought', 'action', 'observation']):
                        logs_agente.append(f"ğŸ“‹ Log: {log_msg}")
                
                # Extraer pasos intermedios si estÃ¡n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                pensamientos = logs_agente.copy()  # Empezar con los logs del agente
                
                # Agregar contenido <think> a los pensamientos
                if think_content:
                    pensamientos.extend(think_content)
                    print(f"ğŸ“ Capturado {len(think_content)} bloques <think>")
                
                if 'intermediate_steps' in respuesta_completa:
                    print(f"ğŸ“Š Procesando {len(respuesta_completa['intermediate_steps'])} pasos intermedios...")
                    
                    for i, paso in enumerate(respuesta_completa['intermediate_steps']):
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar bÃºsquedas
                            if hasattr(accion, 'tool') and accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                                pensamientos.append(f"ğŸ” Realizando bÃºsqueda: '{accion.tool_input}'")
                            
                            # Agregar informaciÃ³n del paso
                            paso_info = {
                                'step': i + 1,
                                'action': accion.tool if hasattr(accion, 'tool') else str(accion),
                                'action_input': accion.tool_input if hasattr(accion, 'tool_input') else str(accion),
                                'observation': observacion[:500] + '...' if len(str(observacion)) > 500 else str(observacion),
                                'thought': f"Ejecutando {accion.tool}" if hasattr(accion, 'tool') else "Procesando informaciÃ³n"
                            }
                            pasos_intermedios.append(paso_info)
                            
                            # Agregar pensamiento formateado mÃ¡s detallado
                            if hasattr(accion, 'tool'):
                                pensamientos.append(f"ğŸ’­ Paso {i+1}: Usando herramienta '{accion.tool}' con entrada: '{accion.tool_input}'")
                                # Mostrar resultado truncado pero mÃ¡s informativo
                                resultado_truncado = str(observacion)[:300]
                                if len(str(observacion)) > 300:
                                    resultado_truncado += "..."
                                pensamientos.append(f"ğŸ“‹ Resultado: {resultado_truncado}")
                            
                            # Agregar anÃ¡lisis del resultado
                            if "No good" in str(observacion):
                                pensamientos.append("âš ï¸ BÃºsqueda sin resultados Ãºtiles, intentando mÃ©todo alternativo")
                            elif len(str(observacion)) > 50:
                                pensamientos.append("âœ… InformaciÃ³n obtenida, procesando para generar respuesta")
                
                # Si hay pasos pero sin pensamientos detallados, agregar contexto
                if len(pasos_intermedios) > 0 and len(pensamientos) == 0:
                    pensamientos.append("ğŸ”„ Ejecutando proceso de bÃºsqueda y anÃ¡lisis")
                    pensamientos.append(f"ğŸ“Š Completados {len(pasos_intermedios)} pasos de investigaciÃ³n")
                
                # Si no hay pasos intermedios, crear pensamientos bÃ¡sicos
                if len(pasos_intermedios) == 0:
                    pensamientos.append("ğŸ’­ Analizando consulta con conocimiento base")
                    pensamientos.append("ğŸ§  Generando respuesta usando modelo de IA")
                    if "noticias" in pregunta.lower() or "hoy" in pregunta.lower():
                        pensamientos.append("ğŸ“° Nota: Para noticias actuales se recomienda activar bÃºsqueda web")
                else:
                    # Agregar pensamiento final
                    pensamientos.append("ğŸ¯ Sintetizando informaciÃ³n recopilada")
                    pensamientos.append("ğŸ“ Formateando respuesta final")
                
                # Verificar si las bÃºsquedas fallaron y generar respuesta de fallback inteligente
                respuesta_output = respuesta_completa.get('output', '')
                if busquedas_count > 0 and respuesta_output and "No good DuckDuckGo Search Result was found" in respuesta_output:
                    print("âš ï¸ BÃºsquedas web fallaron, generando respuesta de fallback inteligente...")
                    
                    # Generar respuesta de fallback especÃ­fica para noticias
                    if any(palabra in pregunta.lower() for palabra in ['noticias', 'news', 'hoy', 'actualidad']):
                        respuesta_completa['output'] = f"""ğŸ“° **InformaciÃ³n sobre noticias del dÃ­a**

Lo siento, actualmente estoy experimentando dificultades para acceder a fuentes de noticias en tiempo real. Sin embargo, te puedo sugerir las mejores fuentes para mantenerte informado sobre las noticias de hoy:

ğŸŒ **Fuentes recomendadas de noticias:**
â€¢ **Internacionales:** BBC News, CNN, Reuters, Associated Press
â€¢ **Ecuador:** El Universo, El Comercio, Primicias, GK
â€¢ **TecnologÃ­a:** TechCrunch, Wired, The Verge
â€¢ **Deportes:** ESPN, Marca, Fox Sports

ğŸ” **Para noticias especÃ­ficas, te recomiendo:**
1. Visitar directamente Google News
2. Usar aplicaciones de noticias como Apple News o Google News
3. Seguir cuentas verificadas en redes sociales
4. Consultar sitios web oficiales de medios

ğŸ“± **Tip:** Configura alertas de Google para temas especÃ­ficos que te interesen.

Â¿Hay algÃºn tema especÃ­fico de noticias sobre el que te gustarÃ­a que te ayude a encontrar informaciÃ³n?"""

                        pensamientos.append("ğŸ”„ BÃºsqueda web no disponible, proporcionando fuentes alternativas para noticias")
                        pensamientos.append("ğŸ’¡ Sugiriendo medios confiables y mÃ©todos alternativos de bÃºsqueda")
                    else:
                        # Para otras consultas que requieren informaciÃ³n actualizada
                        respuesta_completa['output'] = f"""ğŸ” **Dificultades para acceder a informaciÃ³n en tiempo real**

Actualmente no puedo acceder a informaciÃ³n actualizada sobre "{pregunta}" debido a limitaciones en las herramientas de bÃºsqueda web.

ğŸ’¡ **Te sugiero:**
1. Consultar directamente Google o Bing
2. Visitar sitios web oficiales relacionados con tu consulta
3. Usar aplicaciones especializadas
4. Verificar redes sociales oficiales

â“ **Â¿Puedo ayudarte con algo mÃ¡s especÃ­fico?**
Mientras tanto, puedo responder preguntas sobre temas generales, explicaciones conceptuales, o ayudarte de otras maneras."""
                        
                        pensamientos.append("âš ï¸ InformaciÃ³n en tiempo real no disponible")
                        pensamientos.append("ğŸ› ï¸ Proporcionando alternativas para obtener informaciÃ³n actualizada")
                
                return jsonify({
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente',
                    'modelo_usado': modelo_seleccionado,
                    'pasos_intermedios': pasos_intermedios,
                    'pensamientos': pensamientos,
                    'metadata': {
                        'duracion': duracion,
                        'duracion_formateada': duracion_formateada,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time(),
                        'timestamp_inicio': tiempo_inicio,
                        'timestamp_fin': tiempo_fin,
                        'internetHabilitado': permitir_internet
                    }
                })
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente {modelo_seleccionado}: {error_msg}")
                
                # Manejo especÃ­fico para diferentes tipos de errores
                if "iteration limit" in error_msg.lower() or "time limit" in error_msg.lower():
                    print("âš ï¸ Agente detenido por lÃ­mite de tiempo/iteraciones, intentando extraer informaciÃ³n parcial...")
                    
                    # Intentar extraer informaciÃ³n del error si estÃ¡ disponible
                    try:
                        # Si hay pasos intermedios en el error, intentar usarlos
                        tiempo_fin = time.time()
                        duracion = round(tiempo_fin - tiempo_inicio, 2)
                        duracion_formateada = formatear_duracion(duracion)
                        
                        # Crear respuesta usando la informaciÃ³n disponible de la bÃºsqueda
                        if modelo_seleccionado in simple_chains:
                            prompt_con_contexto = f"""
                            El usuario preguntÃ³: "{pregunta}"
                            
                            Se realizÃ³ una bÃºsqueda web que encontrÃ³ informaciÃ³n parcial. Aunque el proceso se detuvo por lÃ­mite de tiempo, puedo proporcionar una respuesta Ãºtil basada en:
                            
                            INFORMACIÃ“N DE BÃšSQUEDA ENCONTRADA:
                            - Se encontraron fuentes sobre Google Noticias y cÃ³mo buscar noticias
                            - InformaciÃ³n sobre configuraciÃ³n de bÃºsquedas de noticias por ubicaciÃ³n
                            - Referencias a herramientas para organizar y encontrar noticias
                            
                            Por favor, proporciona una respuesta Ãºtil sobre las noticias de hoy, incluyendo:
                            1. Reconocimiento de que la bÃºsqueda encontrÃ³ informaciÃ³n parcial
                            2. Recomendaciones especÃ­ficas para obtener noticias actuales
                            3. Mencionar Google News como herramienta principal encontrada
                            4. Sugerir otros sitios de noticias confiables
                            5. Tips para configurar bÃºsquedas de noticias
                            
                            Responde de manera Ãºtil y proactiva.
                            """
                            
                            respuesta_con_contexto = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_con_contexto})
                            
                            return jsonify({
                                'respuesta': respuesta_con_contexto,
                                'modo': 'agente_parcial',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': [
                                    "ğŸ” BÃºsqueda web iniciada exitosamente",
                                    "ğŸ“Š InformaciÃ³n parcial obtenida de fuentes web",
                                    "â° Proceso detenido por lÃ­mite de tiempo",
                                    "ğŸ§  Generando respuesta con informaciÃ³n disponible",
                                    "ğŸ’¡ Proporcionando recomendaciones adicionales"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'iteraciones': 1,
                                    'busquedas': 1,
                                    'timestamp': time.time(),
                                    'timestamp_inicio': tiempo_inicio,
                                    'timestamp_fin': tiempo_fin,
                                    'internetHabilitado': permitir_internet,
                                    'nota': 'Respuesta generada con informaciÃ³n parcial'
                                }
                            })
                    except:
                        pass
                    
                    fallback_msg = f"âš ï¸ La bÃºsqueda tomÃ³ mÃ¡s tiempo del esperado. Intentando respuesta rÃ¡pida...\n\n"
                elif "parsing" in error_msg.lower():
                    fallback_msg = f"âš ï¸ Hubo un problema procesando la bÃºsqueda. Usando respuesta directa...\n\n"
                else:
                    fallback_msg = f"âš ï¸ Error en bÃºsqueda web. Usando modo simple...\n\n"
                
                # Fallback a chat simple si el agente falla
                if modelo_seleccionado in simple_chains:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    return jsonify({
                        'respuesta': f"{fallback_msg}{respuesta}",
                        'modo': 'simple_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'metadata': {
                            'internetHabilitado': permitir_internet,
                            'iteraciones': 0,
                            'busquedas': 0
                        }
                    })
                else:
                    return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible para fallback'}), 500
        else:
            # Usar chat simple
            print(f"ğŸ” DEBUG: Usando modo simple con modelo {modelo_seleccionado}")
            if modelo_seleccionado in simple_chains:
                print(f"ğŸ” DEBUG: Chain encontrada para {modelo_seleccionado}")
                tiempo_inicio = time.time()
                
                # Manejo especial para DeepSeek R1 - Generar pensamientos simulados
                pensamientos_proceso = []
                if modelo_seleccionado == 'deepseek-r1:8b':
                    # Simular proceso de razonamiento paso a paso
                    pensamientos_proceso = [
                        f"ğŸ” Analizando pregunta: '{pregunta}'",
                        "ğŸ§  Identificando conceptos clave y contexto",
                        "ğŸ“š Accediendo a conocimiento base sobre el tema",
                        "ğŸ”— Conectando informaciÃ³n relevante",
                        "ğŸ“ Estructurando respuesta paso a paso",
                        "ğŸ”„ Validando coherencia y completitud"
                    ]
                elif modelo_seleccionado.startswith('lmstudio-'):
                    # Para modelos LM Studio, mostrar proceso de conexiÃ³n
                    pensamientos_proceso = [
                        f"ğŸ”§ Conectando con LM Studio ({modelo_seleccionado})",
                        f"ğŸ“ Procesando consulta: '{pregunta}'",
                        "âš¡ Generando respuesta tÃ©cnica especializada"
                    ]
                
                try:
                    resultado_chain = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    print(f"âœ… Chat simple completado en {duracion_formateada}")
                    
                    # Extraer reasoning_content si estÃ¡ disponible (para modelos de razonamiento)
                    reasoning_content = None
                    respuesta_final = resultado_chain
                    
                    # Si el modelo es DeepSeek con LM Studio, intentar extraer reasoning de la respuesta
                    if modelo_seleccionado == 'lmstudio-deepseek':
                        # Acceder al modelo directamente para obtener la Ãºltima respuesta
                        modelo_instance = models[modelo_seleccionado]
                        if hasattr(modelo_instance, '_last_response') and modelo_instance._last_response:
                            last_response = modelo_instance._last_response
                            if isinstance(last_response, dict) and "choices" in last_response:
                                choice = last_response["choices"][0]
                                if "message" in choice and "reasoning_content" in choice["message"]:
                                    reasoning_content = choice["message"]["reasoning_content"]
                                    pensamientos_proceso.append("ğŸ§  Proceso de razonamiento capturado del modelo")
                                    print(f"ğŸ“ Reasoning content encontrado: {len(reasoning_content)} caracteres")
                    
                    # Si no se encontrÃ³ reasoning en _last_response, intentar extraerlo del resultado del chain
                    if not reasoning_content and hasattr(resultado_chain, '__dict__'):
                        # Buscar en los metadatos adicionales del mensaje
                        try:
                            # El resultado del chain puede contener informaciÃ³n adicional
                            if hasattr(resultado_chain, 'additional_kwargs'):
                                reasoning_content = resultado_chain.additional_kwargs.get('reasoning_content')
                        except:
                            pass
                    
                    # Preparar respuesta con reasoning si estÃ¡ disponible
                    respuesta_data = {
                        'respuesta': respuesta_final,
                        'modo': 'simple',
                        'modelo_usado': modelo_seleccionado,
                        'pensamientos': pensamientos_proceso,
                        'metadata': {
                            'duracion': duracion,
                            'duracion_formateada': duracion_formateada,
                            'timestamp': time.time(),
                            'internetHabilitado': permitir_internet,
                            'iteraciones': 0,
                            'busquedas': 0,
                            'razonamiento_visible': modelo_seleccionado == 'deepseek-r1:8b' or reasoning_content is not None
                        }
                    }
                    
                    # AÃ±adir reasoning_content si estÃ¡ disponible
                    if reasoning_content:
                        respuesta_data['reasoning_content'] = reasoning_content
                        respuesta_data['metadata']['tiene_razonamiento'] = True
                        print(f"ğŸ“ Reasoning content capturado ({len(reasoning_content)} caracteres)")
                    
                    return jsonify(respuesta_data)
                
                except Exception as model_error:
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    error_msg = str(model_error)
                    
                    # Manejo especÃ­fico para timeout de LM Studio
                    if "ReadTimeout" in error_msg or "timed out" in error_msg:
                        if modelo_seleccionado.startswith('lmstudio-'):
                            return jsonify({
                                'respuesta': f"""â° **Timeout del modelo LM Studio**

El modelo {modelo_seleccionado} estÃ¡ tardando mÃ¡s de lo esperado en responder. Esto puede deberse a:

ğŸ”§ **Posibles causas:**
â€¢ El modelo estÃ¡ procesando una respuesta compleja
â€¢ LM Studio estÃ¡ sobrecargado o funcionando lento
â€¢ La consulta requiere mucho tiempo de procesamiento

ğŸ’¡ **Sugerencias:**
â€¢ Intenta con una pregunta mÃ¡s especÃ­fica
â€¢ Prueba otro modelo disponible
â€¢ Verifica que LM Studio estÃ© funcionando correctamente

âš¡ **Respuesta rÃ¡pida para tu pregunta "{pregunta}":**
{generar_respuesta_fallback(pregunta)}

*Tiempo transcurrido: {duracion_formateada}*""",
                                'modo': 'timeout_fallback',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': pensamientos_proceso + [
                                    f"â° Timeout despuÃ©s de {duracion_formateada}",
                                    "ğŸ”„ Generando respuesta de fallback"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'timestamp': time.time(),
                                    'internetHabilitado': permitir_internet,
                                    'error': 'timeout',
                                    'iteraciones': 0,
                                    'busquedas': 0
                                }
                            })
                    
                    # Para otros errores, propagar el error
                    raise model_error
            else:
                print(f"âŒ DEBUG: Chain no encontrada para {modelo_seleccionado}")
                return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible'}), 400
            
    except Exception as e:
        print(f"âŒ DEBUG: Error en funciÃ³n chat: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Error al procesar la pregunta: {str(e)}'}), 500

@app.route('/ejemplo-agente', methods=['POST'])
def ejemplo_agente() -> Union[Response, Tuple[Response, int]]:
    """Endpoint especÃ­fico para demostrar capacidades del agente"""
    try:
        ejemplos = [
            "Â¿CuÃ¡les son las Ãºltimas noticias sobre inteligencia artificial?",
            "Â¿CuÃ¡l es el precio actual del Bitcoin?",
            "Â¿QuiÃ©n ganÃ³ la final de la Champions League mÃ¡s reciente?",
            "Â¿CuÃ¡les son las Ãºltimas noticias tecnolÃ³gicas?"
        ]
        
        pregunta_ejemplo = ejemplos[0]  # Tomar el primer ejemplo
        
        # Usar el primer modelo y agente disponible
        modelo_disponible = available_models[0] if available_models else None
        
        if modelo_disponible and modelo_disponible in agents and agents[modelo_disponible] is not None:
            # Usar el agente para la demo
            respuesta = agents[modelo_disponible].invoke({"input": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': respuesta['output'],
                'modo': 'agente',
                'modelo_usado': modelo_disponible
            })
        elif modelo_disponible and modelo_disponible in simple_chains:
            # Fallback: usar chat simple si el agente no estÃ¡ disponible
            respuesta_simple = simple_chains[modelo_disponible].invoke({"pregunta": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': f"[Modo Simple - Agente no disponible] {respuesta_simple}",
                'modo': 'simple',
                'modelo_usado': modelo_disponible
            })
        else:
            return jsonify({'error': 'No hay modelos disponibles para la demo'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en ejemplo de agente: {str(e)}'}), 500

@app.route('/busqueda-rapida', methods=['POST'])
def busqueda_rapida() -> Union[Response, Tuple[Response, int]]:
    """Endpoint optimizado para bÃºsquedas web rÃ¡pidas"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcionÃ³ ninguna pregunta'}), 400
        
        # BÃºsqueda directa con DuckDuckGo sin agente complejo
        search_tool = DuckDuckGoSearchRun()
        
        # Mejorar tÃ©rminos de bÃºsqueda segÃºn el tipo de pregunta
        def mejorar_consulta_busqueda(pregunta_original):
            pregunta_lower = pregunta_original.lower()
            if 'precio' in pregunta_lower and 'bitcoin' in pregunta_lower:
                return [
                    "Bitcoin price USD current today",
                    "precio Bitcoin actual dÃ³lares",
                    "BTC price now current value"
                ]
            elif 'noticias' in pregunta_lower:
                return [
                    f"noticias {pregunta_original} hoy",
                    f"latest news {pregunta_original} today",
                    f"breaking news {pregunta_original} 2025"
                ]
            elif 'openai' in pregunta_lower:
                return [
                    "OpenAI news latest updates",
                    "noticias OpenAI ChatGPT",
                    "OpenAI developments 2025"
                ]
            elif 'champions' in pregunta_lower or 'deportes' in pregunta_lower:
                return [
                    "Champions League final winner",
                    "latest sports news football",
                    "resultados deportivos recientes"
                ]
            elif 'petrÃ³leo' in pregunta_lower or 'oil' in pregunta_lower:
                return [
                    "oil price current USD barrel",
                    "precio petrÃ³leo actual",
                    "crude oil price today"
                ]
            
            # BÃºsqueda genÃ©rica mejorada
            return [pregunta_original, f"{pregunta_original} today", f"{pregunta_original} 2025"]
        
        try:
            consultas = mejorar_consulta_busqueda(pregunta)
            search_results = None
            consulta_exitosa = None
            
            # Intentar mÃºltiples consultas hasta obtener resultados Ãºtiles
            for consulta in consultas:
                try:
                    print(f"ğŸ” Buscando: {consulta}")
                    resultado = search_tool.invoke(consulta)
                    
                    # Verificar si el resultado tiene contenido Ãºtil
                    if resultado and len(resultado.strip()) > 50:
                        # Verificaciones adicionales para detectar resultados irrelevantes
                        resultado_lower = resultado.lower()
                        palabras_irrelevantes = [
                            "no se encontraron resultados",
                            "pÃ¡gina no encontrada", 
                            "error 404",
                            "no hay resultados",
                            "try again later"
                        ]
                        
                        # Verificar que no tenga palabras irrelevantes
                        es_irrelevante = any(palabra in resultado_lower for palabra in palabras_irrelevantes)
                        
                        if not es_irrelevante:
                            search_results = resultado
                            consulta_exitosa = consulta
                            print(f"âœ… BÃºsqueda exitosa con: {consulta}")
                            break
                        else:
                            print(f"âš ï¸ Resultado irrelevante con: {consulta}")
                
                except Exception as e:
                    print(f"âš ï¸ Error en bÃºsqueda '{consulta}': {e}")
                    continue
            
            if not search_results:
                # Fallback inteligente para cualquier consulta
                modelo = get_model(modelo_seleccionado)
                if modelo and modelo_seleccionado in simple_chains:
                    prompt_fallback = f"""
                    No pude obtener informaciÃ³n actualizada de internet sobre "{pregunta}". 
                    
                    Como experto asistente, proporciona una respuesta Ãºtil que incluya:
                    
                    1. InformaciÃ³n general relevante sobre el tema consultado
                    2. Recomendaciones especÃ­ficas para obtener informaciÃ³n actual:
                       - Sitios web especializados
                       - Aplicaciones mÃ³viles relevantes
                       - BÃºsquedas especÃ­ficas en Google
                    3. Contexto Ãºtil sobre el tema
                    4. Consejos prÃ¡cticos para encontrar la informaciÃ³n
                    
                    SÃ© especÃ­fico y Ãºtil, adaptÃ¡ndote al tipo de consulta realizada.
                    """
                    
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_fallback})
                    
                    return jsonify({
                        'respuesta': f"ğŸŒ **BÃºsqueda web limitada - Respuesta inteligente:**\n\n{respuesta_fallback}",
                        'modo': 'busqueda_inteligente_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Asistente inteligente'
                    })
                else:
                    return jsonify({
                        'respuesta': "âŒ No pude obtener informaciÃ³n actualizada desde internet en este momento. Te recomiendo:\n\n1. Consultar sitios web especializados\n2. Usar aplicaciones mÃ³viles relevantes\n3. Buscar en Google con tÃ©rminos especÃ­ficos\n4. Intentar la bÃºsqueda mÃ¡s tarde",
                        'modo': 'busqueda_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Error en bÃºsqueda'
                    })
            
            # Usar el modelo para resumir y formatear los resultados
            modelo = get_model(modelo_seleccionado)
            if modelo and modelo_seleccionado in simple_chains:
                prompt_busqueda = f"""
                INSTRUCCIONES ESPECÃFICAS: Eres un experto asistente que debe extraer y presentar informaciÃ³n Ãºtil de resultados de bÃºsqueda web.

                PREGUNTA DEL USUARIO: "{pregunta}"
                
                RESULTADOS DE BÃšSQUEDA OBTENIDOS:
                {search_results}

                TAREAS OBLIGATORIAS:
                1. ANALIZA cuidadosamente los resultados de bÃºsqueda
                2. EXTRAE cualquier informaciÃ³n relevante disponible (precios, noticias, datos especÃ­ficos)
                3. Si encuentras datos parciales o relacionados, ÃšSALOS y sÃ© transparente sobre las limitaciones
                4. COMPLEMENTA con informaciÃ³n contextual Ãºtil y recomendaciones
                5. NUNCA digas simplemente "no hay informaciÃ³n" - siempre proporciona valor

                ESTRUCTURA DE RESPUESTA:
                - InformaciÃ³n encontrada (aunque sea limitada)
                - Contexto adicional Ãºtil
                - Recomendaciones para informaciÃ³n mÃ¡s completa
                - Consejos prÃ¡cticos

                IMPORTANTE: SÃ© proactivo y Ãºtil. Si los resultados mencionan sitios web o aplicaciones, incorpÃ³ralos como recomendaciones adicionales.
                
                RESPUESTA EN ESPAÃ‘OL:
                """
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_busqueda})
                
                return jsonify({
                    'respuesta': f"ğŸ” **BÃºsqueda realizada con:** {consulta_exitosa}\n\n{respuesta}",
                    'modo': 'busqueda_rapida',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'DuckDuckGo'
                })
            else:
                return jsonify({
                    'respuesta': f"**Resultados de bÃºsqueda directa:**\n\n{search_results}",
                    'modo': 'busqueda_directa',
                    'modelo_usado': 'ninguno',
                    'fuente': 'DuckDuckGo'
                })
                
        except Exception as search_error:
            return jsonify({'error': f'Error en bÃºsqueda: {str(search_error)}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Error en bÃºsqueda rÃ¡pida: {str(e)}'}), 500

@app.route('/clima-directo', methods=['POST'])
def clima_directo() -> Union[Response, Tuple[Response, int]]:
    """Endpoint especÃ­fico para consultas de clima con respuesta directa inteligente"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcionÃ³ ninguna pregunta'}), 400
        
        # Usar el modelo para generar una respuesta Ãºtil sobre clima
        modelo = get_model(modelo_seleccionado)
        if modelo and modelo_seleccionado in simple_chains:
            prompt_clima = f"""
            El usuario pregunta sobre el clima en {ciudad}. Aunque no puedo acceder a datos meteorolÃ³gicos en tiempo real desde mi entrenamiento, puedo proporcionar informaciÃ³n Ãºtil y recomendaciones prÃ¡cticas.

            Pregunta del usuario: "{pregunta}"

            Proporciona una respuesta Ãºtil que incluya:
            1. Reconocimiento de la limitaciÃ³n para datos en tiempo real
            2. InformaciÃ³n general sobre el clima tÃ­pico de {ciudad} segÃºn la Ã©poca del aÃ±o (es julio 2025)
            3. Recomendaciones especÃ­ficas para obtener informaciÃ³n actual:
               - Sitios web especÃ­ficos (weather.com, accuweather.com, clima.com)
               - Aplicaciones mÃ³viles recomendadas
               - BÃºsquedas especÃ­ficas en Google
            4. Consejos prÃ¡cticos para el clima tÃ­pico de la regiÃ³n en esta Ã©poca

            SÃ© Ãºtil, especÃ­fico y prÃ¡ctico en tu respuesta.
            """
            
            respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            
            return jsonify({
                'respuesta': f"ğŸŒ¤ï¸ **InformaciÃ³n sobre clima en {ciudad}**\n\n{respuesta}",
                'modo': 'clima_directo',
                'modelo_usado': modelo_seleccionado,
                'fuente': 'Respuesta inteligente'
            })
        else:
            return jsonify({
                'respuesta': f"Para obtener el clima actual en {ciudad}, te recomiendo consultar:\nâ€¢ weather.com\nâ€¢ accuweather.com\nâ€¢ Google Weather\nâ€¢ Apps mÃ³viles de clima",
                'modo': 'clima_basico',
                'modelo_usado': 'ninguno',
                'fuente': 'Recomendaciones estÃ¡ticas'
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-actual', methods=['POST'])
def clima_actual() -> Union[Response, Tuple[Response, int]]:
    """Endpoint rÃ¡pido y optimizado para consultas de clima actual"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcionÃ³ ninguna pregunta'}), 400
        
        # Extraer ciudad de la pregunta
        ciudad = 'Quito'  # Default
        pregunta_lower = pregunta.lower()
        
        if 'quito' in pregunta_lower:
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta_lower:
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta_lower:
            ciudad = 'Cuenca'
        elif 'new york' in pregunta_lower or 'nueva york' in pregunta_lower:
            ciudad = 'New York'
        elif 'madrid' in pregunta_lower:
            ciudad = 'Madrid'
        elif 'london' in pregunta_lower or 'londres' in pregunta_lower:
            ciudad = 'London'
        
        print(f"ğŸŒ¤ï¸ Consulta rÃ¡pida de clima para {ciudad}")
        tiempo_inicio = time.time()
        
        # Obtener datos del clima
        clima_data = obtener_clima_api(ciudad)
        
        if clima_data['success']:
            data_clima = clima_data['data']
            
            # Si tenemos un modelo disponible, usarlo para formatear la respuesta
            if modelo_seleccionado in simple_chains:
                prompt_clima = f"""El usuario pregunta: "{pregunta}"

Datos meteorolÃ³gicos ACTUALES para {ciudad}:
ğŸŒ¡ï¸ Temperatura: {data_clima['temperatura']}Â°C
ğŸŒ¡ï¸ SensaciÃ³n tÃ©rmica: {data_clima['sensacion_termica']}Â°C
â˜ï¸ Condiciones: {data_clima['descripcion']}
ğŸ’§ Humedad: {data_clima['humedad']}%
ğŸ’¨ Viento: {data_clima['velocidad_viento']} km/h hacia {data_clima['direccion_viento']}
ğŸ• Ãšltima actualizaciÃ³n: {data_clima['hora_consulta']}

Responde de manera natural y Ãºtil con esta informaciÃ³n actualizada."""
                
                respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            else:
                # Respuesta bÃ¡sica sin modelo
                respuesta_formateada = f"""ğŸŒ¤ï¸ **Clima actual en {ciudad}**

ğŸ“Š **Condiciones actuales:**
â€¢ **Temperatura:** {data_clima['temperatura']}Â°C
â€¢ **SensaciÃ³n tÃ©rmica:** {data_clima['sensacion_termica']}Â°C
â€¢ **Condiciones:** {data_clima['descripcion']}
â€¢ **Humedad:** {data_clima['humedad']}%
â€¢ **Viento:** {data_clima['velocidad_viento']} km/h ({data_clima['direccion_viento']})
â€¢ **Ãšltima actualizaciÃ³n:** {data_clima['hora_consulta']}

*Datos obtenidos de API meteorolÃ³gica en tiempo real*"""
            
            tiempo_fin = time.time()
            duracion = round(tiempo_fin - tiempo_inicio, 2)
            duracion_formateada = formatear_duracion(duracion)
            
            return jsonify({
                'respuesta': respuesta_formateada,
                'modo': 'clima_actual',
                'modelo_usado': modelo_seleccionado,
                'pensamientos': [
                    f"ğŸŒ¤ï¸ Consultando clima actual en {ciudad}",
                    "ğŸ” Conectando con API meteorolÃ³gica wttr.in",
                    f"ğŸ“Š Datos obtenidos: {data_clima['temperatura']}Â°C, {data_clima['descripcion']}",
                    "âœ… Respuesta generada con datos en tiempo real"
                ],
                'metadata': {
                    'duracion': duracion,
                    'duracion_formateada': duracion_formateada,
                    'timestamp': time.time(),
                    'internetHabilitado': True,
                    'iteraciones': 1,
                    'busquedas': 1,
                    'ciudad': ciudad,
                    'fuente_datos': 'wttr.in',
                    'tipo_consulta': 'clima_rapido'
                }
            })
        else:
            return jsonify({
                'respuesta': f"âŒ No pude obtener el clima actual de {ciudad}.\n\nError: {clima_data['error']}\n\nğŸŒ¤ï¸ **Recomendaciones:**\nâ€¢ Consulta weather.com\nâ€¢ Usa Google Weather\nâ€¢ Apps: AccuWeather, Weather Underground",
                'modo': 'clima_error',
                'modelo_usado': modelo_seleccionado,
                'metadata': {
                    'internetHabilitado': True,
                    'error': clima_data['error'],
                    'ciudad': ciudad
                }
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-api', methods=['POST'])
def clima_api() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para obtener clima usando API externa gratuita"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        # Extraer ciudad de la pregunta si estÃ¡ presente
        if 'quito' in pregunta.lower():
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta.lower():
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta.lower():
            ciudad = 'Cuenca'
        
        print(f"ğŸŒ¤ï¸ Solicitando clima para {ciudad}...")
        
        # Obtener clima usando API
        clima_data = obtener_clima_api(ciudad)
        
        # Formatear respuesta
        respuesta_formateada = formatear_respuesta_clima(clima_data, modelo_seleccionado)
        
        return jsonify({
            'respuesta': respuesta_formateada,
            'modo': 'clima_api',
            'modelo_usado': modelo_seleccionado,
            'fuente': 'wttr.in API',
            'ciudad': ciudad,
            'success': clima_data['success']
        })
        
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima API: {str(e)}'}), 500

@app.route('/agente-general', methods=['POST'])
def agente_general() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para demostraciones del agente con bÃºsquedas generales"""
    try:
        data = request.get_json()
        tipo_demo = data.get('tipo', 'clima')  # clima, noticias, bitcoin, deportes
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        # Preguntas predefinidas para diferentes tipos de demos
        demos = {
            'noticias': "Â¿CuÃ¡les son las Ãºltimas noticias sobre inteligencia artificial?",
            'bitcoin': "Â¿CuÃ¡l es el precio actual del Bitcoin en USD?",
            'deportes': "Â¿QuiÃ©n ganÃ³ la final de la Champions League mÃ¡s reciente?",
            'tech': "Â¿CuÃ¡les son las Ãºltimas noticias sobre OpenAI?",
            'economia': "Â¿CuÃ¡l es el precio actual del petrÃ³leo?",
            'general': "Â¿CuÃ¡les son las noticias mÃ¡s importantes de hoy?"
        }
        
        pregunta = demos.get(tipo_demo, demos['noticias'])
        
        # Usar el agente mejorado
        if modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            try:
                print(f"ğŸ¤– Ejecutando demo '{tipo_demo}' con {modelo_seleccionado}")
                tiempo_inicio = time.time()
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                
                # Extraer pasos intermedios si estÃ¡n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                if 'intermediate_steps' in respuesta_completa:
                    for paso in respuesta_completa['intermediate_steps']:
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar bÃºsquedas
                            if accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                            
                            pasos_intermedios.append({
                                'action': accion.tool,
                                'action_input': accion.tool_input,
                                'observation': observacion[:300] + '...' if len(observacion) > 300 else observacion
                            })
                
                return jsonify({
                    'pregunta': pregunta,
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente_general',
                    'modelo_usado': modelo_seleccionado,
                    'tipo_demo': tipo_demo,
                    'pasos_intermedios': pasos_intermedios,
                    'metadata': {
                        'duracion': duracion,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time()
                    }
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente general {modelo_seleccionado}: {error_msg}")
                
                # Fallback inteligente
                if modelo_seleccionado in simple_chains:
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({
                        "pregunta": f"Aunque no puedo buscar en internet en este momento, responde de la mejor manera posible: {pregunta}"
                    })
                    
                    return jsonify({
                        'pregunta': pregunta,
                        'respuesta': f"âš ï¸ Agente no disponible. Respuesta bÃ¡sica:\n\n{respuesta_fallback}",
                        'modo': 'fallback_general',
                        'modelo_usado': modelo_seleccionado,
                        'tipo_demo': tipo_demo
                    })
        
        return jsonify({'error': 'Agente no disponible'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en agente general: {str(e)}'}), 500

@app.route('/api/models/status', methods=['GET'])
def get_models_status():
    """Obtener el estado actual de todos los modelos"""
    try:
        import requests
        
        model_status = {
            'ollama': {
                'available': False,
                'models': []
            },
            'lmstudio': {
                'available': False,
                'models': []
            },
            'gemini': {
                'available': bool(Config.GOOGLE_API_KEY)
            }
        }
        
        # Verificar Ollama
        try:
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                model_status['ollama']['available'] = True
                ollama_models_data = response.json().get('models', [])
                model_status['ollama']['models'] = [
                    {
                        'name': model['name'],
                        'size': model.get('size', 'Unknown'),
                        'modified': model.get('modified_at', 'Unknown')
                    }
                    for model in ollama_models_data
                ]
        except:
            pass
            
        # Verificar LM Studio
        try:
            response = requests.get(f"{Config.LMSTUDIO_BASE_URL}/v1/models", timeout=5)
            if response.status_code == 200:
                model_status['lmstudio']['available'] = True
                lm_models_data = response.json().get('data', [])
                model_status['lmstudio']['models'] = [
                    {
                        'id': model['id'],
                        'object': model.get('object', 'model')
                    }
                    for model in lm_models_data
                ]
        except:
            pass
            
        return jsonify(model_status)
        
    except Exception as e:
        return jsonify({'error': f'Error al obtener estado de modelos: {str(e)}'}), 500

@app.route('/api/models/ollama/<action>', methods=['POST'])
def control_ollama(action):
    """Controlar modelos de Ollama"""
    try:
        import subprocess
        
        if action == 'start':
            # Intentar iniciar Ollama serve en background
            try:
                subprocess.Popen(
                    ['ollama', 'serve'],
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return jsonify({'message': 'Ollama iniciado en background', 'status': 'started'})
            except Exception as e:
                return jsonify({'error': f'Error al iniciar Ollama: {str(e)}'}), 500
                
        elif action == 'stop':
            # En Windows, intentar terminar el proceso ollama
            try:
                if os.name == 'nt':
                    subprocess.run(['taskkill', '/F', '/IM', 'ollama.exe'], check=False)
                else:
                    subprocess.run(['pkill', 'ollama'], check=False)
                return jsonify({'message': 'Ollama detenido', 'status': 'stopped'})
            except Exception as e:
                return jsonify({'error': f'Error al detener Ollama: {str(e)}'}), 500
                
        else:
            return jsonify({'error': 'AcciÃ³n no vÃ¡lida. Use start o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control de Ollama: {str(e)}'}), 500

@app.route('/api/models/lmstudio/<action>', methods=['POST'])
def control_lmstudio(action):
    """Controlar LM Studio"""
    try:
        if action == 'start':
            # Mostrar mensaje de instrucciones para LM Studio
            return jsonify({
                'message': 'Para iniciar LM Studio, abra la aplicaciÃ³n manualmente desde el escritorio o menÃº inicio.',
                'instructions': [
                    '1. Abrir LM Studio desde el escritorio',
                    '2. Cargar un modelo desde la pestaÃ±a "My Models"',
                    '3. Asegurarse de que el servidor local estÃ© en puerto 1234'
                ],
                'status': 'manual_start_required'
            })
            
        elif action == 'stop':
            return jsonify({
                'message': 'Para detener LM Studio, cierre la aplicaciÃ³n manualmente.',
                'instructions': [
                    '1. Hacer clic en el botÃ³n "Stop" en LM Studio',
                    '2. Cerrar la aplicaciÃ³n LM Studio'
                ],
                'status': 'manual_stop_required'
            })
            
        else:
            return jsonify({'error': 'AcciÃ³n no vÃ¡lida. Use start o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control de LM Studio: {str(e)}'}), 500

@app.route('/api/models/ollama/individual/<model_name>/<action>', methods=['POST'])
def control_ollama_individual(model_name, action):
    """Controlar modelos individuales de Ollama"""
    try:
        import subprocess
        import requests
        
        if action == 'run':
            # Ejecutar modelo especÃ­fico
            try:
                # Primero verificar si Ollama estÃ¡ ejecutÃ¡ndose
                try:
                    requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=3)
                except:
                    # Si Ollama no estÃ¡ ejecutÃ¡ndose, iniciarlo primero
                    subprocess.Popen(
                        ['ollama', 'serve'],
                        creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                    )
                    # Esperar un momento para que se inicie
                    import time
                    time.sleep(3)
                
                # Ejecutar el modelo en background
                subprocess.Popen(
                    ['ollama', 'run', model_name],
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                
                return jsonify({
                    'message': f'Modelo {model_name} cargado exitosamente',
                    'model': model_name,
                    'status': 'running'
                })
            except Exception as e:
                return jsonify({'error': f'Error al ejecutar modelo {model_name}: {str(e)}'}), 500
                
        elif action == 'stop':
            # Detener modelo especÃ­fico
            try:
                subprocess.run(['ollama', 'stop', model_name], check=True, timeout=10)
                return jsonify({
                    'message': f'Modelo {model_name} detenido exitosamente',
                    'model': model_name,
                    'status': 'stopped'
                })
            except subprocess.CalledProcessError as e:
                return jsonify({'error': f'Error al detener modelo {model_name}: {str(e)}'}), 500
            except subprocess.TimeoutExpired:
                return jsonify({'error': f'Timeout al detener modelo {model_name}'}), 500
                
        else:
            return jsonify({'error': 'AcciÃ³n no vÃ¡lida. Use run o stop'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Error en control individual de Ollama: {str(e)}'}), 500

@app.route('/api/models/ollama/running', methods=['GET'])
def get_running_ollama_models():
    """Obtener modelos de Ollama que estÃ¡n ejecutÃ¡ndose actualmente"""
    try:
        import requests
        
        # Verificar si Ollama estÃ¡ ejecutÃ¡ndose
        try:
            # Intentar obtener informaciÃ³n de modelos en ejecuciÃ³n
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/ps", timeout=5)
            if response.status_code == 200:
                running_models = response.json().get('models', [])
                
                # TambiÃ©n obtener todos los modelos disponibles
                tags_response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
                all_models = []
                if tags_response.status_code == 200:
                    all_models = tags_response.json().get('models', [])
                
                # Crear estructura de respuesta con estado de cada modelo
                models_status = []
                for model in all_models:
                    model_name = model['name']
                    is_running = any(rm['name'] == model_name for rm in running_models)
                    
                    model_info = {
                        'name': model_name,
                        'is_running': is_running,
                        'size': model.get('size', 'Unknown'),
                        'modified': model.get('modified_at', 'Unknown'),
                        'digest': model.get('digest', '')[:12] if model.get('digest') else ''
                    }
                    
                    # Si estÃ¡ ejecutÃ¡ndose, aÃ±adir informaciÃ³n adicional
                    if is_running:
                        running_info = next((rm for rm in running_models if rm['name'] == model_name), None)
                        if running_info:
                            model_info.update({
                                'vram_usage': running_info.get('size_vram', 0),
                                'expires_at': running_info.get('expires_at', ''),
                            })
                    
                    models_status.append(model_info)
                
                return jsonify({
                    'ollama_available': True,
                    'models': models_status,
                    'running_count': len(running_models)
                })
            else:
                return jsonify({
                    'ollama_available': False,
                    'models': [],
                    'running_count': 0
                })
        except:
            return jsonify({
                'ollama_available': False,
                'models': [],
                'running_count': 0
            })
            
    except Exception as e:
        return jsonify({'error': f'Error al obtener modelos en ejecuciÃ³n: {str(e)}'}), 500

@app.route('/api/ollama/console', methods=['POST'])
def ollama_console_command():
    """Ejecutar comandos de consola Ollama"""
    try:
        import subprocess
        import json
        
        data = request.get_json()
        command = data.get('command', '').strip()
        
        if not command:
            return jsonify({'error': 'Comando vacÃ­o'}), 400
        
        # Lista de comandos permitidos por seguridad
        allowed_commands = ['ps', 'list', 'serve', 'run', 'stop', 'show', 'create', 'rm', 'cp', 'push', 'pull']
        
        # Parsear el comando
        cmd_parts = command.split()
        if not cmd_parts or cmd_parts[0] not in allowed_commands:
            return jsonify({
                'error': f'Comando no permitido. Comandos disponibles: {", ".join(allowed_commands)}',
                'output': '',
                'success': False
            }), 400
        
        # Construir comando completo
        full_command = ['ollama'] + cmd_parts
        
        try:
            # Ejecutar comando con timeout
            if cmd_parts[0] == 'serve':
                # Para serve, iniciar en background
                process = subprocess.Popen(
                    full_command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
                )
                return jsonify({
                    'command': command,
                    'output': 'Ollama serve iniciado en background. Verifica el estado con "ollama ps".',
                    'success': True,
                    'background': True
                })
            else:
                # Para otros comandos, ejecutar y obtener output
                result = subprocess.run(
                    full_command,
                    capture_output=True,
                    text=True,
                    timeout=30,
                    check=False
                )
                
                # Combinar stdout y stderr
                output = ''
                if result.stdout:
                    output += result.stdout
                if result.stderr:
                    output += '\n' + result.stderr if output else result.stderr
                
                if not output:
                    output = f'Comando "{command}" ejecutado exitosamente (sin output)'
                
                return jsonify({
                    'command': command,
                    'output': output.strip(),
                    'success': result.returncode == 0,
                    'return_code': result.returncode
                })
                
        except subprocess.TimeoutExpired:
            return jsonify({
                'command': command,
                'output': 'Error: Comando excediÃ³ el tiempo lÃ­mite (30s)',
                'success': False,
                'timeout': True
            }), 408
            
        except subprocess.CalledProcessError as e:
            return jsonify({
                'command': command,
                'output': f'Error ejecutando comando: {e.stderr if e.stderr else str(e)}',
                'success': False,
                'error': str(e)
            }), 500
            
    except Exception as e:
        return jsonify({
            'error': f'Error interno del servidor: {str(e)}',
            'output': '',
            'success': False
        }), 500

@app.route('/api/ollama/models-for-actions', methods=['GET'])
def get_ollama_models_for_actions():
    """Obtener lista de modelos para acciones (run/stop)"""
    try:
        import requests
        import subprocess
        
        # Intentar obtener modelos via API REST
        try:
            response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return jsonify({
                    'models': models,
                    'source': 'api',
                    'available': True
                })
        except:
            pass
        
        # Si falla la API, intentar con comando
        try:
            result = subprocess.run(
                ['ollama', 'list'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                models = []
                for line in lines:
                    if line.strip():
                        # Extraer nombre del modelo (primera columna)
                        parts = line.split()
                        if parts:
                            models.append(parts[0])
                
                return jsonify({
                    'models': models,
                    'source': 'command',
                    'available': True
                })
        except:
            pass
        
        return jsonify({
            'models': [],
            'source': 'none',
            'available': False,
            'error': 'No se pudo obtener lista de modelos'
        })
        
    except Exception as e:
        return jsonify({
            'models': [],
            'available': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print("ğŸ¤– Iniciando aplicaciÃ³n de IA con Agentes...")
    print(f"ğŸ§  Modelos disponibles: {', '.join(available_models)}")
    print("ğŸ” Herramientas: BÃºsqueda web avanzada")
    print("ğŸŒ Servidor: http://127.0.0.1:5000")
    print("ğŸš€ AplicaciÃ³n de consultas generales lista!")
    app.run(debug=True, host='127.0.0.1', port=5000)
