import os
import json
import requests
import time
from typing import Optional, Union, Tuple
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv

# Importar ChatOllama de la nueva ubicaci√≥n (si est√° disponible), sino usar la anterior
try:
    from langchain_ollama import ChatOllama
    print("‚úÖ Usando langchain_ollama (versi√≥n actualizada)")
except ImportError:
    from langchain_community.chat_models import ChatOllama
    print("‚ö†Ô∏è Usando langchain_community.chat_models (versi√≥n deprecada)")

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain.tools import BaseTool
from langchain_core.tools import Tool
from config import Config

# Cargar variables de entorno
load_dotenv()

app = Flask(__name__)
app.config.from_object(Config)

def formatear_duracion(segundos):
    """Convierte segundos a formato legible (minutos y segundos)"""
    if segundos < 1:
        return f"{round(segundos * 1000)}ms"
    elif segundos < 60:
        return f"{round(segundos, 1)}s"
    else:
        minutos = int(segundos // 60)
        segundos_restantes = round(segundos % 60, 1)
        if segundos_restantes == 0:
            return f"{minutos}m"
        else:
            return f"{minutos}m {segundos_restantes}s"

print("ü§ñ Configurando modelos de IA...")

# Configurar modelos de IA
models = {}

# Configurar modelos de Ollama (locales)
ollama_models = [
    ('llama3', 'Llama3 8B'),
    ('deepseek-coder', 'DeepSeek Coder'),
    ('deepseek-r1:8b', 'DeepSeek R1 8B'),
    ('phi3', 'Microsoft Phi-3'),
    ('gemma:2b', 'Google Gemma 2B')
]

for model_key, model_name in ollama_models:
    try:
        models[model_key] = ChatOllama(
            model=model_key
        )
        print(f"‚úÖ {model_name} configurado correctamente")
    except Exception as e:
        print(f"‚ö†Ô∏è {model_name} no disponible: {e}")
        models[model_key] = None

# Configurar Google Gemini (API)
try:
    if Config.GOOGLE_API_KEY:
        os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
        models['gemini-1.5-flash'] = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash"
        )
        print("‚úÖ Google Gemini 1.5 Flash configurado correctamente")
    else:
        print("‚ö†Ô∏è Google Gemini no disponible: No se encontr√≥ GOOGLE_API_KEY")
        models['gemini-1.5-flash'] = None
except Exception as e:
    print(f"‚ö†Ô∏è Google Gemini no disponible: {e}")
    models['gemini-1.5-flash'] = None

# Verificar que al menos un modelo est√© disponible
available_models = [k for k, v in models.items() if v is not None]
if not available_models:
    print("‚ùå Error: No hay modelos disponibles")
    print("üí° Aseg√∫rate de que:")
    print("   - Ollama est√© ejecut√°ndose con llama3 instalado, O")
    print("   - Tengas GOOGLE_API_KEY configurada en .env")
    exit(1)

print(f"üéØ Modelos disponibles: {', '.join(available_models)}")

# Funci√≥n para obtener el modelo seg√∫n la selecci√≥n
def get_model(model_name: str) -> Optional[Union[ChatOllama, ChatGoogleGenerativeAI]]:
    """Obtiene el modelo solicitado o el primero disponible como fallback"""
    if model_name in models and models[model_name] is not None:
        return models[model_name]
    
    # Fallback al primer modelo disponible
    for model_key, model_instance in models.items():
        if model_instance is not None:
            print(f"‚ö†Ô∏è Usando {model_key} como fallback")
            return model_instance
    
    return None

# Funci√≥n para obtener clima usando API gratuita
def obtener_clima_api(ciudad: str = "Quito") -> dict:
    """Obtiene informaci√≥n del clima usando API gratuita de wttr.in"""
    try:
        # API gratuita que no requiere clave
        url = f"https://wttr.in/{ciudad}?format=j1"
        
        print(f"üåê Consultando API de clima para {ciudad}...")
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer informaci√≥n relevante
            current = data.get('current_condition', [{}])[0]
            weather_info = {
                'temperatura': current.get('temp_C', 'N/A'),
                'descripcion': current.get('weatherDesc', [{}])[0].get('value', 'N/A'),
                'humedad': current.get('humidity', 'N/A'),
                'sensacion_termica': current.get('FeelsLikeC', 'N/A'),
                'velocidad_viento': current.get('windspeedKmph', 'N/A'),
                'direccion_viento': current.get('winddir16Point', 'N/A'),
                'hora_consulta': current.get('observation_time', 'N/A'),
                'ciudad': ciudad
            }
            
            print(f"‚úÖ Clima obtenido: {weather_info['temperatura']}¬∞C en {ciudad}")
            return {'success': True, 'data': weather_info}
            
        else:
            print(f"‚ö†Ô∏è Error API clima: {response.status_code}")
            return {'success': False, 'error': f'Error de API: {response.status_code}'}
            
    except requests.exceptions.Timeout:
        print("‚ö†Ô∏è Timeout al consultar API de clima")
        return {'success': False, 'error': 'Timeout en consulta'}
    except Exception as e:
        print(f"‚ö†Ô∏è Error consultando API de clima: {e}")
        return {'success': False, 'error': str(e)}

# Funci√≥n para b√∫squeda web avanzada usando m√∫ltiples APIs
def busqueda_web_avanzada(query: str) -> str:
    """B√∫squeda web usando m√∫ltiples m√©todos para mayor confiabilidad"""
    print(f"üîç B√∫squeda web avanzada: {query}")
    
    resultados = []
    
    # M√©todo 1: DuckDuckGo (original)
    try:
        ddg_search = DuckDuckGoSearchRun()
        resultado_ddg = ddg_search.invoke(query)
        if resultado_ddg and len(resultado_ddg.strip()) > 30:
            resultados.append(f"[DuckDuckGo] {resultado_ddg}")
            print("‚úÖ DuckDuckGo: Resultados obtenidos")
        else:
            print("‚ö†Ô∏è DuckDuckGo: Sin resultados √∫tiles")
    except Exception as e:
        print(f"‚ö†Ô∏è DuckDuckGo fall√≥: {e}")
    
    # M√©todo 2: Para noticias espec√≠ficas, usar t√©rminos m√°s espec√≠ficos
    if any(palabra in query.lower() for palabra in ['noticias', 'news', 'hoy', 'today', 'actualidad']):
        try:
            # Buscar noticias m√°s espec√≠ficas
            queries_noticias = [
                "noticias tecnolog√≠a inteligencia artificial hoy",
                "noticias Ecuador √∫ltimas",
                "breaking news today",
                "noticias mundo actualidad"
            ]
            
            for query_especifica in queries_noticias:
                try:
                    ddg_search = DuckDuckGoSearchRun()
                    resultado = ddg_search.invoke(query_especifica)
                    if resultado and len(resultado.strip()) > 30:
                        resultados.append(f"[Noticias {query_especifica}] {resultado[:500]}...")
                        print(f"‚úÖ Noticias encontradas para: {query_especifica}")
                        break  # Si encontramos algo, salir del loop
                except:
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è B√∫squeda de noticias espec√≠ficas fall√≥: {e}")
    
    # M√©todo 3: API de b√∫squeda alternativa (usando scraping b√°sico)
    try:
        # Usar una API p√∫blica de b√∫squeda o scraping b√°sico
        url = f"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extraer resultados de la API
            abstract = data.get('Abstract', '')
            answer = data.get('Answer', '')
            
            if abstract:
                resultados.append(f"[API Abstract] {abstract}")
                print("‚úÖ API DuckDuckGo: Abstract obtenido")
            
            if answer:
                resultados.append(f"[API Answer] {answer}")
                print("‚úÖ API DuckDuckGo: Answer obtenido")
                
    except Exception as e:
        print(f"‚ö†Ô∏è API DuckDuckGo fall√≥: {e}")
    
    # M√©todo 3: Para clima espec√≠fico, usar nuestra API
    if any(palabra in query.lower() for palabra in ['clima', 'weather', 'temperatura', 'temperature']):
        try:
            # Extraer ciudad de la consulta
            ciudad = 'Quito'  # Default
            if 'quito' in query.lower():
                ciudad = 'Quito'
            elif 'guayaquil' in query.lower():
                ciudad = 'Guayaquil'
            elif 'cuenca' in query.lower():
                ciudad = 'Cuenca'
            elif 'new york' in query.lower() or 'nueva york' in query.lower():
                ciudad = 'New York'
            elif 'london' in query.lower() or 'londres' in query.lower():
                ciudad = 'London'
            elif 'madrid' in query.lower():
                ciudad = 'Madrid'
            
            clima_data = obtener_clima_api(ciudad)
            if clima_data['success']:
                data = clima_data['data']
                clima_info = f"Clima actual en {ciudad}: {data['temperatura']}¬∞C, {data['descripcion']}, Humedad: {data['humedad']}%, Viento: {data['velocidad_viento']} km/h"
                resultados.append(f"[Clima API] {clima_info}")
                print(f"‚úÖ Clima API: Datos obtenidos para {ciudad}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è API Clima fall√≥: {e}")
    
    # Compilar resultados
    if resultados:
        resultado_final = "\n\n".join(resultados)
        print(f"‚úÖ B√∫squeda completada con {len(resultados)} fuentes")
        return resultado_final
    else:
        print("‚ùå No se obtuvieron resultados de ninguna fuente")
        return f"No se pudo obtener informaci√≥n actualizada sobre '{query}'. Se recomienda consultar fuentes directas como Google, sitios web oficiales o aplicaciones especializadas."

# Crear herramienta personalizada para b√∫squeda web
def crear_herramienta_busqueda():
    """Crea una herramienta de b√∫squeda web personalizada"""
    return Tool(
        name="web_search",
        description="Busca informaci√≥n actual en internet sobre cualquier tema. √ötil para noticias, precios, eventos actuales, etc. Input debe ser una consulta de b√∫squeda espec√≠fica.",
        func=busqueda_web_avanzada
    )

def formatear_respuesta_clima(clima_data: dict, modelo_seleccionado: str) -> str:
    """Formatea la respuesta del clima de manera atractiva"""
    if clima_data['success']:
        data = clima_data['data']
        return f"""üå§Ô∏è **Clima actual en {data['ciudad']}**

üìä **Condiciones actuales:**
‚Ä¢ **Temperatura:** {data['temperatura']}¬∞C
‚Ä¢ **Sensaci√≥n t√©rmica:** {data['sensacion_termica']}¬∞C
‚Ä¢ **Condiciones:** {data['descripcion']}
‚Ä¢ **Humedad:** {data['humedad']}%
‚Ä¢ **Viento:** {data['velocidad_viento']} km/h ({data['direccion_viento']})
‚Ä¢ **√öltima actualizaci√≥n:** {data['hora_consulta']}

üèîÔ∏è **Informaci√≥n contextual:**
Quito se encuentra a 2,850 metros sobre el nivel del mar, lo que influye en su clima templado durante todo el a√±o. Las temperaturas suelen oscilar entre 10¬∞C y 25¬∞C.

üì± **Para m√°s detalles:**
‚Ä¢ Pron√≥stico extendido: [wttr.in/Quito](https://wttr.in/Quito)
‚Ä¢ Apps recomendadas: AccuWeather, Weather Underground
‚Ä¢ Sitios web: weather.com, tiempo.com

*Datos obtenidos de API meteorol√≥gica en tiempo real*"""
    else:
        return f"""‚ùå **No se pudo obtener el clima actual**

Error: {clima_data['error']}

üå§Ô∏è **Informaci√≥n general sobre Quito:**
Quito tiene un clima subtropical de monta√±a con temperaturas relativamente estables durante todo el a√±o:
‚Ä¢ **D√≠a:** 18-24¬∞C
‚Ä¢ **Noche:** 8-15¬∞C
‚Ä¢ **Julio:** Estaci√≥n seca, d√≠as soleados y noches frescas

üì± **Consulta informaci√≥n actualizada en:**
‚Ä¢ Google: "clima Quito Ecuador"
‚Ä¢ AccuWeather.com
‚Ä¢ Weather.com
‚Ä¢ Apps m√≥viles de clima"""

# Configurar herramientas para el agente
herramienta_busqueda = crear_herramienta_busqueda()
tools = [herramienta_busqueda, DuckDuckGoSearchRun()]

# Prompt optimizado para b√∫squedas generales
agent_prompt = PromptTemplate.from_template("""
Eres un asistente de IA especializado en proporcionar respuestas precisas y actuales. Tienes acceso a herramientas de b√∫squeda web.

Herramientas disponibles:
{tools}

REGLAS IMPORTANTES: 
1. SIEMPRE usa b√∫squeda web para: noticias recientes, precios actuales, eventos deportivos, informaci√≥n actualizada
2. NUNCA digas que no tienes acceso a informaci√≥n en tiempo real - ¬°S√ç LO TIENES!
3. Si una b√∫squeda falla, intenta con t√©rminos diferentes
4. Proporciona respuestas √∫tiles basadas en los resultados obtenidos
5. Adapta tu b√∫squeda seg√∫n el tipo de informaci√≥n solicitada

Formato OBLIGATORIO:
Question: la pregunta que debes responder
Thought: necesito buscar informaci√≥n actual sobre [tema espec√≠fico]
Action: duckduckgo_search
Action Input: [t√©rminos de b√∫squeda espec√≠ficos y efectivos]
Observation: [resultados de la b√∫squeda]
Thought: ahora tengo informaci√≥n actual y puedo responder
Final Answer: [respuesta completa en espa√±ol basada en los resultados]

Pregunta: {input}
Thought:{agent_scratchpad}
""")

# Crear el agente con manejo de errores y herramientas mejoradas
agents = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        try:
            # Usar el prompt est√°ndar de React que funciona
            react_prompt = hub.pull("hwchase17/react")
            
            agent = create_react_agent(model_instance, tools, react_prompt)
            agents[model_name] = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                max_iterations=6,  # Incrementado para m√°s an√°lisis
                max_execution_time=30,  # Incrementado para an√°lisis detallado
                handle_parsing_errors=True,
                return_intermediate_steps=True,
                early_stopping_method="generate"  # Permite generar respuesta parcial si llega al l√≠mite
            )
            print(f"‚úÖ Agente {model_name} creado con herramientas avanzadas")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creando agente {model_name}: {e}")
            agents[model_name] = None

# Tambi√©n configurar un chat simple sin agente para preguntas b√°sicas
simple_chat_prompt = ChatPromptTemplate.from_messages([
    ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y √∫tiles en espa√±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaci√≥n espec√≠fica y detallada
3. Si te preguntan "¬øqu√© es X?", explica qu√© es X de manera clara y completa
4. Si te preguntan sobre conceptos t√©cnicos, da definiciones precisas
5. Mant√©n un tono profesional pero accesible
6. No digas solo "estoy aqu√≠ para ayudar" - da la respuesta espec√≠fica

EJEMPLOS:
- Si preguntan "¬øqu√© es Java?" ‚Üí Explica que Java es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es Python?" ‚Üí Explica que Python es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es HTML?" ‚Üí Explica que HTML es un lenguaje de marcado

Siempre proporciona informaci√≥n √∫til y espec√≠fica."""),
    ("user", "{pregunta}")
])

# Crear prompts espec√≠ficos para diferentes modelos
def crear_prompt_para_modelo(model_name: str) -> ChatPromptTemplate:
    """Crea un prompt optimizado seg√∫n el modelo espec√≠fico"""
    
    if model_name == 'deepseek-coder':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek Coder, un asistente especializado en programaci√≥n y desarrollo. Proporciona respuestas t√©cnicas precisas y c√≥digo cuando sea necesario.

ESPECIALIDADES:
- Explicar conceptos de programaci√≥n
- Proporcionar ejemplos de c√≥digo
- Debugging y resoluci√≥n de problemas
- Mejores pr√°cticas de desarrollo

FORMATO DE RESPUESTA:
- Respuestas directas y t√©cnicas
- Incluye ejemplos de c√≥digo cuando sea relevante
- Explica conceptos paso a paso
- Menciona ventajas y desventajas cuando sea apropiado"""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'deepseek-r1:8b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres DeepSeek R1, un modelo de razonamiento avanzado dise√±ado para an√°lisis profundo y respuestas reflexivas.

CARACTER√çSTICAS ESPECIALES:
- Razonamiento paso a paso antes de responder
- An√°lisis cr√≠tico y evaluaci√≥n de m√∫ltiples perspectivas
- Explicaciones detalladas y fundamentadas
- Capacidad de autorreflexi√≥n y correcci√≥n

FORMATO DE RESPUESTA OBLIGATORIO:
üîç **AN√ÅLISIS INICIAL:** [Comprensi√≥n del problema/pregunta]

üß† **RAZONAMIENTO:**
‚Ä¢ **Paso 1:** [Primera consideraci√≥n o enfoque]
‚Ä¢ **Paso 2:** [Segunda consideraci√≥n o an√°lisis]
‚Ä¢ **Paso 3:** [Tercera consideraci√≥n o s√≠ntesis]

üìã **RESPUESTA:** [Conclusi√≥n fundamentada y detallada]

üîÑ **REFLEXI√ìN:** [Validaci√≥n de la respuesta y posibles alternativas]

IMPORTANTE: Siempre usa este formato estructurado para mostrar tu proceso de pensamiento completo."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'gemma:2b':
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente √∫til. Responde de forma clara y completa.

Si preguntan "¬øqu√© es X?", explica qu√© es X con:
1. Definici√≥n principal
2. Caracter√≠sticas importantes
3. Usos principales

EJEMPLO para "¬øqu√© es Java?":
Java es un lenguaje de programaci√≥n orientado a objetos desarrollado por Sun Microsystems (ahora Oracle). Sus caracter√≠sticas principales son:

‚Ä¢ **Multiplataforma**: "Write once, run anywhere" - el c√≥digo Java se ejecuta en cualquier sistema con JVM
‚Ä¢ **Orientado a objetos**: Organiza el c√≥digo en clases y objetos
‚Ä¢ **Robusto y seguro**: Manejo autom√°tico de memoria y verificaci√≥n de c√≥digo
‚Ä¢ **Usos principales**: Aplicaciones empresariales, aplicaciones m√≥viles (Android), desarrollo web

Es uno de los lenguajes m√°s populares para desarrollo de software empresarial y aplicaciones Android.

Responde siempre de manera √∫til y completa."""),
            ("user", "{pregunta}")
        ])
    
    elif model_name == 'phi3':
        return ChatPromptTemplate.from_messages([
            ("system", f"""Eres Microsoft Phi-3, un modelo compacto pero potente dise√±ado para respuestas r√°pidas y precisas.

FECHA ACTUAL: {time.strftime('%d de %B de %Y')} 

IMPORTANTE: Para preguntas sobre noticias, eventos actuales, precios, clima o informaci√≥n reciente, debes indicar claramente que necesitas b√∫squeda web en tiempo real, ya que tu conocimiento tiene una fecha de corte y puede estar desactualizado.

CARACTER√çSTICAS:
- Respuestas concisas pero completas
- Enfoque en eficiencia y claridad
- Proporciona informaci√≥n pr√°ctica y √∫til
- Evita redundancias y texto innecesario
- SIEMPRE reconoce limitaciones temporales para informaci√≥n actual

FORMATO:
1. Respuesta directa a la pregunta
2. Informaci√≥n clave en 2-3 puntos
3. Ejemplo o aplicaci√≥n pr√°ctica si es relevante
4. Para noticias/eventos actuales: Recomendar b√∫squeda web"""),
            ("user", "{pregunta}")
        ])
    
    else:
        # Prompt por defecto para Llama3 y Gemini
        return ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente de IA especializado y conocedor. Tu trabajo es proporcionar respuestas directas, precisas y √∫tiles en espa√±ol.

REGLAS IMPORTANTES:
1. Responde DIRECTAMENTE a la pregunta formulada
2. Proporciona informaci√≥n espec√≠fica y detallada
3. Si te preguntan "¬øqu√© es X?", explica qu√© es X de manera clara y completa
4. Si te preguntan sobre conceptos t√©cnicos, da definiciones precisas
5. Mant√©n un tono profesional pero accesible
6. No digas solo "estoy aqu√≠ para ayudar" - da la respuesta espec√≠fica

EJEMPLOS:
- Si preguntan "¬øqu√© es Java?" ‚Üí Explica que Java es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es Python?" ‚Üí Explica que Python es un lenguaje de programaci√≥n
- Si preguntan "¬øqu√© es HTML?" ‚Üí Explica que HTML es un lenguaje de marcado

Siempre proporciona informaci√≥n √∫til y espec√≠fica."""),
            ("user", "{pregunta}")
        ])

simple_chains = {}
for model_name, model_instance in models.items():
    if model_instance is not None:
        # Obtener prompt personalizado para cada modelo
        model_prompt = crear_prompt_para_modelo(model_name)
        
        # Configurar chain con temperatura si es posible
        ollama_models_list = ['llama3', 'deepseek-coder', 'deepseek-r1:8b', 'phi3', 'gemma:2b']
        
        if model_name in ollama_models_list:
            # Para modelos de Ollama, configurar directamente sin bind (temperatura no es compatible)
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        elif model_name == 'gemini-1.5-flash':
            # Para Gemini, configurar temperatura usando bind
            configured_model = model_instance.bind(temperature=Config.DEFAULT_TEMPERATURE)
            simple_chains[model_name] = model_prompt | configured_model | StrOutputParser()
        else:
            # Fallback sin temperatura espec√≠fica
            simple_chains[model_name] = model_prompt | model_instance | StrOutputParser()
        
        print(f"‚úÖ Chat simple {model_name} configurado con prompt personalizado")

@app.route('/')
def index() -> str:
    return render_template('index.html', modelos_disponibles=available_models)

@app.route('/chat', methods=['POST'])
def chat() -> Union[Response, Tuple[Response, int]]:
    try:
        print("üîç DEBUG: Iniciando funci√≥n chat()")
        data = request.get_json()
        print(f"üîç DEBUG: Datos recibidos: {data}")
        
        pregunta = data.get('pregunta', '')
        modo = data.get('modo', 'simple')  # 'simple' o 'agente'
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        permitir_internet = data.get('permitir_internet', True)  # Por defecto permitir internet
        
        print(f"üîç DEBUG: pregunta='{pregunta}', modo='{modo}', modelo='{modelo_seleccionado}', internet={permitir_internet}")
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Obtener el modelo seleccionado
        modelo = get_model(modelo_seleccionado)
        print(f"üîç DEBUG: Modelo obtenido: {modelo}")
        
        if modelo is None:
            return jsonify({'error': 'No hay modelos disponibles'}), 500

        # Detecci√≥n inteligente para forzar modo agente cuando se necesite informaci√≥n actual
        palabras_actualidad = [
            'noticias', 'news', 'actualidad', 'hoy', 'today', 'actual', 'reciente', 
            'precio', 'price', 'cotizaci√≥n', '√∫ltimo', 'latest', 'breaking',
            'eventos', 'acontecimiento', 'qu√© pas√≥', 'qu√© est√° pasando'
        ]
        
        necesita_busqueda_web = any(palabra in pregunta.lower() for palabra in palabras_actualidad)
        
        if necesita_busqueda_web and permitir_internet and modo == 'simple':
            print(f"üîÑ Detectada consulta que requiere informaci√≥n actual, cambiando a modo agente")
            modo = 'agente'

        # Forzar modo simple si internet est√° deshabilitado y se intenta usar modos que requieren web
        if not permitir_internet and modo in ['agente', 'busqueda_rapida']:
            print(f"üîç DEBUG: Forzando modo simple porque internet est√° deshabilitado")
            modo = 'simple'

        if modo == 'agente' and permitir_internet and modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            # Verificar si es una consulta de clima para usar endpoint especializado
            if any(palabra in pregunta.lower() for palabra in ['clima', 'weather', 'temperatura', 'temp']):
                print(f"üå§Ô∏è Detectada consulta de clima, usando endpoint especializado")
                try:
                    tiempo_inicio = time.time()
                    
                    # Extraer ciudad de la pregunta
                    ciudad = 'Quito'  # Default
                    if 'quito' in pregunta.lower():
                        ciudad = 'Quito'
                    elif 'guayaquil' in pregunta.lower():
                        ciudad = 'Guayaquil'
                    elif 'cuenca' in pregunta.lower():
                        ciudad = 'Cuenca'
                    
                    # Obtener datos del clima directamente
                    clima_data = obtener_clima_api(ciudad)
                    tiempo_fin = time.time()
                    duracion = round(tiempo_fin - tiempo_inicio, 2)
                    duracion_formateada = formatear_duracion(duracion)
                    
                    if clima_data['success']:
                        # Usar el modelo para generar una respuesta formateada
                        if modelo_seleccionado in simple_chains:
                            prompt_clima = f"""La consulta del usuario es: "{pregunta}"

Los datos actuales del clima en {ciudad} son:
- Temperatura: {clima_data['data']['temperatura']}¬∞C
- Condiciones: {clima_data['data']['descripcion']}
- Humedad: {clima_data['data']['humedad']}%
- Sensaci√≥n t√©rmica: {clima_data['data']['sensacion_termica']}¬∞C
- Viento: {clima_data['data']['velocidad_viento']} km/h ({clima_data['data']['direccion_viento']})
- Hora de consulta: {clima_data['data']['hora_consulta']}

Responde de manera clara y √∫til con estos datos actuales."""
                            
                            respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
                            
                            return jsonify({
                                'respuesta': respuesta_formateada,
                                'modo': 'clima_directo_agente',
                                'modelo_usado': modelo_seleccionado,
                                'pensamientos': [
                                    f"üå§Ô∏è Detectada consulta sobre clima en {ciudad}",
                                    "üîç Consultando API meteorol√≥gica en tiempo real",
                                    f"üìä Datos obtenidos: {clima_data['data']['temperatura']}¬∞C, {clima_data['data']['descripcion']}",
                                    "üß† Procesando respuesta personalizada"
                                ],
                                'metadata': {
                                    'duracion': duracion,
                                    'duracion_formateada': duracion_formateada,
                                    'iteraciones': 1,
                                    'busquedas': 1,
                                    'timestamp': time.time(),
                                    'timestamp_inicio': tiempo_inicio,
                                    'timestamp_fin': tiempo_fin,
                                    'internetHabilitado': permitir_internet,
                                    'tipo_consulta': 'clima_optimizada'
                                }
                            })
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en consulta de clima optimizada: {e}")
                    # Fallback al agente normal
                    pass
            
            # Usar el agente para preguntas que puedan requerir b√∫squeda web
            try:
                tiempo_inicio = time.time()
                print(f"ü§ñ Iniciando agente {modelo_seleccionado} para: {pregunta[:50]}...")
                
                # Ejecutar el agente
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                duracion_formateada = formatear_duracion(duracion)
                
                print(f"‚úÖ Agente completado en {duracion_formateada}")
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                pensamientos = []
                
                if 'intermediate_steps' in respuesta_completa:
                    print(f"üìä Procesando {len(respuesta_completa['intermediate_steps'])} pasos intermedios...")
                    
                    for i, paso in enumerate(respuesta_completa['intermediate_steps']):
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar b√∫squedas
                            if hasattr(accion, 'tool') and accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                                pensamientos.append(f"üîç Realizando b√∫squeda: '{accion.tool_input}'")
                            
                            # Agregar informaci√≥n del paso
                            paso_info = {
                                'step': i + 1,
                                'action': accion.tool if hasattr(accion, 'tool') else str(accion),
                                'action_input': accion.tool_input if hasattr(accion, 'tool_input') else str(accion),
                                'observation': observacion[:500] + '...' if len(str(observacion)) > 500 else str(observacion),
                                'thought': f"Ejecutando {accion.tool}" if hasattr(accion, 'tool') else "Procesando informaci√≥n"
                            }
                            pasos_intermedios.append(paso_info)
                            
                            # Agregar pensamiento formateado
                            if hasattr(accion, 'tool'):
                                pensamientos.append(f"üí≠ Paso {i+1}: Usando herramienta '{accion.tool}' con entrada: '{accion.tool_input}'")
                                pensamientos.append(f"üìã Resultado: {observacion[:200]}..." if len(str(observacion)) > 200 else f"üìã Resultado: {observacion}")
                else:
                    print("‚ÑπÔ∏è No se encontraron pasos intermedios")
                    # Para consultas simples, agregar un pensamiento b√°sico
                    pensamientos.append("üí≠ Procesando consulta directamente sin necesidad de b√∫squedas web")
                
                # Verificar si las b√∫squedas fallaron y generar respuesta de fallback inteligente
                respuesta_output = respuesta_completa.get('output', '')
                if busquedas_count > 0 and respuesta_output and "No good DuckDuckGo Search Result was found" in respuesta_output:
                    print("‚ö†Ô∏è B√∫squedas web fallaron, generando respuesta de fallback inteligente...")
                    
                    # Generar respuesta de fallback espec√≠fica para noticias
                    if any(palabra in pregunta.lower() for palabra in ['noticias', 'news', 'hoy', 'actualidad']):
                        respuesta_completa['output'] = f"""üì∞ **Informaci√≥n sobre noticias del d√≠a**

Lo siento, actualmente estoy experimentando dificultades para acceder a fuentes de noticias en tiempo real. Sin embargo, te puedo sugerir las mejores fuentes para mantenerte informado sobre las noticias de hoy:

üåê **Fuentes recomendadas de noticias:**
‚Ä¢ **Internacionales:** BBC News, CNN, Reuters, Associated Press
‚Ä¢ **Ecuador:** El Universo, El Comercio, Primicias, GK
‚Ä¢ **Tecnolog√≠a:** TechCrunch, Wired, The Verge
‚Ä¢ **Deportes:** ESPN, Marca, Fox Sports

üîç **Para noticias espec√≠ficas, te recomiendo:**
1. Visitar directamente Google News
2. Usar aplicaciones de noticias como Apple News o Google News
3. Seguir cuentas verificadas en redes sociales
4. Consultar sitios web oficiales de medios

üì± **Tip:** Configura alertas de Google para temas espec√≠ficos que te interesen.

¬øHay alg√∫n tema espec√≠fico de noticias sobre el que te gustar√≠a que te ayude a encontrar informaci√≥n?"""

                        pensamientos.append("üîÑ B√∫squeda web no disponible, proporcionando fuentes alternativas para noticias")
                        pensamientos.append("üí° Sugiriendo medios confiables y m√©todos alternativos de b√∫squeda")
                    else:
                        # Para otras consultas que requieren informaci√≥n actualizada
                        respuesta_completa['output'] = f"""üîç **Dificultades para acceder a informaci√≥n en tiempo real**

Actualmente no puedo acceder a informaci√≥n actualizada sobre "{pregunta}" debido a limitaciones en las herramientas de b√∫squeda web.

üí° **Te sugiero:**
1. Consultar directamente Google o Bing
2. Visitar sitios web oficiales relacionados con tu consulta
3. Usar aplicaciones especializadas
4. Verificar redes sociales oficiales

‚ùì **¬øPuedo ayudarte con algo m√°s espec√≠fico?**
Mientras tanto, puedo responder preguntas sobre temas generales, explicaciones conceptuales, o ayudarte de otras maneras."""
                        
                        pensamientos.append("‚ö†Ô∏è Informaci√≥n en tiempo real no disponible")
                        pensamientos.append("üõ†Ô∏è Proporcionando alternativas para obtener informaci√≥n actualizada")
                
                return jsonify({
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente',
                    'modelo_usado': modelo_seleccionado,
                    'pasos_intermedios': pasos_intermedios,
                    'pensamientos': pensamientos,
                    'metadata': {
                        'duracion': duracion,
                        'duracion_formateada': duracion_formateada,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time(),
                        'timestamp_inicio': tiempo_inicio,
                        'timestamp_fin': tiempo_fin,
                        'internetHabilitado': permitir_internet
                    }
                })
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente {modelo_seleccionado}: {error_msg}")
                
                # Manejo espec√≠fico para diferentes tipos de errores
                if "iteration limit" in error_msg.lower() or "time limit" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è La b√∫squeda tom√≥ m√°s tiempo del esperado. Intentando respuesta r√°pida...\n\n"
                elif "parsing" in error_msg.lower():
                    fallback_msg = f"‚ö†Ô∏è Hubo un problema procesando la b√∫squeda. Usando respuesta directa...\n\n"
                else:
                    fallback_msg = f"‚ö†Ô∏è Error en b√∫squeda web. Usando modo simple...\n\n"
                
                # Fallback a chat simple si el agente falla
                if modelo_seleccionado in simple_chains:
                    respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                    return jsonify({
                        'respuesta': f"{fallback_msg}{respuesta}",
                        'modo': 'simple_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'metadata': {
                            'internetHabilitado': permitir_internet,
                            'iteraciones': 0,
                            'busquedas': 0
                        }
                    })
                else:
                    return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible para fallback'}), 500
        else:
            # Usar chat simple
            print(f"üîç DEBUG: Usando modo simple con modelo {modelo_seleccionado}")
            if modelo_seleccionado in simple_chains:
                print(f"üîç DEBUG: Chain encontrada para {modelo_seleccionado}")
                tiempo_inicio = time.time()
                
                # Manejo especial para DeepSeek R1 - Generar pensamientos simulados
                pensamientos_proceso = []
                if modelo_seleccionado == 'deepseek-r1:8b':
                    # Simular proceso de razonamiento paso a paso
                    pensamientos_proceso = [
                        f"üîç Analizando pregunta: '{pregunta}'",
                        "üß† Identificando conceptos clave y contexto",
                        "üìö Accediendo a conocimiento base sobre el tema",
                        "üîó Conectando informaci√≥n relevante",
                        "üìù Estructurando respuesta paso a paso",
                        "üîÑ Validando coherencia y completitud"
                    ]
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": pregunta})
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                duracion_formateada = formatear_duracion(duracion)
                
                print(f"‚úÖ Chat simple completado en {duracion_formateada}")
                
                return jsonify({
                    'respuesta': respuesta,
                    'modo': 'simple',
                    'modelo_usado': modelo_seleccionado,
                    'pensamientos': pensamientos_proceso,
                    'metadata': {
                        'duracion': duracion,
                        'duracion_formateada': duracion_formateada,
                        'timestamp': time.time(),
                        'internetHabilitado': permitir_internet,
                        'iteraciones': 0,
                        'busquedas': 0,
                        'razonamiento_visible': modelo_seleccionado == 'deepseek-r1:8b'
                    }
                })
            else:
                print(f"‚ùå DEBUG: Chain no encontrada para {modelo_seleccionado}")
                return jsonify({'error': f'Modelo {modelo_seleccionado} no disponible'}), 400
            
    except Exception as e:
        print(f"‚ùå DEBUG: Error en funci√≥n chat: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Error al procesar la pregunta: {str(e)}'}), 500

@app.route('/ejemplo-agente', methods=['POST'])
def ejemplo_agente() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para demostrar capacidades del agente"""
    try:
        ejemplos = [
            "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            "¬øCu√°l es el precio actual del Bitcoin?",
            "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            "¬øCu√°les son las √∫ltimas noticias tecnol√≥gicas?"
        ]
        
        pregunta_ejemplo = ejemplos[0]  # Tomar el primer ejemplo
        
        # Usar el primer modelo y agente disponible
        modelo_disponible = available_models[0] if available_models else None
        
        if modelo_disponible and modelo_disponible in agents and agents[modelo_disponible] is not None:
            # Usar el agente para la demo
            respuesta = agents[modelo_disponible].invoke({"input": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': respuesta['output'],
                'modo': 'agente',
                'modelo_usado': modelo_disponible
            })
        elif modelo_disponible and modelo_disponible in simple_chains:
            # Fallback: usar chat simple si el agente no est√° disponible
            respuesta_simple = simple_chains[modelo_disponible].invoke({"pregunta": pregunta_ejemplo})
            return jsonify({
                'pregunta': pregunta_ejemplo,
                'respuesta': f"[Modo Simple - Agente no disponible] {respuesta_simple}",
                'modo': 'simple',
                'modelo_usado': modelo_disponible
            })
        else:
            return jsonify({'error': 'No hay modelos disponibles para la demo'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en ejemplo de agente: {str(e)}'}), 500

@app.route('/busqueda-rapida', methods=['POST'])
def busqueda_rapida() -> Union[Response, Tuple[Response, int]]:
    """Endpoint optimizado para b√∫squedas web r√°pidas"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # B√∫squeda directa con DuckDuckGo sin agente complejo
        search_tool = DuckDuckGoSearchRun()
        
        # Mejorar t√©rminos de b√∫squeda seg√∫n el tipo de pregunta
        def mejorar_consulta_busqueda(pregunta_original):
            pregunta_lower = pregunta_original.lower()
            if 'precio' in pregunta_lower and 'bitcoin' in pregunta_lower:
                return [
                    "Bitcoin price USD current today",
                    "precio Bitcoin actual d√≥lares",
                    "BTC price now current value"
                ]
            elif 'noticias' in pregunta_lower:
                return [
                    f"noticias {pregunta_original} hoy",
                    f"latest news {pregunta_original} today",
                    f"breaking news {pregunta_original} 2025"
                ]
            elif 'openai' in pregunta_lower:
                return [
                    "OpenAI news latest updates",
                    "noticias OpenAI ChatGPT",
                    "OpenAI developments 2025"
                ]
            elif 'champions' in pregunta_lower or 'deportes' in pregunta_lower:
                return [
                    "Champions League final winner",
                    "latest sports news football",
                    "resultados deportivos recientes"
                ]
            elif 'petr√≥leo' in pregunta_lower or 'oil' in pregunta_lower:
                return [
                    "oil price current USD barrel",
                    "precio petr√≥leo actual",
                    "crude oil price today"
                ]
            
            # B√∫squeda gen√©rica mejorada
            return [pregunta_original, f"{pregunta_original} today", f"{pregunta_original} 2025"]
        
        try:
            consultas = mejorar_consulta_busqueda(pregunta)
            search_results = None
            consulta_exitosa = None
            
            # Intentar m√∫ltiples consultas hasta obtener resultados √∫tiles
            for consulta in consultas:
                try:
                    print(f"üîç Buscando: {consulta}")
                    resultado = search_tool.invoke(consulta)
                    
                    # Verificar si el resultado tiene contenido √∫til
                    if resultado and len(resultado.strip()) > 50:
                        # Verificaciones adicionales para detectar resultados irrelevantes
                        resultado_lower = resultado.lower()
                        palabras_irrelevantes = [
                            "no se encontraron resultados",
                            "p√°gina no encontrada", 
                            "error 404",
                            "no hay resultados",
                            "try again later"
                        ]
                        
                        # Verificar que no tenga palabras irrelevantes
                        es_irrelevante = any(palabra in resultado_lower for palabra in palabras_irrelevantes)
                        
                        if not es_irrelevante:
                            search_results = resultado
                            consulta_exitosa = consulta
                            print(f"‚úÖ B√∫squeda exitosa con: {consulta}")
                            break
                        else:
                            print(f"‚ö†Ô∏è Resultado irrelevante con: {consulta}")
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Error en b√∫squeda '{consulta}': {e}")
                    continue
            
            if not search_results:
                # Fallback inteligente para cualquier consulta
                modelo = get_model(modelo_seleccionado)
                if modelo and modelo_seleccionado in simple_chains:
                    prompt_fallback = f"""
                    No pude obtener informaci√≥n actualizada de internet sobre "{pregunta}". 
                    
                    Como experto asistente, proporciona una respuesta √∫til que incluya:
                    
                    1. Informaci√≥n general relevante sobre el tema consultado
                    2. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
                       - Sitios web especializados
                       - Aplicaciones m√≥viles relevantes
                       - B√∫squedas espec√≠ficas en Google
                    3. Contexto √∫til sobre el tema
                    4. Consejos pr√°cticos para encontrar la informaci√≥n
                    
                    S√© espec√≠fico y √∫til, adapt√°ndote al tipo de consulta realizada.
                    """
                    
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_fallback})
                    
                    return jsonify({
                        'respuesta': f"üåê **B√∫squeda web limitada - Respuesta inteligente:**\n\n{respuesta_fallback}",
                        'modo': 'busqueda_inteligente_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Asistente inteligente'
                    })
                else:
                    return jsonify({
                        'respuesta': "‚ùå No pude obtener informaci√≥n actualizada desde internet en este momento. Te recomiendo:\n\n1. Consultar sitios web especializados\n2. Usar aplicaciones m√≥viles relevantes\n3. Buscar en Google con t√©rminos espec√≠ficos\n4. Intentar la b√∫squeda m√°s tarde",
                        'modo': 'busqueda_fallback',
                        'modelo_usado': modelo_seleccionado,
                        'fuente': 'Error en b√∫squeda'
                    })
            
            # Usar el modelo para resumir y formatear los resultados
            modelo = get_model(modelo_seleccionado)
            if modelo and modelo_seleccionado in simple_chains:
                prompt_busqueda = f"""
                INSTRUCCIONES ESPEC√çFICAS: Eres un experto asistente que debe extraer y presentar informaci√≥n √∫til de resultados de b√∫squeda web.

                PREGUNTA DEL USUARIO: "{pregunta}"
                
                RESULTADOS DE B√öSQUEDA OBTENIDOS:
                {search_results}

                TAREAS OBLIGATORIAS:
                1. ANALIZA cuidadosamente los resultados de b√∫squeda
                2. EXTRAE cualquier informaci√≥n relevante disponible (precios, noticias, datos espec√≠ficos)
                3. Si encuentras datos parciales o relacionados, √öSALOS y s√© transparente sobre las limitaciones
                4. COMPLEMENTA con informaci√≥n contextual √∫til y recomendaciones
                5. NUNCA digas simplemente "no hay informaci√≥n" - siempre proporciona valor

                ESTRUCTURA DE RESPUESTA:
                - Informaci√≥n encontrada (aunque sea limitada)
                - Contexto adicional √∫til
                - Recomendaciones para informaci√≥n m√°s completa
                - Consejos pr√°cticos

                IMPORTANTE: S√© proactivo y √∫til. Si los resultados mencionan sitios web o aplicaciones, incorp√≥ralos como recomendaciones adicionales.
                
                RESPUESTA EN ESPA√ëOL:
                """
                
                respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_busqueda})
                
                return jsonify({
                    'respuesta': f"üîç **B√∫squeda realizada con:** {consulta_exitosa}\n\n{respuesta}",
                    'modo': 'busqueda_rapida',
                    'modelo_usado': modelo_seleccionado,
                    'fuente': 'DuckDuckGo'
                })
            else:
                return jsonify({
                    'respuesta': f"**Resultados de b√∫squeda directa:**\n\n{search_results}",
                    'modo': 'busqueda_directa',
                    'modelo_usado': 'ninguno',
                    'fuente': 'DuckDuckGo'
                })
                
        except Exception as search_error:
            return jsonify({'error': f'Error en b√∫squeda: {str(search_error)}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Error en b√∫squeda r√°pida: {str(e)}'}), 500

@app.route('/clima-directo', methods=['POST'])
def clima_directo() -> Union[Response, Tuple[Response, int]]:
    """Endpoint espec√≠fico para consultas de clima con respuesta directa inteligente"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Usar el modelo para generar una respuesta √∫til sobre clima
        modelo = get_model(modelo_seleccionado)
        if modelo and modelo_seleccionado in simple_chains:
            prompt_clima = f"""
            El usuario pregunta sobre el clima en {ciudad}. Aunque no puedo acceder a datos meteorol√≥gicos en tiempo real desde mi entrenamiento, puedo proporcionar informaci√≥n √∫til y recomendaciones pr√°cticas.

            Pregunta del usuario: "{pregunta}"

            Proporciona una respuesta √∫til que incluya:
            1. Reconocimiento de la limitaci√≥n para datos en tiempo real
            2. Informaci√≥n general sobre el clima t√≠pico de {ciudad} seg√∫n la √©poca del a√±o (es julio 2025)
            3. Recomendaciones espec√≠ficas para obtener informaci√≥n actual:
               - Sitios web espec√≠ficos (weather.com, accuweather.com, clima.com)
               - Aplicaciones m√≥viles recomendadas
               - B√∫squedas espec√≠ficas en Google
            4. Consejos pr√°cticos para el clima t√≠pico de la regi√≥n en esta √©poca

            S√© √∫til, espec√≠fico y pr√°ctico en tu respuesta.
            """
            
            respuesta = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            
            return jsonify({
                'respuesta': f"üå§Ô∏è **Informaci√≥n sobre clima en {ciudad}**\n\n{respuesta}",
                'modo': 'clima_directo',
                'modelo_usado': modelo_seleccionado,
                'fuente': 'Respuesta inteligente'
            })
        else:
            return jsonify({
                'respuesta': f"Para obtener el clima actual en {ciudad}, te recomiendo consultar:\n‚Ä¢ weather.com\n‚Ä¢ accuweather.com\n‚Ä¢ Google Weather\n‚Ä¢ Apps m√≥viles de clima",
                'modo': 'clima_basico',
                'modelo_usado': 'ninguno',
                'fuente': 'Recomendaciones est√°ticas'
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-actual', methods=['POST'])
def clima_actual() -> Union[Response, Tuple[Response, int]]:
    """Endpoint r√°pido y optimizado para consultas de clima actual"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'llama3')
        
        if not pregunta:
            return jsonify({'error': 'No se proporcion√≥ ninguna pregunta'}), 400
        
        # Extraer ciudad de la pregunta
        ciudad = 'Quito'  # Default
        pregunta_lower = pregunta.lower()
        
        if 'quito' in pregunta_lower:
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta_lower:
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta_lower:
            ciudad = 'Cuenca'
        elif 'new york' in pregunta_lower or 'nueva york' in pregunta_lower:
            ciudad = 'New York'
        elif 'madrid' in pregunta_lower:
            ciudad = 'Madrid'
        elif 'london' in pregunta_lower or 'londres' in pregunta_lower:
            ciudad = 'London'
        
        print(f"üå§Ô∏è Consulta r√°pida de clima para {ciudad}")
        tiempo_inicio = time.time()
        
        # Obtener datos del clima
        clima_data = obtener_clima_api(ciudad)
        
        if clima_data['success']:
            data_clima = clima_data['data']
            
            # Si tenemos un modelo disponible, usarlo para formatear la respuesta
            if modelo_seleccionado in simple_chains:
                prompt_clima = f"""El usuario pregunta: "{pregunta}"

Datos meteorol√≥gicos ACTUALES para {ciudad}:
üå°Ô∏è Temperatura: {data_clima['temperatura']}¬∞C
üå°Ô∏è Sensaci√≥n t√©rmica: {data_clima['sensacion_termica']}¬∞C
‚òÅÔ∏è Condiciones: {data_clima['descripcion']}
üíß Humedad: {data_clima['humedad']}%
üí® Viento: {data_clima['velocidad_viento']} km/h hacia {data_clima['direccion_viento']}
üïê √öltima actualizaci√≥n: {data_clima['hora_consulta']}

Responde de manera natural y √∫til con esta informaci√≥n actualizada."""
                
                respuesta_formateada = simple_chains[modelo_seleccionado].invoke({"pregunta": prompt_clima})
            else:
                # Respuesta b√°sica sin modelo
                respuesta_formateada = f"""üå§Ô∏è **Clima actual en {ciudad}**

üìä **Condiciones actuales:**
‚Ä¢ **Temperatura:** {data_clima['temperatura']}¬∞C
‚Ä¢ **Sensaci√≥n t√©rmica:** {data_clima['sensacion_termica']}¬∞C
‚Ä¢ **Condiciones:** {data_clima['descripcion']}
‚Ä¢ **Humedad:** {data_clima['humedad']}%
‚Ä¢ **Viento:** {data_clima['velocidad_viento']} km/h ({data_clima['direccion_viento']})
‚Ä¢ **√öltima actualizaci√≥n:** {data_clima['hora_consulta']}

*Datos obtenidos de API meteorol√≥gica en tiempo real*"""
            
            tiempo_fin = time.time()
            duracion = round(tiempo_fin - tiempo_inicio, 2)
            duracion_formateada = formatear_duracion(duracion)
            
            return jsonify({
                'respuesta': respuesta_formateada,
                'modo': 'clima_actual',
                'modelo_usado': modelo_seleccionado,
                'pensamientos': [
                    f"üå§Ô∏è Consultando clima actual en {ciudad}",
                    "üîç Conectando con API meteorol√≥gica wttr.in",
                    f"üìä Datos obtenidos: {data_clima['temperatura']}¬∞C, {data_clima['descripcion']}",
                    "‚úÖ Respuesta generada con datos en tiempo real"
                ],
                'metadata': {
                    'duracion': duracion,
                    'duracion_formateada': duracion_formateada,
                    'timestamp': time.time(),
                    'internetHabilitado': True,
                    'iteraciones': 1,
                    'busquedas': 1,
                    'ciudad': ciudad,
                    'fuente_datos': 'wttr.in',
                    'tipo_consulta': 'clima_rapido'
                }
            })
        else:
            return jsonify({
                'respuesta': f"‚ùå No pude obtener el clima actual de {ciudad}.\n\nError: {clima_data['error']}\n\nüå§Ô∏è **Recomendaciones:**\n‚Ä¢ Consulta weather.com\n‚Ä¢ Usa Google Weather\n‚Ä¢ Apps: AccuWeather, Weather Underground",
                'modo': 'clima_error',
                'modelo_usado': modelo_seleccionado,
                'metadata': {
                    'internetHabilitado': True,
                    'error': clima_data['error'],
                    'ciudad': ciudad
                }
            })
            
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima: {str(e)}'}), 500

@app.route('/clima-api', methods=['POST'])
def clima_api() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para obtener clima usando API externa gratuita"""
    try:
        data = request.get_json()
        pregunta = data.get('pregunta', '')
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        ciudad = data.get('ciudad', 'Quito')
        
        # Extraer ciudad de la pregunta si est√° presente
        if 'quito' in pregunta.lower():
            ciudad = 'Quito'
        elif 'guayaquil' in pregunta.lower():
            ciudad = 'Guayaquil'
        elif 'cuenca' in pregunta.lower():
            ciudad = 'Cuenca'
        
        print(f"üå§Ô∏è Solicitando clima para {ciudad}...")
        
        # Obtener clima usando API
        clima_data = obtener_clima_api(ciudad)
        
        # Formatear respuesta
        respuesta_formateada = formatear_respuesta_clima(clima_data, modelo_seleccionado)
        
        return jsonify({
            'respuesta': respuesta_formateada,
            'modo': 'clima_api',
            'modelo_usado': modelo_seleccionado,
            'fuente': 'wttr.in API',
            'ciudad': ciudad,
            'success': clima_data['success']
        })
        
    except Exception as e:
        return jsonify({'error': f'Error en consulta de clima API: {str(e)}'}), 500

@app.route('/agente-general', methods=['POST'])
def agente_general() -> Union[Response, Tuple[Response, int]]:
    """Endpoint para demostraciones del agente con b√∫squedas generales"""
    try:
        data = request.get_json()
        tipo_demo = data.get('tipo', 'clima')  # clima, noticias, bitcoin, deportes
        modelo_seleccionado = data.get('modelo', available_models[0] if available_models else 'gemini-1.5-flash')
        
        # Preguntas predefinidas para diferentes tipos de demos
        demos = {
            'noticias': "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?",
            'bitcoin': "¬øCu√°l es el precio actual del Bitcoin en USD?",
            'deportes': "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?",
            'tech': "¬øCu√°les son las √∫ltimas noticias sobre OpenAI?",
            'economia': "¬øCu√°l es el precio actual del petr√≥leo?",
            'general': "¬øCu√°les son las noticias m√°s importantes de hoy?"
        }
        
        pregunta = demos.get(tipo_demo, demos['noticias'])
        
        # Usar el agente mejorado
        if modelo_seleccionado in agents and agents[modelo_seleccionado] is not None:
            try:
                print(f"ü§ñ Ejecutando demo '{tipo_demo}' con {modelo_seleccionado}")
                tiempo_inicio = time.time()
                respuesta_completa = agents[modelo_seleccionado].invoke({"input": pregunta})
                tiempo_fin = time.time()
                duracion = round(tiempo_fin - tiempo_inicio, 2)
                
                # Extraer pasos intermedios si est√°n disponibles
                pasos_intermedios = []
                busquedas_count = 0
                if 'intermediate_steps' in respuesta_completa:
                    for paso in respuesta_completa['intermediate_steps']:
                        if len(paso) >= 2:
                            accion = paso[0]
                            observacion = paso[1]
                            
                            # Contar b√∫squedas
                            if accion.tool in ['web_search', 'duckduckgo_search']:
                                busquedas_count += 1
                            
                            pasos_intermedios.append({
                                'action': accion.tool,
                                'action_input': accion.tool_input,
                                'observation': observacion[:300] + '...' if len(observacion) > 300 else observacion
                            })
                
                return jsonify({
                    'pregunta': pregunta,
                    'respuesta': respuesta_completa['output'],
                    'modo': 'agente_general',
                    'modelo_usado': modelo_seleccionado,
                    'tipo_demo': tipo_demo,
                    'pasos_intermedios': pasos_intermedios,
                    'metadata': {
                        'duracion': duracion,
                        'iteraciones': len(pasos_intermedios),
                        'busquedas': busquedas_count,
                        'timestamp': time.time()
                    }
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"Error en agente general {modelo_seleccionado}: {error_msg}")
                
                # Fallback inteligente
                if modelo_seleccionado in simple_chains:
                    respuesta_fallback = simple_chains[modelo_seleccionado].invoke({
                        "pregunta": f"Aunque no puedo buscar en internet en este momento, responde de la mejor manera posible: {pregunta}"
                    })
                    
                    return jsonify({
                        'pregunta': pregunta,
                        'respuesta': f"‚ö†Ô∏è Agente no disponible. Respuesta b√°sica:\n\n{respuesta_fallback}",
                        'modo': 'fallback_general',
                        'modelo_usado': modelo_seleccionado,
                        'tipo_demo': tipo_demo
                    })
        
        return jsonify({'error': 'Agente no disponible'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Error en agente general: {str(e)}'}), 500

if __name__ == '__main__':
    print("ü§ñ Iniciando aplicaci√≥n de IA con Agentes...")
    print(f"üß† Modelos disponibles: {', '.join(available_models)}")
    print("üîç Herramientas: B√∫squeda web avanzada")
    print("üåê Servidor: http://127.0.0.1:5000")
    print("üöÄ Aplicaci√≥n de consultas generales lista!")
    app.run(debug=True, host='127.0.0.1', port=5000)
