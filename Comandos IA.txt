 pip install langchain langchain-openai python-dotenv
 
Request
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
  -H 'Content-Type: application/json' \
  -H 'X-goog-api-key: AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
 
Response 

C:\Users\Kevin Arauz\Documents\Proyectos IA>curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" -H "Content-Type: application/json" -H "X-goog-api-key: AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0" -X POST -d "{\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works in a few words\"}]}]}"
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "AI learns patterns from data to make predictions or decisions.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.048756192127863564
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 8,
    "candidatesTokenCount": 12,
    "totalTokenCount": 20,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 8
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 12
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash",
  "responseId": "xo6JaNjFFMOg7dcP_ZekiAE"
}

C:\Users\Kevin Arauz\Documents\Proyectos IA> 
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" -H "Content-Type: application/json" -H "X-goog-api-key: AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0" -X POST -d "{\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works in a few words\"}]}]}"

Invoke-WebRequest -Uri "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" -Method POST -ContentType "application/json" -Headers @{"X-goog-api-key"="AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0"} -Body '{"contents":[{"parts":[{"text":"Explain how AI works in a few words"}]}]}'

pip install -q -U google-genai

Pagina de documentacion
https://ai.google.dev/gemini-api/docs/quickstart?lang=python&hl=es-419

from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)

La opción "Pensando" está activada de forma predeterminada en muchas de nuestras muestras de código.

from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)

como usar longchain

Incluyendo un prompt base o prompt system antes de cada mensaje.

Usando funciones de system_prompt o memory buffer en frameworks como LangChain, LlamaIndex, etc.

si es posible

modelo llama

ollama run gemma3
ollama pull llama3
ollama run gemma:2b

ollama list