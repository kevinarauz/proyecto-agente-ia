# ü§ñ Agente de IA - Aplicaci√≥n Web Inteligente

¬°Aplicaci√≥n web completa con agentes de IA que pueden realizar b√∫squedas web y responder preguntas! Esta aplicaci√≥n utiliza Python, Flask, LangChain y Google Gemini para crear un asistente inteligente con capacidades avanzadas.

## ‚ú® Caracter√≠sticas Principales

- **Chat Simple**: Respuestas directas usando Google Gemini
- **Agente con B√∫squeda Web**: Puede buscar informaci√≥n actual en internet
- **Interfaz Moderna**: Dise√±o responsive con Bootstrap y animaciones
- **Dos Modos de Operaci√≥n**: Simple y Agente con herramientas
- **Demo Interactivo**: Ejemplos predefinidos para probar las capacidades

## üöÄ Ejecuci√≥n R√°pida

La aplicaci√≥n ya est√° configurada y lista para usar. Simplemente ejecuta:

```bash
python app.py
```

Luego abre tu navegador en: `http://127.0.0.1:5000`

> **‚ö†Ô∏è Nota:** Si experimentas errores, consulta el archivo `SOLUCION_ERRORES.md` que contiene soluciones a problemas comunes como errores 500, problemas de dependencias y configuraci√≥n del agente.

## üéØ Concepto del Proyecto

Esta aplicaci√≥n demuestra el poder de los **Agentes de IA**: sistemas que no solo responden preguntas, sino que pueden usar herramientas para realizar acciones y obtener informaci√≥n actualizada.

**Flujo de trabajo principal:**

1.  **Frontend (UI/UX):** El usuario introduce datos (texto, preguntas, etc.) en una interfaz web construida con HTML, CSS y JavaScript.
2.  **Backend (Flask):** Un servidor Flask recibe la solicitud del usuario.
3.  **Orquestador (LangChain):** Flask pasa la solicitud a LangChain, que la formatea y la env√≠a al modelo de IA apropiado (Google Gemini).
4.  **Motor de IA (Google Gemini):** El modelo procesa la solicitud y genera una respuesta.
5.  **Respuesta al Usuario:** LangChain devuelve la respuesta a Flask, que la renderiza en la plantilla HTML y la env√≠a de vuelta al navegador del usuario.

---

## 2. Pila Tecnol√≥gica (Tech Stack)

| Categor√≠a | Tecnolog√≠a | Prop√≥sito |
| :--- | :--- | :--- |
| **Backend** | ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) | Lenguaje de programaci√≥n principal para la l√≥gica del servidor. |
| | ![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white) | Micro-framework web para crear las rutas y manejar las solicitudes HTTP. |
| **Inteligencia Artificial**| ![LangChain](https://img.shields.io/badge/LangChain-000000?style=for-the-badge) | Framework para simplificar la interacci√≥n con los modelos de lenguaje. |
| | ![Google Gemini](https://img.shields.io/badge/Google_Gemini-8E75B7?style=for-the-badge&logo=google-gemini&logoColor=white) | El modelo de lenguaje grande (LLM) que proporciona la "inteligencia". |
| **Frontend** | ![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white) | Estructura de la p√°gina web. |
| | ![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white) | Estilos y dise√±o visual. |
| | ![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black) | Interactividad en el lado del cliente. |
| **UI/UX Frameworks** | ![Bootstrap](https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white) | Framework CSS para un dise√±o responsive y componentes pre-dise√±ados. |
| | ![SweetAlert2](https://img.shields.io/badge/SweetAlert2-850000?style=for-the-badge) | Librer√≠a para crear alertas y modales atractivos y personalizables. |
| | ![AOS](https://img.shields.io/badge/AOS-000000?style=for-the-badge) | Librer√≠a para animaciones al hacer scroll (Animate On Scroll). |

---

## 3. Gu√≠a de Estilo y Dise√±o Frontend (UI/UX)

Para mantener una consistencia visual y una experiencia de usuario de alta calidad, este proyecto sigue una serie de directrices de dise√±o.

### a) Filosof√≠a de Dise√±o
- **Simplicidad y Claridad:** La interfaz debe ser intuitiva y f√°cil de navegar.
- **Consistencia:** Los elementos recurrentes (botones, tarjetas, men√∫s) deben lucir y comportarse de la misma manera en toda la aplicaci√≥n.
- **Mobile-First:** El dise√±o se piensa primero para dispositivos m√≥viles y luego se adapta a pantallas m√°s grandes, garantizando una experiencia √≥ptima en cualquier dispositivo.

### b) Framework Principal: Bootstrap 5
La base de todo el layout y los componentes es **Bootstrap 5**. Se debe priorizar el uso de sus clases y componentes nativos (`container`, `row`, `col-`, `btn`, `card`, `navbar`, etc.) para agilizar el desarrollo y asegurar la responsividad.

### c) Tipograf√≠a
- **Fuente Principal:** Se recomienda usar una fuente sans-serif moderna y legible como **'Inter'** o **'Poppins'**, importada desde Google Fonts.
- **Jerarqu√≠a de T√≠tulos:**
    - `<h1>`: T√≠tulo principal de la p√°gina (ej. 48px).
    - `<h2>`: T√≠tulos de secciones principales (ej. 36px).
    - `<h3>`: Subt√≠tulos o t√≠tulos de tarjetas (ej. 24px).
    - `<p>`: Texto de p√°rrafo (ej. 16px o 1rem).

### d) Layout y Estructura
- **Grid System de Bootstrap:** Para la maquetaci√≥n general de la p√°gina (divisi√≥n en columnas), se debe usar el sistema de rejilla de Bootstrap (`<div class="row">` y `<div class="col-md-6">`).
- **Flexbox:** Para alinear elementos dentro de un componente (centrar un √≠cono y texto en un bot√≥n, distribuir elementos en una cabecera), se deben usar las utilidades de Flexbox de Bootstrap (`d-flex`, `justify-content-*`, `align-items-*`).

### e) Componentes Clave
- **Men√∫s de Navegaci√≥n:** Utilizar el componente `navbar` de Bootstrap, asegurando que sea colapsable en m√≥viles.
- **Botones:** Usar las clases `.btn` de Bootstrap (`.btn-primary`, `.btn-secondary`) para mantener la consistencia en los llamados a la acci√≥n.
- **Tarjetas (Cards):** Para mostrar contenido encapsulado (como las respuestas de la IA), usar el componente `.card` de Bootstrap.

### f) Animaciones y Efectos
- **AOS (Animate On Scroll):** Para animaciones sutiles al hacer scroll y revelar contenido.
- **Transiciones CSS:** Para efectos `hover` en botones y enlaces, usar transiciones suaves (`transition: all 0.3s ease;`) en lugar de cambios bruscos.

---

## 4. Estructura del Proyecto

Para mantener el c√≥digo organizado y escalable, se recomienda la siguiente estructura de carpetas, que separa la l√≥gica (Python), las plantillas (HTML) y los archivos est√°ticos (CSS, JS).

```
/mi_proyecto_ia/
|
‚îú‚îÄ‚îÄ app.py                  # Archivo principal de la aplicaci√≥n Flask y l√≥gica de LangChain
‚îú‚îÄ‚îÄ .env                    # Archivo para guardar claves de API (¬°NO subir a Git!)
‚îú‚îÄ‚îÄ requirements.txt        # Lista de dependencias de Python
|
‚îú‚îÄ‚îÄ /templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html          # Plantilla HTML principal para la interfaz de usuario
|
‚îî‚îÄ‚îÄ /static/
    ‚îú‚îÄ‚îÄ /css/
    ‚îÇ   ‚îî‚îÄ‚îÄ style.css       # Tus estilos CSS personalizados
    ‚îî‚îÄ‚îÄ /js/
        ‚îî‚îÄ‚îÄ script.js       # Tu c√≥digo JavaScript personalizado
```

---

## 5. Gu√≠a de Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el proyecto en tu m√°quina local. Los comandos est√°n adaptados para Windows.

1.  **Clonar el Repositorio** (si estuviera en Git)
    ```bash
    git clone <url-del-repositorio>
    cd mi_proyecto_ia
    ```

2.  **Crear y Activar Entorno Virtual**
    ```bash
    # Crear el entorno (funciona en CMD y PowerShell)
    python -m venv venv

    # Activar en Windows (CMD y PowerShell)
    .\venv\Scripts\activate
    ```

3.  **Instalar Dependencias desde `requirements.txt`**

    El archivo `requirements.txt` es una lista de todas las librer√≠as de Python que nuestro proyecto necesita para funcionar. El comando `pip install -r` lee este archivo e instala todo autom√°ticamente.

    **Paso 3.1: Crear el archivo**
    En la carpeta ra√≠z de tu proyecto (`/mi_proyecto_ia/`), crea un archivo de texto llamado `requirements.txt`.

    **Paso 3.2: A√±adir el contenido**
    Abre el archivo y pega la siguiente lista de librer√≠as:
    ```txt
    flask
    langchain
    langchain-google-genai
    langchain-community # Necesario para modelos locales con Ollama
    python-dotenv
    duckduckgo-search   # Herramienta para agentes
    ```

    **Paso 3.3: Instalar las librer√≠as**
    Aseg√∫rate de que tu entorno virtual est√© activo (deber√≠as ver `(venv)` en tu terminal). Luego, ejecuta el siguiente comando:
    ```bash
    pip install -r requirements.txt
    ```
    Pip leer√° el archivo e instalar√° cada una de las librer√≠as especificadas.

4.  **Configurar Clave de API (Opcional, para Gemini)**
    Crea un archivo `.env` en la ra√≠z del proyecto y a√±ade tu clave de la API de Google Gemini:
    ```
    GOOGLE_API_KEY="AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0"
    ```

---

## 6. Interacci√≥n con la API de Gemini

Existen dos formas principales de comunicarse con la API:

### a) Directa (con cURL o PowerShell)

√ötil para pruebas r√°pidas desde la terminal. Esto te permite entender la estructura fundamental de una petici√≥n a la API.

**Nota sobre el Token de Autenticaci√≥n:**
Toda petici√≥n directa a la API debe incluir tu clave secreta (`API Key`) en los encabezados. Este es tu "token" de autenticaci√≥n. En los ejemplos siguientes, se env√≠a en el encabezado `-H "X-goog-api-key: TU_API_KEY"`. Este paso es crucial sin importar el lenguaje o herramienta que uses.

#### Request (Opci√≥n 1: para S√≠mbolo del sistema - CMD)
```cmd
curl "[https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent](https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent)" -H "Content-Type: application/json" -H "X-goog-api-key: AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0" -X POST -d "{\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works in a few words\"}]}]}"
```

#### Request (Opci√≥n 2: para PowerShell)
```powershell
Invoke-WebRequest -Uri "[https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent](https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent)" -Method POST -ContentType "application/json" -Headers @{"X-goog-api-key"="AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0"} -Body '{"contents":[{"parts":[{"text":"Explain how AI works in a few words"}]}]}'
```

#### Response (Respuesta de la API)
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "AI learns patterns from data to make predictions or decisions.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.048756192127863564
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 8,
    "candidatesTokenCount": 12,
    "totalTokenCount": 20
  }
}
```

### b) A trav√©s del SDK de Python (Recomendado para aplicaciones)

Esta es la forma limpia y profesional de integrar Gemini en tu c√≥digo Python. El SDK maneja toda la complejidad de las solicitudes HTTP (encabezados, JSON, etc.) por ti.

```python
import os
import google.generativeai as genai
from dotenv import load_dotenv

# Carga la clave desde el archivo .env
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Elige el modelo
model = genai.GenerativeModel('gemini-2.0-flash')

# Genera el contenido
response = model.generate_content("Explain how AI works in a few words")

# Imprime la respuesta
print(response.text)
```

---

## 7. Ejecuci√≥n de la Aplicaci√≥n

Una vez que todo est√° configurado, puedes iniciar el servidor de desarrollo de Flask con un simple comando:

```bash
python app.py
```

Abre tu navegador y visita `http://127.0.0.1:5000` para ver tu aplicaci√≥n en acci√≥n.

---

## 8. Prompting Avanzado: Dando Personalidad y Contexto

Para crear aplicaciones m√°s sofisticadas, no basta con enviar solo la pregunta del usuario. Necesitas darle un contexto y rol a la IA.

### a) Usando un "System Prompt" (Mensaje de Sistema)

Un **System Prompt** es una instrucci√≥n base que se env√≠a a la IA en cada solicitud para definir su **rol, tono, o las reglas que debe seguir**. Es la forma de decirle "T√∫ eres..." antes de que vea la pregunta del usuario.

LangChain facilita esto con `ChatPromptTemplate`, que distingue entre mensajes del sistema y mensajes del usuario.

**Ejemplo de implementaci√≥n:**

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# Configura el modelo, asegur√°ndote de pasar la clave de API
llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=os.getenv("GOOGLE_API_KEY"))

# Crea la plantilla del prompt con un mensaje de sistema
prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente experto en historia del Ecuador. Responde siempre en un tono amigable y educativo."),
    ("user", "{pregunta_del_usuario}")
])

# Crea el parser de salida
output_parser = StrOutputParser()

# Construye la cadena (chain)
chain = prompt | llm | output_parser

# Invoca la cadena con una pregunta
respuesta = chain.invoke({"pregunta_del_usuario": "¬øQui√©n fue Eloy Alfaro?"})
print(respuesta)
```

### b) Manteniendo Memoria en la Conversaci√≥n (Memory Buffer)

Para que un chatbot recuerde interacciones pasadas en una misma sesi√≥n (por ejemplo, si el usuario dice "expl√≠came m√°s sobre eso"), necesitas a√±adir un **buffer de memoria**.

En LangChain, esto se logra con objetos de memoria como `ConversationBufferMemory`. Este objeto almacena el historial de la conversaci√≥n y lo inyecta autom√°ticamente en el prompt en cada nueva interacci√≥n, dando a la IA el contexto completo de lo que se ha hablado.

Integrar memoria es un paso m√°s avanzado, pero es esencial para construir verdaderos asistentes conversacionales.

---

## 9. Alternativa Local: Ejecutando un LLM en tu Propia M√°quina

No siempre necesitas depender de una API de pago. Puedes ejecutar modelos de lenguaje de c√≥digo abierto como **Llama 3**, **Gemma** o **Phi-3** directamente en tu computadora. La forma m√°s sencilla de hacerlo es con **Ollama**.

### a) ¬øQu√© es Ollama?

Ollama es una herramienta que simplifica la descarga, configuraci√≥n y ejecuci√≥n de LLMs en tu m√°quina local. Crea un servidor local al que puedes hacer peticiones, de forma muy similar a como lo har√≠as con una API en la nube.

### b) Instalaci√≥n de Ollama

1.  **Descarga Ollama:** Ve a [ollama.com](https://ollama.com/) e instala la aplicaci√≥n para Windows.
2.  **Verifica la Instalaci√≥n:** Abre una nueva terminal (CMD o PowerShell) y ejecuta `ollama`. Si ves una lista de comandos, la instalaci√≥n fue exitosa.

### c) Gestionando Modelos con Ollama

Puedes tener m√∫ltiples modelos descargados y cambiar entre ellos f√°cilmente.

#### 1. Listar los Modelos Instalados
Para ver qu√© modelos ya tienes en tu m√°quina, usa el comando `list`.

```bash
ollama list
```
Esto te mostrar√° una tabla con el nombre, ID, tama√±o y fecha de modificaci√≥n de cada modelo.

#### 2. Descargar un Modelo (`pull`)
El comando `pull` solo descarga un modelo de internet a tu disco duro para tenerlo listo.

```bash
# Descargar la versi√≥n est√°ndar (8B) de Llama 3
ollama pull llama3

# Descargar la versi√≥n m√°s liviana (2B) de Gemma
ollama pull gemma:2b
```

#### 3. Ejecutar un Modelo (`run`)
El comando `run` inicia una sesi√≥n de chat interactiva con el modelo que elijas. Si el modelo no est√° descargado, `run` lo descargar√° autom√°ticamente primero.

```bash
# Iniciar un chat con Llama 3
ollama run llama3

# Iniciar un chat con la versi√≥n 2B de Gemma
ollama run gemma:2b
```
Puedes usar `Ctrl+D` o escribir `/bye` para salir de la sesi√≥n de chat.

### d) Eligiendo un Modelo Local
No todos los modelos son iguales. Aqu√≠ tienes una comparaci√≥n r√°pida de los modelos m√°s populares para ayudarte a decidir cu√°l usar.

| Modelo | Par√°metros | Tama√±o (Peso) | Uso Ideal | Comando `pull` |
| :--- | :--- | :--- | :--- | :--- |
| **Llama 3** | 8B | ~4.7 GB | **El mejor todoterreno.** Excelente razonamiento, bueno para c√≥digo y seguir instrucciones complejas. Ideal para empezar. | `ollama pull llama3` |
| **Gemma** | 2B | ~1.7 GB | **El m√°s liviano.** Perfecto para m√°quinas con pocos recursos (4-8 GB RAM). Bueno para tareas creativas y chat general. | `ollama pull gemma:2b` |
| **Phi-3** | 3.8B | ~2.3 GB | **Potente y peque√±o.** Muy bueno en l√≥gica y c√≥digo, a menudo supera a modelos m√°s grandes en estas √°reas. Gran alternativa a Llama 3. | `ollama pull phi3` |

**B** = Billones (Miles de millones) de par√°metros. M√°s par√°metros generalmente significan mayor capacidad, pero tambi√©n mayores requisitos de hardware.

### e) Usando Modelos Locales como una API
La gran ventaja de Ollama es que, por defecto, **expone cualquier modelo que ejecutes como una API REST en tu `localhost`**. Esto te permite desarrollar tu aplicaci√≥n consumiendo una API local gratuita, y si en el futuro necesitas m√°s potencia, solo tienes que cambiar la URL a un proveedor de API en la nube.

1.  **Aseg√∫rate de que Ollama est√© corriendo:** La aplicaci√≥n de Ollama debe estar en ejecuci√≥n en tu bandeja del sistema en Windows.
2.  **Haz una petici√≥n a la API local:** Abre una terminal y usa `curl` para enviar una petici√≥n al servidor de Ollama, que corre en el puerto `11434`.

**Ejemplo de API Request a Llama 3 local (CMD):**
```cmd
curl http://localhost:11434/api/generate -d "{\"model\": \"llama3\", \"prompt\": \"Why is the sky blue?\", \"stream\": false}"
```
* `"model": "llama3"`: Especifica qu√© modelo de los que tienes descargados quieres usar.
* `"prompt": "..."`: Tu pregunta.
* `"stream": false`: Para recibir la respuesta completa de una vez.

Esto te devolver√° un JSON muy similar al de la API de Gemini, demostrando que puedes desarrollar con la misma l√≥gica de API.

### f) Integraci√≥n con LangChain

LangChain tiene una integraci√≥n nativa con Ollama, lo que hace que cambiar de un modelo en la nube a uno local sea trivial.

**Ejemplo de implementaci√≥n:**

```python
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. Inicializa el modelo local a trav√©s de Ollama
# Aseg√∫rate de que el nombre del modelo coincida con uno de tu lista de `ollama list`.
llm = ChatOllama(model="llama3")

# 2. Crea el prompt (puedes usar el mismo formato que antes)
prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un pirata. Responde a todas las preguntas con jerga de pirata."),
    ("user", "{pregunta}")
])

# 3. Construye la cadena
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

# 4. Invoca la cadena
respuesta = chain.invoke({"pregunta": "¬øCu√°l es la capital de Jap√≥n?"})
print(respuesta)
# Salida esperada: "¬°Ahoy, marinero! La capital de esas tierras lejanas es Tokio, ¬°arrr!"
```

Usar un modelo local es una excelente opci√≥n para desarrollo, prototipado r√°pido y para aplicaciones donde la privacidad de los datos es una prioridad, ya que ninguna informaci√≥n sale de tu m√°quina.

---

## 10. M√°s All√° de las Preguntas: Automatizaci√≥n de Tareas con Agentes de IA

El verdadero poder de los LLMs no es solo responder preguntas, sino **realizar acciones**. Para esto, usamos **Agentes de IA**.

### a) ¬øQu√© es un Agente de IA?

Un agente es un sistema que utiliza un LLM como su "cerebro" para razonar y decidir qu√© acciones tomar. En lugar de solo generar texto, un agente puede usar un conjunto de **herramientas** para interactuar con el mundo exterior.

El ciclo de un agente es:
1.  **Observa:** Recibe una solicitud del usuario (ej. "Investiga el clima en Guayaquil y res√∫melo").
2.  **Piensa:** El LLM decide qu√© herramienta necesita para cumplir la solicitud (ej. "Necesito una herramienta de b√∫squeda web").
3.  **Act√∫a:** El agente ejecuta la herramienta elegida (ej. busca en internet "clima en Guayaquil").
4.  **Observa de nuevo:** Recibe el resultado de la herramienta (ej. una p√°gina con datos del clima).
5.  **Repite:** El LLM procesa el resultado y decide el siguiente paso (ej. "Ahora voy a resumir este texto") hasta que la tarea original est√© completa.

### b) Ejemplo Pr√°ctico: Un Agente de B√∫squeda Web

Vamos a crear un agente simple que puede navegar por internet para responder preguntas sobre eventos actuales. Usaremos la herramienta de b√∫squeda de DuckDuckGo, que no requiere clave de API.

**1. Actualiza tus dependencias:**
Aseg√∫rate de que tu archivo `requirements.txt` incluya `duckduckgo-search` y vuelve a instalar las dependencias si es necesario.

**2. C√≥digo del Agente:**
Este c√≥digo crea un agente que sabe usar la herramienta de b√∫squeda.

```python
from langchain_community.chat_models import ChatOllama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain import hub

# 1. Inicializa el modelo local (el cerebro del agente)
llm = ChatOllama(model="llama3")

# 2. Define las herramientas que el agente puede usar
# En este caso, solo una herramienta para buscar en DuckDuckGo
tools = [DuckDuckGoSearchRun()]

# 3. Carga un prompt pre-dise√±ado para agentes (ReAct)
# Este prompt le ense√±a al LLM c√≥mo pensar y usar herramientas.
prompt = hub.pull("hwchase17/react")

# 4. Crea el agente
agent = create_react_agent(llm, tools, prompt)

# 5. Crea el "ejecutor" del agente, que es lo que realmente corre el ciclo
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 6. Invoca al agente con una pregunta que requiera informaci√≥n actual
# El agente se dar√° cuenta de que no sabe la respuesta y usar√° la herramienta de b√∫squeda.
pregunta = "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?"
respuesta = agent_executor.invoke({"input": pregunta})

print(respuesta)
```

Al ejecutar este c√≥digo con `verbose=True`, ver√°s en la terminal todo el proceso de "pensamiento" del agente: c√≥mo decide usar la herramienta de b√∫squeda, qu√© busca, qu√© encuentra y c√≥mo formula la respuesta final. Esto es la automatizaci√≥n de tareas en acci√≥n.

---