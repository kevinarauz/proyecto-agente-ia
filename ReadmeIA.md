# ü§ñ Agente de IA - Aplicaci√≥n Web Inteligente

¬°Aplicaci√≥n web completa con agentes de IA que pueden realizar b√∫squedas web y responder preguntas! Esta aplicaci√≥n utiliza Python, Flask, LangChain y Google Gemini para crear un asistente inteligente con capacidades avanzadas.

## ‚ú® Caracter√≠sticas Principales

- **Chat Simple**: Respuestas directas usando Google Gemini
- **Agente con B√∫squeda Web**: Puede buscar informaci√≥n actual en internet
- **Interfaz Moderna**: Dise√±o responsive con Bootstrap y animaciones
- **Dos Modos de Operaci√≥n**: Simple y Agente con herramientas
- **Demo Interactivo**: Ejemplos predefinidos para probar las capacidades

## üöÄ Ejecuci√≥n R√°pida

La aplicaci√≥n ya est√° configurada y lista para usar. Simplemente ejecuta:

```bash
python app.py
```

Luego abre tu navegador en: `http://127.0.0.1:5000`

> **‚ö†Ô∏è Nota:** Si experimentas errores, consulta la secci√≥n de **Troubleshooting** al final de este documento o el archivo `SOLUCION_ERRORES.md`.

## üéØ Concepto del Proyecto

Esta aplicaci√≥n demuestra el poder de los **Agentes de IA**: sistemas que no solo responden preguntas, sino que pueden usar herramientas para realizar acciones y obtener informaci√≥n actualizada.

**Flujo de trabajo principal:**

1.  **Frontend (UI/UX):** El usuario introduce datos (texto, preguntas, etc.) en una interfaz web construida con HTML, CSS y JavaScript.
2.  **Backend (Flask):** Un servidor Flask recibe la solicitud del usuario.
3.  **Orquestador (LangChain):** Flask pasa la solicitud a LangChain, que la formatea y la env√≠a al modelo de IA apropiado (Google Gemini).
4.  **Motor de IA (Google Gemini):** El modelo procesa la solicitud y genera una respuesta.
5.  **Respuesta al Usuario:** LangChain devuelve la respuesta a Flask, que la renderiza en la plantilla HTML y la env√≠a de vuelta al navegador del usuario.

---

## 2. Pila Tecnol√≥gica (Tech Stack)

| Categor√≠a | Tecnolog√≠a | Prop√≥sito |
| :--- | :--- | :--- |
| **Backend** | ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) | Lenguaje de programaci√≥n principal para la l√≥gica del servidor. |
| | ![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white) | Micro-framework web para crear las rutas y manejar las solicitudes HTTP. |
| **Inteligencia Artificial**| ![LangChain](https://img.shields.io/badge/LangChain-000000?style=for-the-badge) | Framework para simplificar la interacci√≥n con los modelos de lenguaje. |
| | ![Google Gemini](https://img.shields.io/badge/Google_Gemini-8E75B7?style=for-the-badge&logo=google-gemini&logoColor=white) | El modelo de lenguaje grande (LLM) que proporciona la "inteligencia". |
| **Frontend** | ![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white) | Estructura de la p√°gina web. |
| | ![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white) | Estilos y dise√±o visual. |
| | ![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black) | Interactividad en el lado del cliente. |
| **UI/UX Frameworks** | ![Bootstrap](https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white) | Framework CSS para un dise√±o responsive y componentes pre-dise√±ados. |
| | ![SweetAlert2](https://img.shields.io/badge/SweetAlert2-850000?style=for-the-badge) | Librer√≠a para crear alertas y modales atractivos y personalizables. |
| | ![AOS](https://img.shields.io/badge/AOS-000000?style=for-the-badge) | Librer√≠a para animaciones al hacer scroll (Animate On Scroll). |

---

## 3. Gu√≠a de Estilo y Dise√±o Frontend (UI/UX)

Para mantener una consistencia visual y una experiencia de usuario de alta calidad, este proyecto sigue una serie de directrices de dise√±o.

### a) Filosof√≠a de Dise√±o
- **Simplicidad y Claridad:** La interfaz debe ser intuitiva y f√°cil de navegar.
- **Consistencia:** Los elementos recurrentes (botones, tarjetas, men√∫s) deben lucir y comportarse de la misma manera en toda la aplicaci√≥n.
- **Mobile-First:** El dise√±o se piensa primero para dispositivos m√≥viles y luego se adapta a pantallas m√°s grandes, garantizando una experiencia √≥ptima en cualquier dispositivo.

### b) Framework Principal: Bootstrap 5
La base de todo el layout y los componentes es **Bootstrap 5**. Se debe priorizar el uso de sus clases y componentes nativos (`container`, `row`, `col-`, `btn`, `card`, `navbar`, etc.) para agilizar el desarrollo y asegurar la responsividad.

### c) Tipograf√≠a
- **Fuente Principal:** Se recomienda usar una fuente sans-serif moderna y legible como **'Inter'** o **'Poppins'**, importada desde Google Fonts.
- **Jerarqu√≠a de T√≠tulos:**
    - `<h1>`: T√≠tulo principal de la p√°gina (ej. 48px).
    - `<h2>`: T√≠tulos de secciones principales (ej. 36px).
    - `<h3>`: Subt√≠tulos o t√≠tulos de tarjetas (ej. 24px).
    - `<p>`: Texto de p√°rrafo (ej. 16px o 1rem).

### d) Layout y Estructura
- **Grid System de Bootstrap:** Para la maquetaci√≥n general de la p√°gina (divisi√≥n en columnas), se debe usar el sistema de rejilla de Bootstrap (`<div class="row">` y `<div class="col-md-6">`).
- **Flexbox:** Para alinear elementos dentro de un componente (centrar un √≠cono y texto en un bot√≥n, distribuir elementos en una cabecera), se deben usar las utilidades de Flexbox de Bootstrap (`d-flex`, `justify-content-*`, `align-items-*`).

### e) Componentes Clave
- **Men√∫s de Navegaci√≥n:** Utilizar el componente `navbar` de Bootstrap, asegurando que sea colapsable en m√≥viles.
- **Botones:** Usar las clases `.btn` de Bootstrap (`.btn-primary`, `.btn-secondary`) para mantener la consistencia en los llamados a la acci√≥n.
- **Tarjetas (Cards):** Para mostrar contenido encapsulado (como las respuestas de la IA), usar el componente `.card` de Bootstrap.

### f) Animaciones y Efectos
- **AOS (Animate On Scroll):** Para animaciones sutiles al hacer scroll y revelar contenido.
- **Transiciones CSS:** Para efectos `hover` en botones y enlaces, usar transiciones suaves (`transition: all 0.3s ease;`) en lugar de cambios bruscos.

---

## 4. Estructura del Proyecto

Para mantener el c√≥digo organizado y escalable, se recomienda la siguiente estructura de carpetas.

```
/mi_proyecto_ia/
|
‚îú‚îÄ‚îÄ app.py                  # Archivo principal de la aplicaci√≥n Flask y l√≥gica de LangChain
‚îú‚îÄ‚îÄ .env                    # Archivo para guardar claves de API (¬°NO subir a Git!)
‚îú‚îÄ‚îÄ requirements.txt        # Lista de dependencias de Python
|
‚îú‚îÄ‚îÄ /templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html          # Plantilla HTML principal para la interfaz de usuario
|
‚îî‚îÄ‚îÄ /static/
    ‚îú‚îÄ‚îÄ /css/
    ‚îÇ   ‚îî‚îÄ‚îÄ style.css       # Tus estilos CSS personalizados
    ‚îî‚îÄ‚îÄ /js/
        ‚îî‚îÄ‚îÄ script.js       # Tu c√≥digo JavaScript personalizado
```

---

## 5. Gu√≠a de Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el proyecto en tu m√°quina local. Los comandos est√°n adaptados para Windows.

1.  **Clonar el Repositorio** (si estuviera en Git)
    ```bash
    git clone <url-del-repositorio>
    cd mi_proyecto_ia
    ```

2.  **Crear y Activar Entorno Virtual**
    ```bash
    # Crear el entorno (funciona en CMD y PowerShell)
    python -m venv venv

    # Activar en Windows (CMD y PowerShell)
    .\venv\Scripts\activate
    ```

3.  **Instalar Dependencias desde `requirements.txt`**

    El archivo `requirements.txt` es una lista de todas las librer√≠as de Python que nuestro proyecto necesita para funcionar. El comando `pip install -r` lee este archivo e instala todo autom√°ticamente.

    **Paso 3.1: Crear el archivo**
    En la carpeta ra√≠z de tu proyecto (`/mi_proyecto_ia/`), crea un archivo de texto llamado `requirements.txt`.

    **Paso 3.2: A√±adir el contenido**
    Abre el archivo y pega la siguiente lista de librer√≠as:
    ```txt
    flask
    langchain
    langchain-google-genai
    langchain-community # Necesario para modelos locales con Ollama
    python-dotenv
    duckduckgo-search   # Herramienta para agentes
    ```

    **Paso 3.3: Instalar las librer√≠as**
    Aseg√∫rate de que tu entorno virtual est√© activo (deber√≠as ver `(venv)` en tu terminal). Luego, ejecuta el siguiente comando:
    ```bash
    pip install -r requirements.txt
    ```
    Pip leer√° el archivo e instalar√° cada una de las librer√≠as especificadas.

4.  **Configurar Clave de API (Opcional, para Gemini)**
    Crea un archivo `.env` en la ra√≠z del proyecto y a√±ade tu clave de la API de Google Gemini:
    ```
    GOOGLE_API_KEY="AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0"
    ```

---

## 6. Interacci√≥n con la API de Gemini

Existen dos formas principales de comunicarse con la API:

### a) Directa (con cURL o PowerShell)

√ötil para pruebas r√°pidas desde la terminal. Esto te permite entender la estructura fundamental de una petici√≥n a la API.

**Nota sobre el Token de Autenticaci√≥n:**
Toda petici√≥n directa a la API debe incluir tu clave secreta (`API Key`) en los encabezados. Este es tu "token" de autenticaci√≥n.

#### Request (Opci√≥n 1: para S√≠mbolo del sistema - CMD)
```cmd
curl "[https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent](https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent)" -H "Content-Type: application/json" -H "X-goog-api-key: AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0" -X POST -d "{\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works in a few words\"}]}]}"
```

#### Request (Opci√≥n 2: para PowerShell)
```powershell
Invoke-WebRequest -Uri "[https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent](https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent)" -Method POST -ContentType "application/json" -Headers @{"X-goog-api-key"="AIzaSyCypmLSsEUQ7qoCYZXjy_pbXRKVR1a51D0"} -Body '{"contents":[{"parts":[{"text":"Explain how AI works in a few words"}]}]}'
```

#### Response (Respuesta de la API)
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "AI learns patterns from data to make predictions or decisions.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP"
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 8,
    "candidatesTokenCount": 12,
    "totalTokenCount": 20
  }
}
```

### b) A trav√©s del SDK de Python (Recomendado para aplicaciones)

Esta es la forma limpia y profesional de integrar Gemini en tu c√≥digo Python.

```python
import os
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

model = genai.GenerativeModel('gemini-2.0-flash')
response = model.generate_content("Explain how AI works in a few words")
print(response.text)
```

---

## 7. Ejecuci√≥n de la Aplicaci√≥n

Una vez que todo est√° configurado, puedes iniciar el servidor de desarrollo de Flask:

```bash
python app.py
```

Abre tu navegador y visita `http://127.0.0.1:5000` para ver tu aplicaci√≥n en acci√≥n.

---

## 8. Prompting Avanzado: Dando Personalidad y Contexto

### a) Usando un "System Prompt" (Mensaje de Sistema)

Un **System Prompt** es una instrucci√≥n base que se env√≠a a la IA en cada solicitud para definir su **rol, tono, o las reglas que debe seguir**.

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=os.getenv("GOOGLE_API_KEY"))

prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente experto en historia del Ecuador. Responde siempre en un tono amigable y educativo."),
    ("user", "{pregunta_del_usuario}")
])

output_parser = StrOutputParser()
chain = prompt | llm | output_parser
respuesta = chain.invoke({"pregunta_del_usuario": "¬øQui√©n fue Eloy Alfaro?"})
print(respuesta)
```

### b) Manteniendo Memoria en la Conversaci√≥n (Memory Buffer)

Para que un chatbot recuerde interacciones pasadas, necesitas a√±adir un **buffer de memoria** (ej. `ConversationBufferMemory` en LangChain). Este objeto almacena el historial y lo inyecta autom√°ticamente en el prompt, dando contexto a la IA.

---

## 9. Alternativa Local: Ejecutando un LLM en tu Propia M√°quina

Puedes ejecutar modelos de c√≥digo abierto como **Llama 3** en tu computadora con **Ollama**.

### a) Instalaci√≥n y Gesti√≥n de Modelos
1.  **Instala Ollama:** Desc√°rgalo desde [ollama.com](https://ollama.com/) para Windows.
2.  **Explora Modelos Disponibles:** Para ver la lista completa de modelos que puedes descargar, visita la biblioteca oficial en la web: **[https://ollama.com/library](https://ollama.com/library)**. Ollama no tiene un comando de terminal para buscar modelos remotos.
3.  **Gestiona Modelos Localmente:**
    ```bash
    # Ver modelos que ya tienes instalados
    ollama list

    # Descargar un modelo de la biblioteca
    ollama pull llama3

    # Chatear con un modelo instalado
    ollama run llama3
    ```

### b) Eligiendo un Modelo Local: Comparativa
No todos los modelos son iguales. Aqu√≠ tienes una comparaci√≥n para ayudarte a decidir.

| Modelo | Par√°metros | Tama√±o (Peso) | Fortalezas Clave | Comando `pull` |
| :--- | :--- | :--- | :--- | :--- |
| **Llama 3** | 8B | ~4.7 GB | **El mejor todoterreno.** Excelente razonamiento general, bueno para seguir instrucciones complejas. La opci√≥n m√°s segura y recomendada para empezar. | `ollama pull llama3` |
| **DeepSeek-Coder** | 7B | ~4.1 GB | **Especialista en C√≥digo.** Superior en tareas de programaci√≥n, autocompletado y l√≥gica de c√≥digo. Muy buen razonador general. | `ollama pull deepseek-coder` |
| **Mistral** | 7B | ~4.1 GB | **El m√°s eficiente.** Rendimiento incre√≠ble para su tama√±o, a menudo compite con modelos m√°s grandes. Muy r√°pido y balanceado. | `ollama pull mistral` |
| **Phi-3** | 3.8B | ~2.3 GB | **Potente y peque√±o.** Muy bueno en l√≥gica y c√≥digo para su tama√±o. Ideal si Llama 3 es un poco pesado para tu sistema. | `ollama pull phi3` |

**B** = Billones (Miles de millones) de par√°metros. M√°s par√°metros generalmente significan mayor capacidad, pero tambi√©n mayores requisitos de hardware.

### c) Usando Modelos Locales como una API
Ollama expone una API REST en `http://localhost:11434`. Puedes hacerle peticiones directamente.

**Ejemplo de API Request a Llama 3 local (CMD):**
```cmd
curl http://localhost:11434/api/generate -d "{\"model\": \"llama3\", \"prompt\": \"Why is the sky blue?\", \"stream\": false}"
```

### d) Integraci√≥n con LangChain

```python
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOllama(model="llama3")

prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un pirata. Responde a todas las preguntas con jerga de pirata."),
    ("user", "{pregunta}")
])

output_parser = StrOutputParser()
chain = prompt | llm | output_parser
respuesta = chain.invoke({"pregunta": "¬øCu√°l es la capital de Jap√≥n?"})
print(respuesta)
```

---

## 10. M√°s All√° de las Preguntas: Automatizaci√≥n de Tareas con Agentes de IA

Un **Agente de IA** usa un LLM como "cerebro" para razonar y utilizar **herramientas** (como una b√∫squeda web) para realizar acciones.

### a) Ejemplo Pr√°ctico: Un Agente de B√∫squeda Web

```python
from langchain_community.chat_models import ChatOllama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain import hub

# 1. Inicializa el modelo local (el cerebro del agente)
llm = ChatOllama(model="llama3")

# 2. Define las herramientas que el agente puede usar
tools = [DuckDuckGoSearchRun()]

# 3. Carga un prompt pre-dise√±ado para agentes (ReAct)
prompt = hub.pull("hwchase17/react")

# 4. Crea el agente
agent = create_react_agent(llm, tools, prompt)

# 5. Crea el "ejecutor" del agente
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 6. Invoca al agente
pregunta = "¬øQui√©n gan√≥ la final de la Champions League m√°s reciente?"
respuesta = agent_executor.invoke({"input": pregunta})

print(respuesta)
```
Al ejecutar con `verbose=True`, ver√°s el proceso de "pensamiento" del agente en la terminal.

---

## 11. Troubleshooting y Soluci√≥n de Problemas

### üîß Problemas Comunes y Soluciones

Si encuentras errores al ejecutar la aplicaci√≥n, aqu√≠ tienes las soluciones m√°s comunes:

#### Error 500 (Internal Server Error)
```
‚ùå Problema: La aplicaci√≥n devuelve error 500 al hacer peticiones
‚úÖ Soluci√≥n: Verificar que todas las dependencias est√©n instaladas correctamente
```bash
# Reinstalar dependencias
pip install -r requirements.txt

# Verificar configuraci√≥n
python diagnostico.py
```

#### Error de Conexi√≥n (ERR_CONNECTION_RESET)
```
‚ùå Problema: El navegador no puede conectarse al servidor
‚úÖ Soluci√≥n: Asegurarse de que la aplicaci√≥n est√© ejecut√°ndose
```bash
# Verificar que Python est√© ejecutando la aplicaci√≥n
python app.py

# Si no funciona, intentar:
python run_app.py  # Script con mejor manejo de errores
```

#### Problemas con el Agente
```
‚ùå Problema: El agente no funciona correctamente
‚úÖ Soluci√≥n: La aplicaci√≥n tiene fallback autom√°tico a chat simple
```

El c√≥digo est√° dise√±ado para funcionar incluso si el agente falla, usando el chat simple como respaldo.

#### Error de API Key
```
‚ùå Problema: Error relacionado con GOOGLE_API_KEY
‚úÖ Soluci√≥n: Verificar archivo .env
```bash
# Verificar que el archivo .env contenga:
GOOGLE_API_KEY="tu_clave_aqui"
```

### üìã Scripts de Diagn√≥stico

Para diagnosticar problemas, usa estos scripts incluidos:

```bash
# Verificar todas las dependencias y configuraci√≥n
python diagnostico.py

# Probar imports espec√≠ficos
python test_imports.py

# Ejecutar con mejor manejo de errores
python run_app.py
```

### üìù Archivos de Ayuda

- `SOLUCION_ERRORES.md`: Documentaci√≥n completa de errores solucionados
- `diagnostico.py`: Script para verificar el estado de la aplicaci√≥n
- `test_imports.py`: Verificaci√≥n de dependencias espec√≠ficas

### üÜò Si Todo Falla

1. **Verificar versi√≥n de Python**: Debe ser 3.8 o superior
2. **Recrear entorno virtual**: 
   ```bash
   python -m venv venv_nuevo
   .\venv_nuevo\Scripts\activate
   pip install -r requirements.txt
   ```
3. **Verificar conexi√≥n a internet**: Necesaria para descargar dependencias y usar el agente
4. **Consultar logs**: Revisar mensajes de error en la terminal para m√°s detalles
